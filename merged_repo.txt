2025-11-07T15:53:56.0985129+01:00] [fetch_features] start
[ M T 5 ]   C o m p a n y = M e t a Q u o t e s   L t d .   |   T e r m i n a l N a m e = M e t a T r a d e r   5   |   L o g i n = 6 2 7 1 9 0 2 5   |   S e r v e r = O A N D A T M S - M T 5   |   A c c o u n t N a m e = P L / M 6 2 7 1 9 0 2 5 / P L N 
 
 [ C F G ]   s y m b o l = G O L D . p r o   t f = M 5   k e e p _ d a y s = 7 2 0   u p d a t e _ d a y s = 6 0 
 
 S a v e d   b a r s :   d a t a / X A U U S D _ M 5 . c s v   ( 1 2 5 1 7 ) 
 
 S a v e d   t i c k s :   d a t a / X A U U S D _ t i c k s _ s a m p l e . c s v   ( 6 5 9 8 6 6 ) 
 
 C o s t   s u g g e s t i o n s   - >   r e p o r t s / c o s t s _ s u g g e s t i o n . y a m l 
 
 S a v e d   f e a t u r e s   - >   d a t a / X A U U S D _ M 5 _ f e a t u r e s . c s v   ( r o w s = 2 6 3 ,   c o l s = 7 9 ) 
 
 
[2025-11-07T15:54:09.8430797+01:00] [fetch_features] done

[2025-11-07T16:04:09.8545045+01:00] [fetch_features] start
[ M T 5 ]   C o m p a n y = M e t a Q u o t e s   L t d .   |   T e r m i n a l N a m e = M e t a T r a d e r   5   |   L o g i n = 6 2 7 1 9 0 2 5   |   S e r v e r = O A N D A T M S - M T 5   |   A c c o u n t N a m e = P L / M 6 2 7 1 9 0 2 5 / P L N 
 
 [ C F G ]   s y m b o l = G O L D . p r o   t f = M 5   k e e p _ d a y s = 7 2 0   u p d a t e _ d a y s = 6 0 
 
 S a v e d   b a r s :   d a t a / X A U U S D _ M 5 . c s v   ( 1 2 5 1 9 ) 
 
 S a v e d   t i c k s :   d a t a / X A U U S D _ t i c k s _ s a m p l e . c s v   ( 6 5 7 2 1 8 ) 
 
 C o s t   s u g g e s t i o n s   - >   r e p o r t s / c o s t s _ s u g g e s t i o n . y a m l 
 
 S a v e d   f e a t u r e s   - >   d a t a / X A U U S D _ M 5 _ f e a t u r e s . c s v   ( r o w s = 2 6 3 ,   c o l s = 7 9 ) 
 
 
[2025-11-07T16:04:22.2156028+01:00] [fetch_features] done


# -*- coding: utf-8 -*-
"""
ops/mini_update_ppo.py  – bezpieczny mini-update PPO na świeżych danych.
Użycie:
  python ops/mini_update_ppo.py --timesteps 300000 --train_days 120 --val_days 30 --seed 42
Loguje sporo informacji, żeby w PowerShell zobaczyć pełny stack trace.
"""
import argparse, json, sys, traceback
from pathlib import Path
import pandas as pd, yaml
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor, sync_envs_normalization
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.utils import set_random_seed

# ---- CLI
ap = argparse.ArgumentParser()
ap.add_argument("--timesteps", type=int, default=300000)
ap.add_argument("--train_days", type=int, default=120)
ap.add_argument("--val_days", type=int, default=30)
ap.add_argument("--seed", type=int, default=42)
args = ap.parse_args()

def main():
    cfg = yaml.safe_load(open("config.yaml","r",encoding="utf-8"))
    FEAT = cfg["files"]["features_csv"]
    MODEL_PATH = Path(cfg["files"]["model_path"])
    VECNORM_PATH = Path(cfg.get("files",{}).get("vecnorm_path","models/vecnorm_xauusd_m5.pkl"))
    WINDOW = int(cfg.get("window",128))
    COSTS = cfg["costs"]
    ENV_CFG = cfg.get("env",{}) or {}
    REWARD_MODE = ENV_CFG.get("reward_mode", cfg.get("reward_mode","pct"))
    FLIP_PENALTY = float(ENV_CFG.get("flip_penalty", cfg.get("flip_penalty",0.0)))
    TRADE_HOURS = ENV_CFG.get("trade_hours_utc", cfg.get("trade_hours_utc", None))
    MIN_EQ = float(ENV_CFG.get("min_equity", 0.8))

    # Dane
    df = pd.read_csv(FEAT, parse_dates=["time"]).sort_values("time").reset_index(drop=True)
    val_end = df["time"].max()
    val_start = val_end - pd.Timedelta(days=args.val_days)
    train_start = val_start - pd.Timedelta(days=args.train_days)

    train_df = df[(df["time"]>=train_start) & (df["time"]<val_start)].copy()
    val_df   = df[(df["time"]>=val_start)  & (df["time"]<=val_end)].copy()

    print(f"[mini_update] Training on {len(train_df)} rows (~{args.train_days}d), timesteps={args.timesteps}", flush=True)
    if len(train_df) <= WINDOW or len(val_df) <= WINDOW:
        raise RuntimeError(f"Too few rows for window={WINDOW}: train={len(train_df)}, val={len(val_df)}")

    # features_spec (jeśli istnieje)
    features_spec = None
    fspec = Path("models/features_spec.json")
    if fspec.exists():
        features_spec = json.loads(fspec.read_text(encoding="utf-8")).get("feature_columns", None)

    # Środowisko
    from env_xau import XauTradingEnv
    def make_train():
        env = XauTradingEnv(train_df, window=WINDOW,
            spread_abs=COSTS["spread_abs"], commission_rate=COSTS["commission_rate"], slippage_k=COSTS["slippage_k"],
            reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
            enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
        return TimeLimit(env, max_episode_steps=6000)

    def make_eval():
        env = XauTradingEnv(val_df, window=WINDOW,
            spread_abs=COSTS["spread_abs"], commission_rate=COSTS["commission_rate"], slippage_k=COSTS["slippage_k"],
            reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
            enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
        return TimeLimit(env, max_episode_steps=3000)

    set_random_seed(args.seed)
    vtrain = DummyVecEnv([make_train]); vtrain = VecMonitor(vtrain); vtrain = VecNormalize(vtrain, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)
    veval  = DummyVecEnv([make_eval]);  veval  = VecMonitor(veval);  veval  = VecNormalize(veval, training=False, norm_obs=True, norm_reward=False)
    sync_envs_normalization(veval, vtrain)

    policy_kwargs = dict(net_arch=dict(pi=[128,128], vf=[128,128]))
    tb_log_dir = "logs/ppo_gold_m5"
    model = PPO("MlpPolicy", vtrain, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02,
                policy_kwargs=policy_kwargs, seed=args.seed, verbose=1, tensorboard_log=tb_log_dir)

    eval_cb = EvalCallback(veval, best_model_save_path=str(MODEL_PATH.parent), log_path="logs/eval",
                           eval_freq=max(args.timesteps//4, 10000), n_eval_episodes=1,
                           deterministic=True, render=False)
    model.learn(total_timesteps=args.timesteps, callback=eval_cb)

    MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
    model.save(str(MODEL_PATH))
    vtrain.save(str(VECNORM_PATH))
    print(f"[mini_update] Saved model -> {MODEL_PATH}")
    print(f"[mini_update] Saved VecNormalize -> {VECNORM_PATH}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("\n[mini_update][FATAL] Uncaught exception:\n", file=sys.stderr, flush=True)
        traceback.print_exc()
        sys.exit(1)


# encoding: utf-8
$ErrorActionPreference = "Stop"

# === KONFIG (jak u Ciebie) ===
$REPO = "C:\xau_rl"
$PY   = "$REPO\.venv\Scripts\python.exe"
$LOG  = "$REPO\logs\services\mini_update_ppo.log"
$LOCK = "$REPO\logs\services\mini_update_ppo.lock"

# Przygotowanie
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
if (-not (Test-Path -LiteralPath $PY)) { $PY = "python" }  # awaryjnie z PATH

# --- LOCK NA UCHWYCIE (bez wyścigu dwóch instancji) ---
$script:LockHandle = $null
function Acquire-Lock {
    try {
        $script:LockHandle = [System.IO.File]::Open(
            $LOCK,
            [System.IO.FileMode]::OpenOrCreate,
            [System.IO.FileAccess]::ReadWrite,
            [System.IO.FileShare]::None
        )
        return $true
    } catch {
        return $false
    }
}
function Release-Lock {
    try {
        if ($script:LockHandle) {
            $script:LockHandle.Close()
            $script:LockHandle.Dispose()
            $script:LockHandle = $null
        }
    } catch {}
    try { Remove-Item $LOCK -Force -ErrorAction SilentlyContinue } catch {}
}

function Log-Line([string]$msg){
    try { Add-Content -LiteralPath $LOG -Value $msg -Encoding UTF8 } catch {}
}

function Run-Once {
    Log-Line "`n[$(Get-Date -Format o)] [mini_update] start"

    # --- DOKŁADNIE JAK W fetch_features: redirekcja *>> do TEGO SAMEGO LOGU ---
    # Całe stdout+stderr z Pythona poleci do $LOG, bez pośrednich handlerów.
    & $PY -u "ops\mini_update_ppo.py" "--timesteps" "300000" *>> $LOG

    $code = $LASTEXITCODE
    Log-Line "[$(Get-Date -Format o)] [mini_update] exit_code=$code"
    if ($code -ne 0) { throw "mini_update failed with exit_code=$code (see log)" }
    Log-Line "[$(Get-Date -Format o)] [mini_update] done"
}

# --- Globalny trap: nie pozwól wyjść bez logu, w razie wyjątku opóźnij pętlę ---
trap {
    try { Log-Line "[ERROR] TRAP: $($_.Exception.Message)`n$($_ | Out-String)" } catch {}
    Start-Sleep -Seconds 5
    continue
}

# === Pętla co 6h (jak w Twojej wersji) ===
while ($true) {
    try {
        if (Acquire-Lock) {
            try {
                Run-Once
            } finally {
                Release-Lock
            }
        } else {
            Log-Line "`n[$(Get-Date -Format o)] [mini_update] skipped (locked)"
        }
    } catch {
        Log-Line "[ERROR] $($_.Exception.Message)`n$($_ | Out-String)"
        Release-Lock
        Start-Sleep -Seconds 5  # bezpiecznik żeby NSSM nie „karuzelował”
    }

    Start-Sleep -Seconds (6*3600)  # co 6 godzin
}
