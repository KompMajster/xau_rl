# ops/feature_discovery_weekly.py
# -*- coding: utf-8 -*-
"""
Feature discovery z:
 - twardym kluczem FRED (Twoje środowisko prywatne),
 - opcjonalnym fallbackiem CSV dla FRED/GPR/COT/Calendar,
 - rankingiem (|Spearman| do targetu 1-krok do przodu) + lags,
 - dywersyfikacją: kara za częste użycie w historii + min. 20% nowych cech,
 - raportem Jaccard/Hamming i zapisem historii wyborów.

Artefakty:
  models/features_spec.json
  models/features_spec_history.jsonl  (append-only)
  reports/feature_discovery_rank.csv
  reports/feature_discovery_summary.txt
  logs/feature_discovery.log
"""
import argparse, os, sys, json, traceback
from pathlib import Path
import pandas as pd
import numpy as np
import yaml
from datetime import datetime, timedelta

ROOT = Path(__file__).resolve().parents[1]
os.chdir(ROOT)
LOG_DIR = ROOT / "logs"; LOG_DIR.mkdir(parents=True, exist_ok=True)
LOG = LOG_DIR / "feature_discovery.log"

HIST_PATH = ROOT / "models" / "features_spec_history.jsonl"  # historia wyborów
SUMMARY_PATH = ROOT / "reports" / "feature_discovery_summary.txt"
RANK_PATH = ROOT / "reports" / "feature_discovery_rank.csv"

# --- parametry dywersyfikacji ---
MIN_NEW_RATIO = 0.20     # min 20% nowych cech względem poprzedniego wyboru
HIST_WINDOW = 8          # okno historii do kary za powtarzalność (ostatnie N wyborów)
REPEAT_PENALTY = 0.90    # każdorazowe wystąpienie obniża score * 0.90^freq (freq w oknie)

# --- twardy klucz FRED (Twoje środowisko prywatne) ---
FRED_API_KEY_HARDCODED = "404d32dffe691fd6159eed5cceb44e80"

def log(msg: str):
    ts = datetime.now().isoformat(timespec="seconds")
    with open(LOG, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")

def load_cfg():
    with open(ROOT / "config.yaml", "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def read_features_csv(cfg):
    p = Path(cfg["files"]["features_csv"])
    return (
        pd.read_csv(p, parse_dates=["time"])
          .sort_values("time").reset_index(drop=True)
    )

# ---------- łączenie po czasie ----------
def safe_merge_on_time(df_base: pd.DataFrame, df_right: pd.DataFrame, col_map: dict):
    """Resample prawych danych do 1min + ffill, merge_asof do M5 w df_base."""
    if df_right is None or df_right.empty:
        return df_base
    r = df_right.copy()
    if "time" not in r.columns:
        raise ValueError("Right dataframe has no 'time'")
    r = (r.sort_values("time").drop_duplicates("time")
           .set_index("time").asfreq("1min").ffill().reset_index()
           .rename(columns=col_map))
    out = pd.merge_asof(
        df_base.sort_values("time"),
        r.sort_values("time"),
        on="time",
        direction="backward",
        tolerance=pd.Timedelta(minutes=10)
    )
    return out

# ---------- FRED ----------
def try_load_fred(series_id: str):
    """Najpierw fredapi z wpisanym kluczem; jeśli nie zadziała -> CSV fallback."""
    if not series_id:
        return None
    try:
        from fredapi import Fred  # może nie być zainstalowane
        fred = Fred(api_key=FRED_API_KEY_HARDCODED)
        s = fred.get_series(series_id)
        df = s.to_frame(name=series_id).reset_index()
        df = df.rename(columns={"index": "time"})
        df["time"] = pd.to_datetime(df["time"], utc=True)
        return df
    except Exception as e:
        log(f"[warn] FRED api unavailable ({e}); trying CSV fallback")

    csv_p = ROOT / "external" / "fred" / f"{series_id}.csv"
    if csv_p.exists():
        df = pd.read_csv(csv_p)
        col_time = "time" if "time" in df.columns else ("date" if "date" in df.columns else None)
        if not col_time or "value" not in df.columns:
            raise RuntimeError(f"FRED CSV malformed: {csv_p}")
        df[col_time] = pd.to_datetime(df[col_time], utc=True)
        df = df.rename(columns={col_time: "time", "value": series_id})
        return df
    return None

def try_load_simple_csv(p: Path, time_col="time"):
    if not p or not Path(p).exists():
        return None
    df = pd.read_csv(p)
    if time_col not in df.columns:
        for c in ["date", "Date"]:
            if c in df.columns:
                time_col = c
                break
    if time_col not in df.columns:
        return None
    df[time_col] = pd.to_datetime(df[time_col], utc=True)
    return df.rename(columns={time_col: "time"})

# ---------- target/lag/ranking ----------
def build_target(df: pd.DataFrame, price_col="close", horizon=1):
    return df[price_col].pct_change().shift(-horizon).rename("target_fwd1")

def add_lags(df: pd.DataFrame, cols: list, max_lag: int):
    d = df.copy()
    for c in cols:
        for L in range(1, max_lag + 1):
            d[f"{c}_lag{L}"] = d[c].shift(L)
    return d

def score_features(df: pd.DataFrame, feature_cols: list, target_col="target_fwd1"):
    """Ranking wg |rho| (Spearman)."""
    scores = []
    y = df[target_col].astype(float)
    for c in feature_cols:
        try:
            x = df[c].astype(float)
            ok = x.notna() & y.notna()
            if ok.sum() < 200:
                continue
            rho = x[ok].corr(y[ok], method="spearman")
            scores.append((c, float(abs(rho))))
        except Exception:
            continue
    scores.sort(key=lambda t: t[1], reverse=True)
    return pd.DataFrame(scores, columns=["feature", "abs_spearman"])

# ---------- historia/penalizacja/dywersyfikacja ----------
def load_history(max_lines=HIST_WINDOW):
    """Wczytaj ostatnie N rekordów historii wyborów (jsonl)."""
    if not HIST_PATH.exists():
        return []
    rows = []
    with open(HIST_PATH, "r", encoding="utf-8") as f:
        for line in f:
            try:
                rows.append(json.loads(line))
            except:
                pass
    return rows[-max_lines:] if rows else []

def save_history(entry: dict):
    HIST_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(HIST_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False) + "\n")

def jaccard(a: set, b: set) -> float:
    if not a and not b: return 1.0
    return len(a & b) / max(1, len(a | b))

def apply_repeat_penalty(rank_df: pd.DataFrame, hist_rows: list):
    """
    Obniż score cech często wybieranych w ostatnich wyborach:
      score' = score * (REPEAT_PENALTY ** freq)
    gdzie freq = liczba wystąpień cechy w oknie historii.
    """
    if not hist_rows or rank_df.empty:
        return rank_df.copy()
    freq = {}
    for r in hist_rows:
        feats = r.get("feature_columns", [])
        for c in feats:
            freq[c] = freq.get(c, 0) + 1
    out = rank_df.copy()
    out["repeat_freq"] = out["feature"].map(lambda c: freq.get(c, 0))
    out["score_adj"] = out["abs_spearman"] * (REPEAT_PENALTY ** out["repeat_freq"])
    out = out.sort_values("score_adj", ascending=False).reset_index(drop=True)
    return out

def diversify_select(adjusted_rank: pd.DataFrame, prev_features: list, topK: int, min_new_ratio=MIN_NEW_RATIO):
    """
    Wybór listy o długości topK z wymuszeniem, że min_new_ratio to cechy,
    których nie było w poprzednim wyborze.
    """
    prev_set = set(prev_features or [])
    new_needed = int(np.ceil(topK * min_new_ratio))

    chosen = []
    # 1) najpierw weź „nowe” cechy z góry rankingu
    for _, row in adjusted_rank.iterrows():
        f = row["feature"]
        if f not in prev_set and f not in chosen:
            chosen.append(f)
            if len([c for c in chosen if c not in prev_set]) >= new_needed:
                break

    # 2) dopełnij listę najlepszymi (mogą być już „stare”)
    if len(chosen) < topK:
        for _, row in adjusted_rank.iterrows():
            f = row["feature"]
            if f not in chosen:
                chosen.append(f)
                if len(chosen) >= topK:
                    break

    return chosen[:topK]

# ---------- main ----------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--topK", type=int, default=96)
    ap.add_argument("--max_lag", type=int, default=3)
    ap.add_argument("--out", type=str, default="models/features_spec.json")
    args = ap.parse_args()

    log("feature_discovery bootstrap_alive")
    cfg = load_cfg()
    log("start")

    df = read_features_csv(cfg)
    df["target_fwd1"] = build_target(df)

    # ---- fundamenty i kalendarz (opcjonalnie, bez crasha) ----
    try:
        fcfg = (cfg.get("fundamentals") or {})
        usd_id = fcfg.get("usd_series_id", None)   # np. DTWEXBGS
        fred_df = try_load_fred(usd_id) if usd_id else None
        if fred_df is not None:
            df = safe_merge_on_time(df, fred_df, {usd_id: f"fred_{usd_id}"})
            col = f"fred_{usd_id}"
            mu = df[col].rolling(5000, min_periods=100).mean()
            sd = df[col].rolling(5000, min_periods=100).std().replace(0, np.nan)
            df[col] = (df[col] - mu) / (sd + 1e-8)
        else:
            log("[info] FRED series not attached (no api/csv)")

        gpr_p = fcfg.get("gpr_csv", None)
        cot_p = fcfg.get("cot_csv", None)
        cal_cfg = (cfg.get("calendar") or {})
        cal_p = cal_cfg.get("calendar_csv", None)

        gpr = try_load_simple_csv(ROOT / gpr_p) if gpr_p else None
        if gpr is not None:
            vcols = [c for c in gpr.columns if c != "time"]
            if vcols:
                gpr = gpr[["time", vcols[0]]].rename(columns={vcols[0]: "gpr"})
                df = safe_merge_on_time(df, gpr, {"gpr": "gpr"})

        cot = try_load_simple_csv(ROOT / cot_p) if cot_p else None
        if cot is not None:
            vcols = [c for c in cot.columns if c != "time"]
            if vcols:
                cot = cot[["time", vcols[0]]].rename(columns={vcols[0]: "cot"})
                df = safe_merge_on_time(df, cot, {"cot": "cot"})

        cal = try_load_simple_csv(ROOT / cal_p) if cal_p else None
        if cal is not None:
            cal["event_flag"] = 1
            df = safe_merge_on_time(df, cal[["time", "event_flag"]], {"event_flag": "us_event"})
    except Exception as e:
        log(f"[warn] fundamentals/calendar attach failed: {e}")

    # ---- przygotuj listę kandydatów + lags ----
    base_cols = ["time","open","high","low","close","tick_volume","spread"]
    all_cols = [c for c in df.columns if c not in base_cols + ["target_fwd1"]]
    df = add_lags(df, all_cols, max_lag=args.max_lag)

    cand_cols = [c for c in df.columns if c not in base_cols + ["target_fwd1"]]
    rank = score_features(df, cand_cols, target_col="target_fwd1")
    if rank.empty:
        raise RuntimeError("Ranking pusty – brak danych do selekcji.")

    # ---- załaduj historię, zastosuj karę za powtarzanie ----
    hist_rows = load_history(HIST_WINDOW)
    prev_feats = hist_rows[-1]["feature_columns"] if hist_rows else []
    rank_adj = apply_repeat_penalty(rank, hist_rows)

    # zapisz ranking (z karą)
    out_rank = rank_adj.rename(columns={"score_adj":"score_after_penalty"}) \
                      .assign(prev_used=lambda x: x["feature"].isin(prev_feats))
    RANK_PATH.parent.mkdir(parents=True, exist_ok=True)
    out_rank.to_csv(RANK_PATH, index=False)

    # ---- wybór z minimalnym udziałem nowych cech ----
    topK = max(int(args.topK), 8)
    chosen = diversify_select(rank_adj, prev_feats, topK, MIN_NEW_RATIO)

    # ---- zapisz spec i historię ----
    spec = {
        "feature_columns": chosen,
        "price_column": "close_norm" if "close_norm" in df.columns else "close",
        "generated_at": datetime.now().isoformat(timespec="seconds")
    }
    out_p = ROOT / args.out
    out_p.parent.mkdir(parents=True, exist_ok=True)
    out_p.write_text(json.dumps(spec, ensure_ascii=False, indent=2), encoding="utf-8")

    save_history({
        "ts": datetime.now().isoformat(timespec="seconds"),
        "feature_columns": chosen
    })

    # ---- raport: czy faktycznie zmieniamy wskaźniki? ----
    prev_set = set(prev_feats or [])
    cur_set = set(chosen or [])
    jac = jaccard(prev_set, cur_set)
    chg = len(cur_set - prev_set)
    kept = len(cur_set & prev_set)
    with open(SUMMARY_PATH, "w", encoding="utf-8") as f:
        f.write("# Feature Discovery Summary\n")
        f.write(f"Generated at: {datetime.now().isoformat(timespec='seconds')}\n")
        f.write(f"TopK: {topK}\n")
        f.write(f"Prev size: {len(prev_set)}, Cur size: {len(cur_set)}\n")
        f.write(f"Jaccard(prev,cur): {jac:.3f}  (1.0 = identyczne, 0.0 = zupełnie różne)\n")
        f.write(f"Kept from prev: {kept}\n")
        f.write(f"New vs prev:   {chg}\n")
        f.write(f"Min new ratio enforced: {MIN_NEW_RATIO:.2f}\n")
    log(f"saved spec -> {out_p.as_posix()}, rank -> {RANK_PATH.as_posix()}, summary -> {SUMMARY_PATH.as_posix()}")
    print("Feature discovery completed.")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"[ERROR] {e}")
        log(traceback.format_exc())
        raise
