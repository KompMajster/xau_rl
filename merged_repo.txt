

=== FILE: check.py ===

import pandas as pd
df = pd.read_csv("data/XAUUSD_M5.csv", parse_dates=["time"])
print("bars rows:", len(df), "from:", df["time"].min(), "to:", df["time"].max())
print(df.tail(3))

=== FILE: config.yaml ===

symbol: GOLD.pro
timeframe: M5
history_days: 720
window: 128
mt5:
  bars_chunk: 100000
  update_days: 60
files:
  bars_csv: data/XAUUSD_M5.csv
  ticks_csv: data/XAUUSD_ticks_sample.csv
  features_csv: data/XAUUSD_M5_features.csv
  model_path: models/ppo_xauusd_m5.zip
  vecnorm_path: models/vecnorm_xauusd_m5.pkl
costs:
  spread_abs: 0.42
  commission_rate: 0.0001
  slippage_k: 0.0
env:
  reward_mode: pct
  flip_penalty: 0.002
  trade_hours_utc:
  - 06:00
  - '20:00'
  enforce_flat_outside_hours: true
  min_equity: 0.8
fundamentals:
  usd_series_id: DTWEXBGS
  gpr_csv: external/gpr/gpr.csv
  cot_csv: external/cot_gold.csv
calendar:
  calendar_csv: external/calendar_us.csv
  events_whitelist:
  - Non Farm Payrolls
  - Unemployment Rate
  - CPI
  - Core CPI
  - Core PCE
  - ISM Manufacturing PMI
  - ISM Services PMI
  - FOMC Interest Rate Decision
  - Fed Press Conference


=== FILE: env_xau.py ===

# -*- coding: utf-8 -*-
import numpy as np, pandas as pd
import gymnasium as gym
from gymnasium import spaces
from datetime import time as dtime

class XauTradingEnv(gym.Env):
    metadata = {"render_modes": []}

    def __init__(self, df: pd.DataFrame, window=128,
                 spread_abs=0.05, commission_rate=0.0001, slippage_k=0.10,
                 reward_mode: str = "pct", use_close_norm: bool = True,
                 flip_penalty: float = 0.0, trade_hours_utc=None,
                 enforce_flat_outside_hours: bool = True, features_spec: list | None = None,
                 min_equity: float = 0.8):
        super().__init__()
        self.df = df.sort_values('time').reset_index(drop=True)
        self.window = int(window)
        self.spread_abs = float(spread_abs)
        self.commission_rate = float(commission_rate)
        self.slippage_k = float(slippage_k)
        assert reward_mode in {"pct","points"}
        self.reward_mode = reward_mode
        self.use_close_norm = use_close_norm
        self.flip_penalty = float(flip_penalty)
        self.enforce_flat_outside = bool(enforce_flat_outside_hours)
        self.min_equity = float(min_equity)

        # Okno godzin handlu (UTC)
        self.trade_hours = None
        if trade_hours_utc and isinstance(trade_hours_utc, (list, tuple)) and len(trade_hours_utc) == 2:
            try:
                s = [int(x) for x in str(trade_hours_utc[0]).split(":")]
                e = [int(x) for x in str(trade_hours_utc[1]).split(":")]
                self.trade_hours = (dtime(s[0], s[1] if len(s) > 1 else 0),
                                    dtime(e[0], e[1] if len(e) > 1 else 0))
            except Exception:
                self.trade_hours = None

        # Kolumny cech
        base_cols = ['open', 'high', 'low', 'close', 'tick_volume', 'spread', 'time']
        if features_spec is not None:
            missing = [c for c in features_spec if c not in self.df.columns]
            if missing:
                raise ValueError(f"Missing feature columns: {missing}")
            self.feat_cols = list(features_spec)
        else:
            self.feat_cols = [c for c in self.df.columns if c not in base_cols]

        self.price_col = 'close_norm' if (self.use_close_norm and 'close_norm' in self.df.columns) else 'close'
        if len(self.df) <= self.window:
            raise ValueError(f"Not enough rows: {len(self.df)} <= window {self.window}")

        # Observation: [window x (1 + n_features)] + [pos, unrealized]
        obs_dim = self.window * (1 + len(self.feat_cols)) + 2
        self.action_space = spaces.Discrete(3)  # 0=SHORT, 1=FLAT, 2=LONG
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)

        # Stan początkowy
        self._start = self.window
        self._i = None
        self.pos = 0
        self.entry = None
        self.equity = 1.0
        self.prev_eq = 1.0

    def _in_trade_hours(self, ts) -> bool:
        if self.trade_hours is None:
            return True
        try:
            t = ts.to_pydatetime().time()
        except Exception:
            t = ts
        start, end = self.trade_hours
        if start <= end:
            return (t >= start) and (t <= end)
        # okno nocne np. 22:00–06:00
        return (t >= start) or (t <= end)

    def _obs(self):
        sl = slice(self._i - self.window, self._i)
        block_df = self.df.iloc[sl][[self.price_col] + self.feat_cols]
        block = block_df.to_numpy(dtype=np.float32)
        expected = 1 + len(self.feat_cols)
        if block.shape != (self.window, expected):
            raise RuntimeError(f"Bad window shape: {block.shape} vs {(self.window, expected)}")
        flat = block.flatten()

        price = float(self.df.iloc[self._i]['close'])
        unreal = 0.0
        if self.pos != 0 and self.entry is not None:
            dir_ = 1 if self.pos > 0 else -1
            unreal = dir_ * (price - self.entry)

        return np.concatenate([flat, np.array([self.pos, unreal], dtype=np.float32)])

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self._i = self._start
        self.pos = 0
        self.entry = None
        self.equity = 1.0
        self.prev_eq = 1.0
        return self._obs(), {}

    def step(self, action):
        if isinstance(action, (np.ndarray, list, tuple)):
            action = int(action[0])
        else:
            action = int(action)
        if not self.action_space.contains(action):
            raise ValueError("Invalid action")

        info = {}
        ts = self.df.iloc[self._i]['time']
        inside = self._in_trade_hours(ts)
        price = float(self.df.iloc[self._i]['close'])
        prev = float(self.df.iloc[self._i - 1]['close'])
        slip = self.slippage_k * abs(price - prev)

        desired = [-1, 0, 1][action]
        if not inside and self.enforce_flat_outside:
            desired = 0

        # Zmiana pozycji (koszty + ewentualny flip penalty)
        if desired != self.pos:
            # flip penalty gdy odwracasz kierunek bez przejścia na 0
            if self.pos != 0 and desired != 0 and np.sign(self.pos) != np.sign(desired) and self.flip_penalty > 0:
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - self.flip_penalty, 1e-6)
                else:
                    self.equity -= self.flip_penalty
                info['flip_penalty'] = float(self.flip_penalty)

            # koszt zamknięcia starej pozycji
            if self.pos != 0 and self.entry is not None:
                cost = (self.spread_abs + slip) / max(price, 1e-12) + self.commission_rate
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - cost, 1e-6)
                else:
                    self.equity -= cost

            # otwórz nową (lub wyzeruj)
            self.pos = desired
            if self.pos != 0:
                cost = (self.spread_abs + slip) / max(price, 1e-12) + self.commission_rate
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - cost, 1e-6)
                else:
                    self.equity -= cost
                self.entry = price
            else:
                self.entry = None

        # Zmiana equity w kroku
        if self.reward_mode == 'pct':
            step_ret = 0.0
            if self.pos != 0:
                dir_ = 1 if self.pos > 0 else -1
                step_ret = dir_ * ((price / max(prev, 1e-12)) - 1.0)
            self.equity *= (1.0 + step_ret)
            reward = float(self.equity - self.prev_eq)
        else:
            reward = float(self.equity - self.prev_eq)

        self.prev_eq = self.equity
        self._i += 1

        terminated = bool(self._i >= len(self.df) - 1)
        truncated = False
        if self.equity <= self.min_equity:
            truncated = True
            info['early_stop'] = True

        info.update({
            'time': ts,
            'price': price,
            'pos': int(self.pos),
            'equity': float(self.equity),
            'inside_hours': bool(inside)
        })
        return self._obs(), reward, terminated, truncated, info

=== FILE: features_check.py ===

import pandas as pd

df = pd.read_csv("data/XAUUSD_M5_features.csv", parse_dates=["time"])
print("features rows:", len(df), "from:", df["time"].min(), "to:", df["time"].max())
print(df.tail(3))

=== FILE: fetch__mt5_data.py ===

# -*- coding: utf-8 -*-
"""
fetch__mt5_data.py
------------------
Pobiera świece M5 (inkrementalnie do 720 dni) oraz próbkę ticków z MT5.

• Inkrementalny merge do jednego CSV z 720 dni (przycinanie, deduplikacja po 'time').
• Obejścia brokerów: copy_rates_range (naive datetimes) z fallbackiem do copy_rates_from_pos (tail),
  retry/backoff, chunkowanie przy pierwszym pobraniu.
• Atomiczny zapis CSV.
• Ticki weekend-safe (ostrzeżenie, nie przerywa).
• Sugestie kosztów (median/p75 spread + heurystyka slippage_k).
• Czas wszędzie UTC-aware.

Użycie:
    python fetch__mt5_data.py
"""

from __future__ import annotations

import MetaTrader5 as mt5
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
from pathlib import Path
import time
import yaml

# nasze utilsy
from utils.atomic import atomic_write_csv
from utils.mt5_health import ensure_mt5_ready

# ============================================================
# Konfiguracja
# ============================================================
CFG_PATH = 'config.yaml'
cfg = yaml.safe_load(open(CFG_PATH, 'r', encoding='utf-8'))

if not isinstance(cfg, dict) or 'files' not in cfg:
    raise RuntimeError(f"{CFG_PATH} jest pusty lub ma złą strukturę (brak sekcji 'files').")

SYMBOL = cfg.get('symbol', 'XAUUSD')
BARS_CSV = cfg['files']['bars_csv']
TICKS_CSV = cfg['files']['ticks_csv']
HISTORY_DAYS = int(cfg.get('history_days', 720))

TF_MAP = {
    'M1':  mt5.TIMEFRAME_M1,
    'M5':  mt5.TIMEFRAME_M5,
    'M15': mt5.TIMEFRAME_M15,
    'H1':  mt5.TIMEFRAME_H1,
}
TF_NAME = str(cfg.get('timeframe', 'M5'))
TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)

MT5_CFG = cfg.get('mt5', {}) or {}
CHUNK = int(MT5_CFG.get('bars_chunk', 20000))   # 20k ~ 70–90 dni M5 (zależnie od brokera)
UPDATE_DAYS = int(MT5_CFG.get('update_days', 60))

# ============================================================
# MT5 init / symbol utils
# ============================================================
def init_mt5():
    """Initialize MT5 i wypisz krótkie info o terminalu/koncie."""
    if not ensure_mt5_ready():
        last_err = mt5.last_error()
        raise RuntimeError(f"MT5 not ready (terminal/account). Start MT5 and login. last_error={last_err}")
    term = mt5.terminal_info()
    acc = mt5.account_info()
    parts = []
    if getattr(term, "company", None): parts.append(f"Company={term.company}")
    if getattr(term, "name", None):    parts.append(f"TerminalName={term.name}")
    if getattr(acc, "login", 0):       parts.append(f"Login={acc.login}")
    if getattr(acc, "server", None):   parts.append(f"Server={acc.server}")
    if getattr(acc, "name", None):     parts.append(f"AccountName={acc.name}")
    print("[MT5] " + " | ".join(parts) if parts else "[MT5] initialized")

def ensure_symbol(symbol: str) -> bool:
    """Upewnij się, że symbol jest widoczny w Market Watch."""
    info = mt5.symbol_info(symbol)
    if info is None:
        mt5.symbol_select(symbol, True)
        info = mt5.symbol_info(symbol)
        if info is None:
            return False
    if not info.visible:
        if not mt5.symbol_select(symbol, True):
            return False
    return True

def suggest_similar(symbol: str, limit=12):
    """Podpowiedz nazwy symboli powiązanych (np. GOLD/XAU) – zależne od serwera."""
    out = []
    try:
        for s in mt5.symbols_get():
            nm = s.name.upper()
            if ("GOLD" in nm) or ("XAU" in nm):
                out.append(s.name)
        if not out:
            out = [s.name for s in mt5.symbols_get()][:limit]
        return sorted(set(out))[:limit]
    except Exception:
        return []

# ============================================================
# Helpers
# ============================================================
def with_retries(fn, attempts=3, sleep_s=2, *a, **kw):
    last = None
    for i in range(attempts):
        try:
            return fn(*a, **kw)
        except Exception as e:
            last = e
            time.sleep(sleep_s * (i + 1))
    if last:
        raise last

def _to_df(rates) -> pd.DataFrame:
    """Konwersja stawek MT5 -> kanoniczny DataFrame z sanity-checkiem kolumn i czasu."""
    df = pd.DataFrame(rates)
    if df.empty:
        return df
    df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
    if 'real_volume' in df.columns:
        df.rename(columns={'real_volume': 'tick_volume'}, inplace=True)
    cols = ['time', 'open', 'high', 'low', 'close', 'tick_volume', 'spread']
    # zachowaj tylko znane kolumny
    df = df[[c for c in cols if c in df.columns]].copy()
    # usuń zduplikowane nazwy i rekordy
    df = (df.loc[:, ~df.columns.duplicated()]
            .sort_values('time')
            .drop_duplicates('time')
            .reset_index(drop=True))
    return df

def _make_naive(dt: datetime) -> datetime:
    """Zwróć 'naive' datetime (bez tzinfo) – część brokerów tego wymaga dla copy_rates_range."""
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

# ============================================================
# Pobieranie danych
# ============================================================
def fetch_range(symbol, timeframe, utc_from: datetime, utc_to: datetime) -> pd.DataFrame:
    """
    Pobierz zakres 'range' z obejściami:
    • daty 'naive' (bez tzinfo),
    • fallback: tail od końca (copy_rates_from_pos) gdy range zwraca pustkę/Invalid params.
    """
    frm_n = _make_naive(utc_from)
    to_n  = _make_naive(utc_to)
    rates = mt5.copy_rates_range(symbol, timeframe, frm_n, to_n)
    if rates is None or len(rates) == 0:
        # fallback: tail od końca
        tail = mt5.copy_rates_from_pos(symbol, timeframe, 0, CHUNK)
        if tail is None or len(tail) == 0:
            last_err = mt5.last_error()
            raise RuntimeError(f"copy_rates_range empty for '{symbol}'. last_error={last_err}")
        return _to_df(tail)
    return _to_df(rates)

def fetch_from_pos_tail(symbol, timeframe, count) -> pd.DataFrame:
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None or len(rates) == 0:
        last_err = mt5.last_error()
        raise RuntimeError(f"copy_rates_from_pos empty for '{symbol}'. last_error={last_err}")
    return _to_df(rates)

def load_existing_bars(path: str):
    p = Path(path)
    if not p.exists():
        return None
    try:
        df = pd.read_csv(p, parse_dates=['time'])
        if df.empty:
            return None
        # Usuń kolumny techniczne i duplikaty nazw
        bad = [c for c in df.columns if str(c).startswith('Unnamed')]
        if bad:
            df = df.drop(columns=bad)
        df = df.loc[:, ~df.columns.duplicated()]
        keep = ['time','open','high','low','close','tick_volume','spread']
        df = df[[c for c in keep if c in df.columns]].copy()
        # Upewnij się, że 'time' jest UTC-aware
        if pd.api.types.is_datetime64_any_dtype(df['time']):
            if df['time'].dt.tz is None:
                df['time'] = df['time'].dt.tz_localize('UTC')
            else:
                df['time'] = df['time'].dt.tz_convert('UTC')
        else:
            df['time'] = pd.to_datetime(df['time'], utc=True)
        df = (df.sort_values('time')
                .drop_duplicates('time')
                .reset_index(drop=True))
        return df
    except Exception:
        return None

def merge_clip(existing: pd.DataFrame | None, new_df: pd.DataFrame, keep_days: int) -> pd.DataFrame:
    """Scal istniejące i nowe świece, deduplikuj po 'time', przytnij do keep_days i oczyść nagłówki."""
    def _sanitize(d):
        if d is None or d.empty:
            return d
        d = d.loc[:, ~d.columns.duplicated()]
        bad = [c for c in d.columns if str(c).startswith('Unnamed')]
        if bad:
            d = d.drop(columns=bad)
        return d

    existing = _sanitize(existing)
    new_df   = _sanitize(new_df)

    if existing is None or existing.empty:
        base = new_df.copy()
    else:
        common = [c for c in existing.columns if c in new_df.columns]
        base = (pd.concat([existing[common], new_df[common]], axis=0, ignore_index=True, copy=False)
                  .drop_duplicates('time')
                  .sort_values('time'))

    # time -> UTC-aware
    if pd.api.types.is_datetime64_any_dtype(base['time']):
        if base['time'].dt.tz is None:
            base['time'] = base['time'].dt.tz_localize('UTC')
        else:
            base['time'] = base['time'].dt.tz_convert('UTC')
    else:
        base['time'] = pd.to_datetime(base['time'], utc=True)

    # clip do okna czasowego
    cutoff = datetime.now(timezone.utc) - timedelta(days=keep_days + 1)
    base = base[base['time'] >= pd.Timestamp(cutoff)].reset_index(drop=True)
    return base

def fetch_bars_incremental(symbol, timeframe, keep_days: int, update_days: int) -> pd.DataFrame:
    """
    Strategia:
    • Gdy brak pliku – zbierz historię 'od końca' (copy_rates_from_pos) w 1–4 chunkach,
      aż pokryjesz ~keep_days (z marginesem). Jeśli to się nie uda – spróbuj krótkiego range.
    • Gdy plik istnieje – dociągnij ostatnie update_days (mały range z fallbackiem).
    """
    existing = load_existing_bars(BARS_CSV)
    if existing is None:
        wanted_days = keep_days + 2
        parts_df = None
        # 1–4 próby z narastającą liczbą świec (od końca)
        for i in range(1, 5):
            count = CHUNK * i
            try:
                dft = with_retries(lambda: fetch_from_pos_tail(symbol, timeframe, count))
            except Exception:
                time.sleep(1.0)
                continue
            if dft is not None and len(dft) > 0:
                parts_df = dft
                span_days = (parts_df['time'].max() - parts_df['time'].min()).days
                if span_days >= wanted_days:
                    break
        if parts_df is None or parts_df.empty:
            # Ostatnia próba – krótki range (np. 60 dni)
            to  = datetime.now(timezone.utc)
            frm = to - timedelta(days=min(60, wanted_days))
            parts_df = fetch_range(symbol, timeframe, frm, to)
        return merge_clip(None, parts_df, keep_days)

    # Inkrementalnie: dociągnij update_days (mały range z fallbackiem)
    to  = datetime.now(timezone.utc)
    frm = to - timedelta(days=max(2, update_days))
    fresh = fetch_range(symbol, timeframe, frm, to)
    merged = merge_clip(existing, fresh, keep_days)
    return merged
def fetch_ticks(symbol: str, hours: int = 24) -> pd.DataFrame:
    utc_to   = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(hours=hours)
    ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)
    if ticks is None or len(ticks) == 0:
        last_err = mt5.last_error()
        raise RuntimeError(f"No MT5 ticks for '{symbol}'. last_error={last_err}")
    tdf = pd.DataFrame(ticks)
    tdf['time'] = pd.to_datetime(tdf['time'], unit='s', utc=True)
    return tdf[['time', 'bid', 'ask', 'last', 'volume']]

def suggest_costs(tdf: pd.DataFrame) -> dict:
    """Sugeruj koszty na bazie ticków: median/p75 spread, heurystyczny slippage_k."""
    spr = (tdf['ask'] - tdf['bid']).astype(float).replace([np.inf, -np.inf], np.nan).dropna()
    med_spread = float(np.median(spr)) if len(spr) else 0.0
    p75_spread = float(np.percentile(spr, 75)) if len(spr) else 0.0
    mid = (tdf['ask'] + tdf['bid']) / 2.0
    dm = (mid.diff().abs()).replace([np.inf, -np.inf], np.nan).dropna()
    med_dm = float(np.median(dm)) if len(dm) else 0.0
    med_price = float(np.nanmedian(mid)) if len(mid) else 1.0
    # heurystyka: jaką część typowego ruchu stanowi przeciętny skok mida
    slippage_k = float(min(max(med_dm / max(med_price, 1e-12), 0.0), 0.01))
    return {
        'spread_abs_median': round(med_spread, 5),
        'spread_abs_p75':    round(p75_spread, 5),
        'slippage_k_suggested': round(slippage_k, 4)
    }

# ============================================================
# Main
# ============================================================
def main():
    init_mt5()
    try:
        print(f"[CFG] symbol={SYMBOL} tf={TF_NAME} keep_days={HISTORY_DAYS} update_days={UPDATE_DAYS}")

        if not ensure_symbol(SYMBOL):
            similar = suggest_similar(SYMBOL)
            hint = ", ".join(similar) if similar else "(brak propozycji)"
            raise SystemExit(
                "Symbol not found or not visible in Market Watch: '{}'\n"
                "Try one of these (server-specific): {}\n"
                "Also open Symbols window and SHOW the instrument."
                .format(SYMBOL, hint)
            )

        # ŚWIECE
        bars = fetch_bars_incremental(SYMBOL, TF, keep_days=HISTORY_DAYS, update_days=UPDATE_DAYS)
        assert {'time', 'open', 'high', 'low', 'close'}.issubset(bars.columns), "Bars frame malformed"
        assert len(bars) > 100, "Too few bars"
        atomic_write_csv(bars, BARS_CSV)
        print(f"Saved bars: {BARS_CSV} ({len(bars)})")

        # TICKI (opcjonalnie; weekend-safe)
        try:
            ticks = with_retries(lambda: fetch_ticks(SYMBOL, hours=24))
            atomic_write_csv(ticks, TICKS_CSV)
            print(f"Saved ticks: {TICKS_CSV} ({len(ticks)})")

            sugg = suggest_costs(ticks)
            rep = Path('reports'); rep.mkdir(parents=True, exist_ok=True)
            Path('reports/costs_suggestion.yaml').write_text(
                yaml.safe_dump(sugg, allow_unicode=True, sort_keys=False),
                encoding='utf-8'
            )
            print("Cost suggestions -> reports/costs_suggestion.yaml")
        except Exception as e:
            # np. weekend: brak ticków (OK)
            print(f"[warn] ticks unavailable ({e}); keeping previous {TICKS_CSV}")

    finally:
        mt5.shutdown()

if __name__ == '__main__':
    main()

=== FILE: merged_repo.txt ===



=== FILE: check.py ===

import pandas as pd
df = pd.read_csv("data/XAUUSD_M5.csv", parse_dates=["time"])
print("bars rows:", len(df), "from:", df["time"].min(), "to:", df["time"].max())
print(df.tail(3))

=== FILE: config.yaml ===

symbol: GOLD.pro
timeframe: M5
history_days: 720
window: 128
mt5:
  bars_chunk: 100000
  update_days: 60
files:
  bars_csv: data/XAUUSD_M5.csv
  ticks_csv: data/XAUUSD_ticks_sample.csv
  features_csv: data/XAUUSD_M5_features.csv
  model_path: models/ppo_xauusd_m5.zip
  vecnorm_path: models/vecnorm_xauusd_m5.pkl
costs:
  spread_abs: 0.42
  commission_rate: 0.0001
  slippage_k: 0.0
env:
  reward_mode: pct
  flip_penalty: 0.002
  trade_hours_utc:
  - 06:00
  - '20:00'
  enforce_flat_outside_hours: true
  min_equity: 0.8
fundamentals:
  usd_series_id: DTWEXBGS
  gpr_csv: external/gpr/gpr.csv
  cot_csv: external/cot_gold.csv
calendar:
  calendar_csv: external/calendar_us.csv
  events_whitelist:
  - Non Farm Payrolls
  - Unemployment Rate
  - CPI
  - Core CPI
  - Core PCE
  - ISM Manufacturing PMI
  - ISM Services PMI
  - FOMC Interest Rate Decision
  - Fed Press Conference


=== FILE: env_xau.py ===

# -*- coding: utf-8 -*-
import numpy as np, pandas as pd
import gymnasium as gym
from gymnasium import spaces
from datetime import time as dtime

class XauTradingEnv(gym.Env):
    metadata = {"render_modes": []}

    def __init__(self, df: pd.DataFrame, window=128,
                 spread_abs=0.05, commission_rate=0.0001, slippage_k=0.10,
                 reward_mode: str = "pct", use_close_norm: bool = True,
                 flip_penalty: float = 0.0, trade_hours_utc=None,
                 enforce_flat_outside_hours: bool = True, features_spec: list | None = None,
                 min_equity: float = 0.8):
        super().__init__()
        self.df = df.sort_values('time').reset_index(drop=True)
        self.window = int(window)
        self.spread_abs = float(spread_abs)
        self.commission_rate = float(commission_rate)
        self.slippage_k = float(slippage_k)
        assert reward_mode in {"pct","points"}
        self.reward_mode = reward_mode
        self.use_close_norm = use_close_norm
        self.flip_penalty = float(flip_penalty)
        self.enforce_flat_outside = bool(enforce_flat_outside_hours)
        self.min_equity = float(min_equity)

        # Okno godzin handlu (UTC)
        self.trade_hours = None
        if trade_hours_utc and isinstance(trade_hours_utc, (list, tuple)) and len(trade_hours_utc) == 2:
            try:
                s = [int(x) for x in str(trade_hours_utc[0]).split(":")]
                e = [int(x) for x in str(trade_hours_utc[1]).split(":")]
                self.trade_hours = (dtime(s[0], s[1] if len(s) > 1 else 0),
                                    dtime(e[0], e[1] if len(e) > 1 else 0))
            except Exception:
                self.trade_hours = None

        # Kolumny cech
        base_cols = ['open', 'high', 'low', 'close', 'tick_volume', 'spread', 'time']
        if features_spec is not None:
            missing = [c for c in features_spec if c not in self.df.columns]
            if missing:
                raise ValueError(f"Missing feature columns: {missing}")
            self.feat_cols = list(features_spec)
        else:
            self.feat_cols = [c for c in self.df.columns if c not in base_cols]

        self.price_col = 'close_norm' if (self.use_close_norm and 'close_norm' in self.df.columns) else 'close'
        if len(self.df) <= self.window:
            raise ValueError(f"Not enough rows: {len(self.df)} <= window {self.window}")

        # Observation: [window x (1 + n_features)] + [pos, unrealized]
        obs_dim = self.window * (1 + len(self.feat_cols)) + 2
        self.action_space = spaces.Discrete(3)  # 0=SHORT, 1=FLAT, 2=LONG
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)

        # Stan poczÄ…tkowy
        self._start = self.window
        self._i = None
        self.pos = 0
        self.entry = None
        self.equity = 1.0
        self.prev_eq = 1.0

    def _in_trade_hours(self, ts) -> bool:
        if self.trade_hours is None:
            return True
        try:
            t = ts.to_pydatetime().time()
        except Exception:
            t = ts
        start, end = self.trade_hours
        if start <= end:
            return (t >= start) and (t <= end)
        # okno nocne np. 22:00â€“06:00
        return (t >= start) or (t <= end)

    def _obs(self):
        sl = slice(self._i - self.window, self._i)
        block_df = self.df.iloc[sl][[self.price_col] + self.feat_cols]
        block = block_df.to_numpy(dtype=np.float32)
        expected = 1 + len(self.feat_cols)
        if block.shape != (self.window, expected):
            raise RuntimeError(f"Bad window shape: {block.shape} vs {(self.window, expected)}")
        flat = block.flatten()

        price = float(self.df.iloc[self._i]['close'])
        unreal = 0.0
        if self.pos != 0 and self.entry is not None:
            dir_ = 1 if self.pos > 0 else -1
            unreal = dir_ * (price - self.entry)

        return np.concatenate([flat, np.array([self.pos, unreal], dtype=np.float32)])

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self._i = self._start
        self.pos = 0
        self.entry = None
        self.equity = 1.0
        self.prev_eq = 1.0
        return self._obs(), {}

    def step(self, action):
        if isinstance(action, (np.ndarray, list, tuple)):
            action = int(action[0])
        else:
            action = int(action)
        if not self.action_space.contains(action):
            raise ValueError("Invalid action")

        info = {}
        ts = self.df.iloc[self._i]['time']
        inside = self._in_trade_hours(ts)
        price = float(self.df.iloc[self._i]['close'])
        prev = float(self.df.iloc[self._i - 1]['close'])
        slip = self.slippage_k * abs(price - prev)

        desired = [-1, 0, 1][action]
        if not inside and self.enforce_flat_outside:
            desired = 0

        # Zmiana pozycji (koszty + ewentualny flip penalty)
        if desired != self.pos:
            # flip penalty gdy odwracasz kierunek bez przejÅ›cia na 0
            if self.pos != 0 and desired != 0 and np.sign(self.pos) != np.sign(desired) and self.flip_penalty > 0:
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - self.flip_penalty, 1e-6)
                else:
                    self.equity -= self.flip_penalty
                info['flip_penalty'] = float(self.flip_penalty)

            # koszt zamkniÄ™cia starej pozycji
            if self.pos != 0 and self.entry is not None:
                cost = (self.spread_abs + slip) / max(price, 1e-12) + self.commission_rate
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - cost, 1e-6)
                else:
                    self.equity -= cost

            # otwÃ³rz nowÄ… (lub wyzeruj)
            self.pos = desired
            if self.pos != 0:
                cost = (self.spread_abs + slip) / max(price, 1e-12) + self.commission_rate
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - cost, 1e-6)
                else:
                    self.equity -= cost
                self.entry = price
            else:
                self.entry = None

        # Zmiana equity w kroku
        if self.reward_mode == 'pct':
            step_ret = 0.0
            if self.pos != 0:
                dir_ = 1 if self.pos > 0 else -1
                step_ret = dir_ * ((price / max(prev, 1e-12)) - 1.0)
            self.equity *= (1.0 + step_ret)
            reward = float(self.equity - self.prev_eq)
        else:
            reward = float(self.equity - self.prev_eq)

        self.prev_eq = self.equity
        self._i += 1

        terminated = bool(self._i >= len(self.df) - 1)
        truncated = False
        if self.equity <= self.min_equity:
            truncated = True
            info['early_stop'] = True

        info.update({
            'time': ts,
            'price': price,
            'pos': int(self.pos),
            'equity': float(self.equity),
            'inside_hours': bool(inside)
        })
        return self._obs(), reward, terminated, truncated, info

=== FILE: features_check.py ===

import pandas as pd

df = pd.read_csv("data/XAUUSD_M5_features.csv", parse_dates=["time"])
print("features rows:", len(df), "from:", df["time"].min(), "to:", df["time"].max())
print(df.tail(3))

=== FILE: fetch__mt5_data.py ===

# -*- coding: utf-8 -*-
"""
fetch__mt5_data.py
------------------
Pobiera Å›wiece M5 (inkrementalnie do 720 dni) oraz prÃ³bkÄ™ tickÃ³w z MT5.

â€¢ Inkrementalny merge do jednego CSV z 720 dni (przycinanie, deduplikacja po 'time').
â€¢ ObejÅ›cia brokerÃ³w: copy_rates_range (naive datetimes) z fallbackiem do copy_rates_from_pos (tail),
  retry/backoff, chunkowanie przy pierwszym pobraniu.
â€¢ Atomiczny zapis CSV.
â€¢ Ticki weekend-safe (ostrzeÅ¼enie, nie przerywa).
â€¢ Sugestie kosztÃ³w (median/p75 spread + heurystyka slippage_k).
â€¢ Czas wszÄ™dzie UTC-aware.

UÅ¼ycie:
    python fetch__mt5_data.py
"""

from __future__ import annotations

import MetaTrader5 as mt5
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
from pathlib import Path
import time
import yaml

# nasze utilsy
from utils.atomic import atomic_write_csv
from utils.mt5_health import ensure_mt5_ready

# ============================================================
# Konfiguracja
# ============================================================
CFG_PATH = 'config.yaml'
cfg = yaml.safe_load(open(CFG_PATH, 'r', encoding='utf-8'))

if not isinstance(cfg, dict) or 'files' not in cfg:
    raise RuntimeError(f"{CFG_PATH} jest pusty lub ma zÅ‚Ä… strukturÄ™ (brak sekcji 'files').")

SYMBOL = cfg.get('symbol', 'XAUUSD')
BARS_CSV = cfg['files']['bars_csv']
TICKS_CSV = cfg['files']['ticks_csv']
HISTORY_DAYS = int(cfg.get('history_days', 720))

TF_MAP = {
    'M1':  mt5.TIMEFRAME_M1,
    'M5':  mt5.TIMEFRAME_M5,
    'M15': mt5.TIMEFRAME_M15,
    'H1':  mt5.TIMEFRAME_H1,
}
TF_NAME = str(cfg.get('timeframe', 'M5'))
TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)

MT5_CFG = cfg.get('mt5', {}) or {}
CHUNK = int(MT5_CFG.get('bars_chunk', 20000))   # 20k ~ 70â€“90 dni M5 (zaleÅ¼nie od brokera)
UPDATE_DAYS = int(MT5_CFG.get('update_days', 60))

# ============================================================
# MT5 init / symbol utils
# ============================================================
def init_mt5():
    """Initialize MT5 i wypisz krÃ³tkie info o terminalu/koncie."""
    if not ensure_mt5_ready():
        last_err = mt5.last_error()
        raise RuntimeError(f"MT5 not ready (terminal/account). Start MT5 and login. last_error={last_err}")
    term = mt5.terminal_info()
    acc = mt5.account_info()
    parts = []
    if getattr(term, "company", None): parts.append(f"Company={term.company}")
    if getattr(term, "name", None):    parts.append(f"TerminalName={term.name}")
    if getattr(acc, "login", 0):       parts.append(f"Login={acc.login}")
    if getattr(acc, "server", None):   parts.append(f"Server={acc.server}")
    if getattr(acc, "name", None):     parts.append(f"AccountName={acc.name}")
    print("[MT5] " + " | ".join(parts) if parts else "[MT5] initialized")

def ensure_symbol(symbol: str) -> bool:
    """Upewnij siÄ™, Å¼e symbol jest widoczny w Market Watch."""
    info = mt5.symbol_info(symbol)
    if info is None:
        mt5.symbol_select(symbol, True)
        info = mt5.symbol_info(symbol)
        if info is None:
            return False
    if not info.visible:
        if not mt5.symbol_select(symbol, True):
            return False
    return True

def suggest_similar(symbol: str, limit=12):
    """Podpowiedz nazwy symboli powiÄ…zanych (np. GOLD/XAU) â€“ zaleÅ¼ne od serwera."""
    out = []
    try:
        for s in mt5.symbols_get():
            nm = s.name.upper()
            if ("GOLD" in nm) or ("XAU" in nm):
                out.append(s.name)
        if not out:
            out = [s.name for s in mt5.symbols_get()][:limit]
        return sorted(set(out))[:limit]
    except Exception:
        return []

# ============================================================
# Helpers
# ============================================================
def with_retries(fn, attempts=3, sleep_s=2, *a, **kw):
    last = None
    for i in range(attempts):
        try:
            return fn(*a, **kw)
        except Exception as e:
            last = e
            time.sleep(sleep_s * (i + 1))
    if last:
        raise last

def _to_df(rates) -> pd.DataFrame:
    """Konwersja stawek MT5 -> kanoniczny DataFrame z sanity-checkiem kolumn i czasu."""
    df = pd.DataFrame(rates)
    if df.empty:
        return df
    df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
    if 'real_volume' in df.columns:
        df.rename(columns={'real_volume': 'tick_volume'}, inplace=True)
    cols = ['time', 'open', 'high', 'low', 'close', 'tick_volume', 'spread']
    # zachowaj tylko znane kolumny
    df = df[[c for c in cols if c in df.columns]].copy()
    # usuÅ„ zduplikowane nazwy i rekordy
    df = (df.loc[:, ~df.columns.duplicated()]
            .sort_values('time')
            .drop_duplicates('time')
            .reset_index(drop=True))
    return df

def _make_naive(dt: datetime) -> datetime:
    """ZwrÃ³Ä‡ 'naive' datetime (bez tzinfo) â€“ czÄ™Å›Ä‡ brokerÃ³w tego wymaga dla copy_rates_range."""
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

# ============================================================
# Pobieranie danych
# ============================================================
def fetch_range(symbol, timeframe, utc_from: datetime, utc_to: datetime) -> pd.DataFrame:
    """
    Pobierz zakres 'range' z obejÅ›ciami:
    â€¢ daty 'naive' (bez tzinfo),
    â€¢ fallback: tail od koÅ„ca (copy_rates_from_pos) gdy range zwraca pustkÄ™/Invalid params.
    """
    frm_n = _make_naive(utc_from)
    to_n  = _make_naive(utc_to)
    rates = mt5.copy_rates_range(symbol, timeframe, frm_n, to_n)
    if rates is None or len(rates) == 0:
        # fallback: tail od koÅ„ca
        tail = mt5.copy_rates_from_pos(symbol, timeframe, 0, CHUNK)
        if tail is None or len(tail) == 0:
            last_err = mt5.last_error()
            raise RuntimeError(f"copy_rates_range empty for '{symbol}'. last_error={last_err}")
        return _to_df(tail)
    return _to_df(rates)

def fetch_from_pos_tail(symbol, timeframe, count) -> pd.DataFrame:
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None or len(rates) == 0:
        last_err = mt5.last_error()
        raise RuntimeError(f"copy_rates_from_pos empty for '{symbol}'. last_error={last_err}")
    return _to_df(rates)

def load_existing_bars(path: str):
    p = Path(path)
    if not p.exists():
        return None
    try:
        df = pd.read_csv(p, parse_dates=['time'])
        if df.empty:
            return None
        # UsuÅ„ kolumny techniczne i duplikaty nazw
        bad = [c for c in df.columns if str(c).startswith('Unnamed')]
        if bad:
            df = df.drop(columns=bad)
        df = df.loc[:, ~df.columns.duplicated()]
        keep = ['time','open','high','low','close','tick_volume','spread']
        df = df[[c for c in keep if c in df.columns]].copy()
        # Upewnij siÄ™, Å¼e 'time' jest UTC-aware
        if pd.api.types.is_datetime64_any_dtype(df['time']):
            if df['time'].dt.tz is None:
                df['time'] = df['time'].dt.tz_localize('UTC')
            else:
                df['time'] = df['time'].dt.tz_convert('UTC')
        else:
            df['time'] = pd.to_datetime(df['time'], utc=True)
        df = (df.sort_values('time')
                .drop_duplicates('time')
                .reset_index(drop=True))
        return df
    except Exception:
        return None

def merge_clip(existing: pd.DataFrame | None, new_df: pd.DataFrame, keep_days: int) -> pd.DataFrame:
    """Scal istniejÄ…ce i nowe Å›wiece, deduplikuj po 'time', przytnij do keep_days i oczyÅ›Ä‡ nagÅ‚Ã³wki."""
    def _sanitize(d):
        if d is None or d.empty:
            return d
        d = d.loc[:, ~d.columns.duplicated()]
        bad = [c for c in d.columns if str(c).startswith('Unnamed')]
        if bad:
            d = d.drop(columns=bad)
        return d

    existing = _sanitize(existing)
    new_df   = _sanitize(new_df)

    if existing is None or existing.empty:
        base = new_df.copy()
    else:
        common = [c for c in existing.columns if c in new_df.columns]
        base = (pd.concat([existing[common], new_df[common]], axis=0, ignore_index=True, copy=False)
                  .drop_duplicates('time')
                  .sort_values('time'))

    # time -> UTC-aware
    if pd.api.types.is_datetime64_any_dtype(base['time']):
        if base['time'].dt.tz is None:
            base['time'] = base['time'].dt.tz_localize('UTC')
        else:
            base['time'] = base['time'].dt.tz_convert('UTC')
    else:
        base['time'] = pd.to_datetime(base['time'], utc=True)

    # clip do okna czasowego
    cutoff = datetime.now(timezone.utc) - timedelta(days=keep_days + 1)
    base = base[base['time'] >= pd.Timestamp(cutoff)].reset_index(drop=True)
    return base

def fetch_bars_incremental(symbol, timeframe, keep_days: int, update_days: int) -> pd.DataFrame:
    """
    Strategia:
    â€¢ Gdy brak pliku â€“ zbierz historiÄ™ 'od koÅ„ca' (copy_rates_from_pos) w 1â€“4 chunkach,
      aÅ¼ pokryjesz ~keep_days (z marginesem). JeÅ›li to siÄ™ nie uda â€“ sprÃ³buj krÃ³tkiego range.
    â€¢ Gdy plik istnieje â€“ dociÄ…gnij ostatnie update_days (maÅ‚y range z fallbackiem).
    """
    existing = load_existing_bars(BARS_CSV)
    if existing is None:
        wanted_days = keep_days + 2
        parts_df = None
        # 1â€“4 prÃ³by z narastajÄ…cÄ… liczbÄ… Å›wiec (od koÅ„ca)
        for i in range(1, 5):
            count = CHUNK * i
            try:
                dft = with_retries(lambda: fetch_from_pos_tail(symbol, timeframe, count))
            except Exception:
                time.sleep(1.0)
                continue
            if dft is not None and len(dft) > 0:
                parts_df = dft
                span_days = (parts_df['time'].max() - parts_df['time'].min()).days
                if span_days >= wanted_days:
                    break
        if parts_df is None or parts_df.empty:
            # Ostatnia prÃ³ba â€“ krÃ³tki range (np. 60 dni)
            to  = datetime.now(timezone.utc)
            frm = to - timedelta(days=min(60, wanted_days))
            parts_df = fetch_range(symbol, timeframe, frm, to)
        return merge_clip(None, parts_df, keep_days)

    # Inkrementalnie: dociÄ…gnij update_days (maÅ‚y range z fallbackiem)
    to  = datetime.now(timezone.utc)
    frm = to - timedelta(days=max(2, update_days))
    fresh = fetch_range(symbol, timeframe, frm, to)
    merged = merge_clip(existing, fresh, keep_days)
    return merged
def fetch_ticks(symbol: str, hours: int = 24) -> pd.DataFrame:
    utc_to   = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(hours=hours)
    ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)
    if ticks is None or len(ticks) == 0:
        last_err = mt5.last_error()
        raise RuntimeError(f"No MT5 ticks for '{symbol}'. last_error={last_err}")
    tdf = pd.DataFrame(ticks)
    tdf['time'] = pd.to_datetime(tdf['time'], unit='s', utc=True)
    return tdf[['time', 'bid', 'ask', 'last', 'volume']]

def suggest_costs(tdf: pd.DataFrame) -> dict:
    """Sugeruj koszty na bazie tickÃ³w: median/p75 spread, heurystyczny slippage_k."""
    spr = (tdf['ask'] - tdf['bid']).astype(float).replace([np.inf, -np.inf], np.nan).dropna()
    med_spread = float(np.median(spr)) if len(spr) else 0.0
    p75_spread = float(np.percentile(spr, 75)) if len(spr) else 0.0
    mid = (tdf['ask'] + tdf['bid']) / 2.0
    dm = (mid.diff().abs()).replace([np.inf, -np.inf], np.nan).dropna()
    med_dm = float(np.median(dm)) if len(dm) else 0.0
    med_price = float(np.nanmedian(mid)) if len(mid) else 1.0
    # heurystyka: jakÄ… czÄ™Å›Ä‡ typowego ruchu stanowi przeciÄ™tny skok mida
    slippage_k = float(min(max(med_dm / max(med_price, 1e-12), 0.0), 0.01))
    return {
        'spread_abs_median': round(med_spread, 5),
        'spread_abs_p75':    round(p75_spread, 5),
        'slippage_k_suggested': round(slippage_k, 4)
    }

# ============================================================
# Main
# ============================================================
def main():
    init_mt5()
    try:
        print(f"[CFG] symbol={SYMBOL} tf={TF_NAME} keep_days={HISTORY_DAYS} update_days={UPDATE_DAYS}")

        if not ensure_symbol(SYMBOL):
            similar = suggest_similar(SYMBOL)
            hint = ", ".join(similar) if similar else "(brak propozycji)"
            raise SystemExit(
                "Symbol not found or not visible in Market Watch: '{}'\n"
                "Try one of these (server-specific): {}\n"
                "Also open Symbols window and SHOW the instrument."
                .format(SYMBOL, hint)
            )

        # ÅšWIECE
        bars = fetch_bars_incremental(SYMBOL, TF, keep_days=HISTORY_DAYS, update_days=UPDATE_DAYS)
        assert {'time', 'open', 'high', 'low', 'close'}.issubset(bars.columns), "Bars frame malformed"
        assert len(bars) > 100, "Too few bars"
        atomic_write_csv(bars, BARS_CSV)
        print(f"Saved bars: {BARS_CSV} ({len(bars)})")

        # TICKI (opcjonalnie; weekend-safe)
        try:
            ticks = with_retries(lambda: fetch_ticks(SYMBOL, hours=24))
            atomic_write_csv(ticks, TICKS_CSV)
            print(f"Saved ticks: {TICKS_CSV} ({len(ticks)})")

            sugg = suggest_costs(ticks)
            rep = Path('reports'); rep.mkdir(parents=True, exist_ok=True)
            Path('reports/costs_suggestion.yaml').write_text(
                yaml.safe_dump(sugg, allow_unicode=True, sort_keys=False),
                encoding='utf-8'
            )
            print("Cost suggestions -> reports/costs_suggestion.yaml")
        except Exception as e:
            # np. weekend: brak tickÃ³w (OK)
            print(f"[warn] ticks unavailable ({e}); keeping previous {TICKS_CSV}")

    finally:
        mt5.shutdown()

if __name__ == '__main__':
    main()

=== FILE: merge_repo.py ===

import os
import chardet

# === KONFIGURACJA ===
repo_path = r"C:\xau_rl"           # ≈öcie≈ºka do folderu repozytorium
output_file = "merged_repo.txt"    # Nazwa wynikowego pliku

# Rozszerzenia plik√≥w do do≈ÇƒÖczenia
included_extensions = (".py", ".ipynb", ".txt", ".md", ".yaml", ".yml")

# Foldery, kt√≥re majƒÖ byƒá pominiƒôte
excluded_dirs = {".venv", "__pycache__", ".git", ".idea", ".vscode", "build", "dist"}

# === FUNKCJA SCALANIA ===
with open(output_file, "w", encoding="utf-8") as outfile:
    for root, dirs, files in os.walk(repo_path):
        # Pomijaj foldery systemowe / techniczne
        dirs[:] = [d for d in dirs if d not in excluded_dirs]

        for filename in files:
            if filename.endswith(included_extensions):
                file_path = os.path.join(root, filename)
                try:
                    # Automatyczne wykrywanie kodowania
                    with open(file_path, "rb") as raw:
                        result = chardet.detect(raw.read(4096))
                        encoding = result["encoding"] or "utf-8"

                    with open(file_path, "r", encoding=encoding, errors="replace") as infile:
                        relative_path = os.path.relpath(file_path, repo_path)
                        outfile.write(f"\n\n=== FILE: {relative_path} ===\n\n")
                        outfile.write(infile.read())

                    print(f"‚úÖ Dodano: {relative_path}")

                except Exception as e:
                    print(f"‚ö†Ô∏è Nie uda≈Ço siƒô odczytaƒá pliku {filename}: {e}")

print(f"\nüéâ Gotowe! Wszystkie pliki scalono do: {output_file}")

=== FILE: quick_env_test.py ===

import yaml, pandas as pd
from env_xau import XauTradingEnv
cfg=yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
df=pd.read_csv(cfg['files']['features_csv'], parse_dates=['time']).sort_values('time').reset_index(drop=True)
df=df.tail(max(int(cfg.get('window',128))+256,512))
env=XauTradingEnv(df, window=int(cfg.get('window',128)),
    spread_abs=cfg['costs']['spread_abs'], commission_rate=cfg['costs']['commission_rate'], slippage_k=cfg['costs']['slippage_k'],
    reward_mode=cfg.get('env',{}).get('reward_mode', cfg.get('reward_mode','pct')), use_close_norm=True,
    min_equity=float(cfg.get('env',{}).get('min_equity',0.8)))
obs,info=env.reset(); print('reset OK', len(obs))
obs,r,term,trunc,info=env.step(1); print('step OK', r, term, trunc)


=== FILE: README.md ===

# XAUUSD RL PPO (M5) – Starter (Demo/Edu)
**Uwaga**: tylko do celów edukacyjnych, backtestów i *paper tradingu* (demo). Brak kodu wysyłającego realne zlecenia.

## Szybki start
```powershell
python -m venv .venv
# .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
# Na końcu zainstaluj TORCH odpowiedni dla CPU/GPU z pytorch.org (wheel).
python fetch__mt5_data.py
python utils\calibration.py
python utils\calibration.py --apply
python features\build_features.py
python rl\train_ppo.py --timesteps 1500000
python rl\evaluate.py --out_dir reports
python utils\make_report.py

24/7 na Windows
Użyj skryptów w ops\ oraz NSSM (patrz ops\install_services.ps1).
Dane i historia
Trzymamy 720 dni historii świec. Zbieracz działa inkrementalnie: dociąga ostatnie update_days (domyślnie 60), scala z istniejacym CSV i przycina do 720 dni. Zapisy CSV są atomowe.


=== FILE: requirements.txt ===

pandas>=2.0
numpy>=1.24
pytz
PyYAML
joblib
matplotlib
tensorboard
MetaTrader5
gymnasium>=0.29
stable-baselines3>=2.2.1
python-dotenv
fredapi
scikit-learn
requests
chardet

=== FILE: save_fred_cache.py ===

# save_gpr_cache.py
import pandas as pd
from pathlib import Path
import requests
import io

# Źródło CSV (benchmark GPR, miesięczny) – publiczny plik z witryny autorów:
URL = "https://www.matteoiacoviello.com/gpr_files/GPR.csv"

def main():
    r = requests.get(URL, timeout=30)
    r.raise_for_status()
    # Oryginalny CSV ma nagłówki 'date' i 'GPR' lub podobne – normalizujemy do (date,gpr)
    df = pd.read_csv(io.StringIO(r.text))
    # Ujednolicenie nazw kolumn:
    cols = {c.lower(): c for c in df.columns}
    # próbujemy znaleźć właściwe kolumny bez względu na wielkość liter
    date_col = [c for c in df.columns if c.lower() == 'date'][0]
    gpr_col  = [c for c in df.columns if c.lower() in ('gpr','gpr_index','value')][0]
    out = pd.DataFrame({
        'date': pd.to_datetime(df[date_col]).dt.tz_localize('UTC').dt.normalize(),
        'gpr': pd.to_numeric(df[gpr_col], errors='coerce')
    }).dropna().sort_values('date')
    out['date'] = out['date'].dt.strftime('%Y-%m-%d')

    path = Path("external/gpr"); path.mkdir(parents=True, exist_ok=True)
    out_path = path / "gpr.csv"
    out.to_csv(out_path, index=False)
    print(f"Saved GPR -> {out_path} ({len(out)} rows)")

if __name__ == "__main__":
    main()

=== FILE: save_te_calendar_cache.py ===

# -*- coding: utf-8 -*-
"""
save_te_calendar_cache.py
Pobiera kalendarz makro z Trading Economics i zapisuje cache do external/calendar_us.csv
Zmienna środowiskowa: TE_API_KEY=<Twój_klucz>
"""

from __future__ import annotations
import os, sys, csv
from pathlib import Path
from datetime import datetime, timedelta, timezone
import requests
import pandas as pd

# Domyślny zakres: od dziś-7 dni do dziś+21 dni (żeby objąć zdarzenia przed i po)
LOOKBACK_DAYS = int(os.getenv("TE_LOOKBACK_DAYS", "7"))
LOOKAHEAD_DAYS = int(os.getenv("TE_LOOKAHEAD_DAYS", "21"))
COUNTRY = os.getenv("TE_COUNTRY", "United States")

def iso_date(d: datetime) -> str:
    return d.strftime("%Y-%m-%d")

def fetch_te_calendar(api_key: str, country: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    API docs: https://docs.tradingeconomics.com/economic_calendar/
    Endpoint: GET /calendar/{start}/{end}?country=...&c=APIKEY
    """
    base = f"https://api.tradingeconomics.com/calendar/{start_date}/{end_date}"
    params = {"country": country, "c": api_key}
    r = requests.get(base, params=params, timeout=40)
    r.raise_for_status()
    data = r.json()

    rows = []
    for it in data:
        # Pola w TE bywają różne zależnie od endpointu — ujednolicamy
        ts = it.get("Date", it.get("DateUtc") or it.get("date"))
        if not ts:
            continue
        try:
            ts_utc = pd.to_datetime(ts, utc=True)
        except Exception:
            # czasami API zwraca bez Z — wymuś UTC
            ts_utc = pd.to_datetime(ts).tz_localize("UTC")
        rows.append({
            "time_utc" : ts_utc,
            "country"  : it.get("Country") or "",
            "event"    : it.get("Event") or it.get("Category") or "",
            "actual"   : it.get("Actual") if it.get("Actual") not in (None, "") else "",
            "previous" : it.get("Previous") if it.get("Previous") not in (None, "") else "",
            "forecast" : it.get("Forecast") if it.get("Forecast") not in (None, "") else "",
            # importance: TE zwykle zwraca 1..3 lub string; rzutujemy na int (brak -> 0)
            "importance": int(it.get("ImportanceValue", it.get("Importance", 0)) or 0),
        })

    df = pd.DataFrame(rows)
    if df.empty:
        return df
    # Normalizacja i sort
    df = df.sort_values("time_utc").reset_index(drop=True)
    return df

def main():
    api_key = os.getenv("TE_API_KEY")
    if not api_key:
        print("Brak TE_API_KEY. Ustaw zmienną środowiskową TE_API_KEY i uruchom ponownie.", file=sys.stderr)
        sys.exit(2)

    now = datetime.now(timezone.utc)
    start = now - timedelta(days=LOOKBACK_DAYS)
    end   = now + timedelta(days=LOOKAHEAD_DAYS)

    df = fetch_te_calendar(api_key, COUNTRY, iso_date(start), iso_date(end))
    out_dir = Path("external"); out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / "calendar_us.csv"

    if df.empty:
        # jeśli API nic nie zwróciło — nie nadpisujemy istniejącego cache
        if out_path.exists():
            print(f"[warn] Pusta odpowiedź z TE. Zachowuję istniejący cache: {out_path}")
            sys.exit(0)
        else:
            print("[warn] Pusta odpowiedź z TE i brak wcześniejszego cache. Nic nie zapisano.")
            sys.exit(0)

    # Zapisz CSV w formacie zgodnym z features/calendar_features.py
    df_out = df.copy()
    # Upewnijmy się, że kolumny są w oczekiwanym zestawie
    df_out["time_utc"] = df_out["time_utc"].dt.strftime("%Y-%m-%dT%H:%M:%SZ")
    cols = ["time_utc","country","event","actual","previous","forecast","importance"]
    df_out = df_out[cols]
    df_out.to_csv(out_path, index=False, quoting=csv.QUOTE_MINIMAL)
    print(f"Saved TE calendar -> {out_path} ({len(df_out)} rows)")

if __name__ == "__main__":
    main()

=== FILE: features\build_features.py ===

# -*- coding: utf-8 -*-
import pandas as pd, numpy as np, yaml, json, os
from pathlib import Path

from features.fundamentals import build_fundamentals_features
from features.calendar_features import make_event_features, load_calendar_csv

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
BARS = cfg['files']['bars_csv']; FEAT = cfg['files']['features_csv']

def macd(series, fast=12, slow=26, signal=9):
    ema_fast = series.ewm(span=fast, adjust=False).mean()
    ema_slow = series.ewm(span=slow, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False).mean()
    return macd_line, signal_line, macd_line - signal_line
def bbands(series, period=20, n_std=2.0):
    ma = series.rolling(period).mean()
    sd = series.rolling(period).std()
    upper = ma + n_std*sd; lower = ma - n_std*sd
    width = (upper - lower) / (ma.replace(0,np.nan).abs() + 1e-12)
    return ma, upper, lower, width

def add_tech_features(df):
    df = df.sort_values('time').reset_index(drop=True)
    c = df['close']
    df['ret1'] = np.log(c).diff()
    df['ema10'] = c.ewm(span=10).mean(); df['ema50']=c.ewm(span=50).mean(); df['ema200']=c.ewm(span=200).mean()
    d = c.diff(); up=d.clip(lower=0).ewm(alpha=1/14,adjust=False).mean(); down=(-d.clip(upper=0)).ewm(alpha=1/14,adjust=False).mean(); rs=up/(down+1e-12)
    df['rsi14'] = 100 - (100/(1+rs))
    h,l,cl = df['high'], df['low'], df['close']
    tr = np.maximum(h-l, np.maximum((h-cl.shift()).abs(), (l-cl.shift()).abs()))
    df['atr14'] = tr.ewm(alpha=1/14, adjust=False).mean()
    m,s,hst = macd(c); df['macd']=m; df['macd_signal']=s; df['macd_hist']=hst
    bb_ma, bb_up, bb_lo, bb_w = bbands(c); df['bb_ma']=bb_ma; df['bb_up']=bb_up; df['bb_lo']=bb_lo; df['bb_width']=bb_w
    df['minute']=df['time'].dt.hour*60+df['time'].dt.minute
    df['tod_sin']=np.sin(2*np.pi*df['minute']/1440); df['tod_cos']=np.cos(2*np.pi*df['minute']/1440); df.drop(columns=['minute'], inplace=True)
    df['dow']=df['time'].dt.dayofweek
    df['dow_sin']=np.sin(2*np.pi*df['dow']/7); df['dow_cos']=np.cos(2*np.pi*df['dow']/7); df.drop(columns=['dow'], inplace=True)
    df['close_log']=np.log(c.clip(lower=1e-12))
    mu = df['close_log'].rolling(2000, min_periods=200).mean()
    sd = df['close_log'].rolling(2000, min_periods=200).std().replace(0,np.nan)
    df['close_norm']=(df['close_log']-mu)/(sd+1e-8)
    # normalizacja rolling wybranych featurów
    feat_cols=['ret1','ema10','ema50','ema200','rsi14','atr14','macd','macd_signal','macd_hist','bb_ma','bb_up','bb_lo','bb_width','tod_sin','tod_cos','dow_sin','dow_cos']
    for col in feat_cols:
        mu = df[col].rolling(2000, min_periods=200).mean()
        sd = df[col].rolling(2000, min_periods=200).std().replace(0,np.nan)
        df[col]=(df[col]-mu)/(sd+1e-8)
    return df

def main():
    bars = pd.read_csv(BARS, parse_dates=['time']).sort_values('time').reset_index(drop=True)
    df = add_tech_features(bars)

    # --- FUNDAMENTALS ---
    fred_key = os.getenv("FRED_API_KEY", None)
    gpr_csv = cfg.get('fundamentals',{}).get('gpr_csv', None)
    cot_csv = cfg.get('fundamentals',{}).get('cot_csv', None)
    usd_series_id = cfg.get('fundamentals',{}).get('usd_series_id', "DTWEXBGS")
    fund = None
    try:
        fund = build_fundamentals_features(BARS, fred_key, gpr_csv=gpr_csv, cot_csv=cot_csv, usd_series_id=usd_series_id)
    except Exception as e:
        print(f"[warn] fundamentals skipped: {e}")

    # --- CALENDAR ---
    cal_csv = cfg.get('calendar',{}).get('calendar_csv', None)
    cal_events = cfg.get('calendar',{}).get('events_whitelist', None)
    cal_df = None
    cal_feat = None
    if cal_csv and Path(cal_csv).exists():
        cal_df = load_calendar_csv(cal_csv)
        cal_feat = make_event_features(BARS, cal_df, events_whitelist=cal_events, impact_threshold=2)

    # MERGE
    parts = [df]
    if fund is not None:
        parts.append(fund.drop(columns=['time']))
    if cal_feat is not None:
        parts.append(cal_feat.drop(columns=['time']))
    full = pd.concat(parts, axis=1)

    # dropna i zapis
    full = full.dropna().reset_index(drop=True)
    Path(FEAT).parent.mkdir(parents=True, exist_ok=True)
    full.to_csv(FEAT, index=False)

    # features_spec.json (jeśli istnieje – zachowaj; jeśli nie – wygeneruj z defaultu technicznego)
    base_cols = ['time','open','high','low','close','tick_volume','spread']
    feature_columns = [c for c in full.columns if c not in base_cols + ['close_log','close_norm']]
    spec = {"feature_columns": feature_columns, "price_column":"close_norm"}
    Path('models').mkdir(parents=True, exist_ok=True)
    Path('models/features_spec.json').write_text(json.dumps(spec, ensure_ascii=False, indent=2), encoding='utf-8')
    print("Saved features and features_spec.json (with fundamentals/calendar if available).")

if __name__ == "__main__":
    main()

=== FILE: features\calendar_features.py ===

# -*- coding: utf-8 -*-
"""
features/calendar_features.py
Kalendarz makro (Trading Economics) -> cechy 'time-to-event', 'post-event', dummies,
oraz 'surprise' (tylko w minucie publikacji). Bezpośredni request jest opcjonalny:
możesz też zasilać z lokalnego CSV (cache).

Dokumentacja TE Calendar API: https://docs.tradingeconomics.com/economic_calendar/  # cite: turn2search2
"""

from __future__ import annotations
import os, json
import pandas as pd, numpy as np
from pathlib import Path
from datetime import timezone

try:
    import requests  # type: ignore
except Exception:
    requests = None

DEFAULT_EVENTS = [
    "Non Farm Payrolls", "Unemployment Rate", "CPI", "Core CPI", "Core PCE",
    "ISM Manufacturing PMI", "ISM Services PMI", "FOMC Interest Rate Decision",
    "Fed Press Conference"
]

def fetch_te_calendar(api_key: str,
                      country: str = "United States",
                      start_iso: str = None,
                      end_iso: str = None) -> pd.DataFrame:
    """
    Pobiera kalendarz z Trading Economics. Jeśli requests brak -> podnieś błąd.
    Zwraca DataFrame z kolumnami: time_utc, country, event, actual, previous, forecast, importance
    """
    if requests is None:
        raise ImportError("Brak 'requests'. Użyj lokalnego CSV cache albo zainstaluj requests.")

    base = "https://api.tradingeconomics.com/calendar"
    params = {"country": country, "c": api_key}
    if start_iso and end_iso:
        base = f"{base}/{start_iso}/{end_iso}"

    r = requests.get(base, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    rows = []
    for it in data:
        try:
            ts = pd.to_datetime(it.get('Date', it.get('DateUtc') or it.get('date')), utc=True)
            rows.append({
                'time_utc': ts,
                'country': it.get('Country'),
                'event': it.get('Event') or it.get('Category'),
                'actual': it.get('Actual') if it.get('Actual') not in (None, '') else np.nan,
                'previous': it.get('Previous') if it.get('Previous') not in (None, '') else np.nan,
                'forecast': it.get('Forecast') if it.get('Forecast') not in (None, '') else np.nan,
                'importance': it.get('Importance') or it.get('ImportanceValue') or 0
            })
        except Exception:
            continue
    df = pd.DataFrame(rows).dropna(subset=['time_utc','event']).sort_values('time_utc')
    return df

def load_calendar_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df['time_utc'] = pd.to_datetime(df['time_utc'], utc=True)
    return df.sort_values('time_utc')

def make_event_features(bars_csv: str,
                        calendar_df: pd.DataFrame,
                        events_whitelist: list[str] | None = None,
                        impact_threshold: int = 2) -> pd.DataFrame:
    """
    Tworzy cechy eventowe na indeksie M5 z bars_csv:
    - tt_event_min: minuty do najbliższego eventu (clipped np. do +/- 720)
    - post_event_min: minuty od ostatniego eventu
    - is_event_hi: 1 w minucie publikacji eventu o wysokim impakcie
    - surprise_{short}: (actual - forecast) tylko w minucie publikacji
    - one-hot per event (bazując na events_whitelist)

    'impact_threshold' – filtruje tylko eventy o wysokim impakcie.
    """
    bars = pd.read_csv(bars_csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
    bars['time'] = bars['time'].dt.tz_localize("UTC") if bars['time'].dt.tz is None else bars['time'].dt.tz_convert("UTC")

    ev = calendar_df.copy()
    if events_whitelist:
        ev = ev[ev['event'].isin(events_whitelist)].copy()

    # Map importance do int (0..3)
    def _imp(v):
        if v in (None, np.nan, ''): return 0
        try:
            return int(v)
        except Exception:
            return 0
    ev['imp_i'] = ev['importance'].apply(_imp)
    ev = ev[ev['imp_i'] >= impact_threshold]

    idx = pd.DatetimeIndex(bars['time'])
    # Najbliższy event >= now
    next_ts = np.array([ev['time_utc'][ev['time_utc'] >= t].min() if (ev['time_utc'] >= t).any() else pd.NaT for t in idx])
    prev_ts = np.array([ev['time_utc'][ev['time_utc'] <= t].max() if (ev['time_utc'] <= t).any() else pd.NaT for t in idx])

    tt_event_min = pd.Series(((pd.to_datetime(next_ts) - idx).astype('timedelta64[m]')).astype('float'), index=bars.index)
    post_event_min = pd.Series(((idx - pd.to_datetime(prev_ts)).astype('timedelta64[m]')).astype('float'), index=bars.index)
    tt_event_min = tt_event_min.clip(-720, 720).fillna(720.0)
    post_event_min = post_event_min.clip(-720, 720).fillna(720.0)

    # Flaga minuty publikacji (dokładne dopasowanie do 5-min slotu)
    pub_map = {pd.Timestamp(t, tz="UTC"): i for i, t in enumerate(ev['time_utc'])}
    is_event_hi = bars['time'].map(lambda t: 1 if t.floor('5min') in pub_map else 0).astype(int)

    # Surprise: tylko gdy jest exact minuta publikacji
    ev = ev.set_index(ev['time_utc'].dt.floor('5min'))
    ev_small = ev[['event','actual','forecast']].copy()
    ev_small['surprise'] = pd.to_numeric(ev_small['actual'], errors='coerce') - pd.to_numeric(ev_small['forecast'], errors='coerce')
    surprise = pd.Series(0.0, index=bars.index)
    surp_idx = bars['time'].dt.floor('5min')
    surprise.loc[is_event_hi == 1] = ev_small.reindex(surp_idx[is_event_hi == 1])['surprise'].values

    # One-hot po nazwie eventu (whitelist)
    features = {
        'tt_event_min': tt_event_min.values,
        'post_event_min': post_event_min.values,
        'is_event_hi': is_event_hi.values,
        'ev_surprise': surprise.fillna(0.0).values,
    }
    if events_whitelist:
        for name in events_whitelist:
            k = f"ev_{name.lower().replace(' ', '_').replace('/', '_')}"
            mask = (surp_idx.isin(ev_small.index)) & (ev_small.reindex(surp_idx)['event'] == name)
            features[k] = mask.astype(int).values

    out = pd.DataFrame(features, index=bars.index)
    out.insert(0, 'time', bars['time'].values)
    return out


=== FILE: features\discover.py ===

# -*- coding: utf-8 -*-
"""
features/discover.py
Selektor cech:
- IC (Spearman) cecha vs ret_{+1} (M5) na rolling oknach -> mediana IC, stabilność.
- (Opcjonalnie) ważność z prostego modelu (Lasso / GBDT), jeśli scikit-learn dostępny.
- Ograniczenie zmian: max +/- 'change_frac' względem poprzedniego features_spec.json.

Wejście: pełny plik features_csv (po zbudowaniu), config window, K, change_frac.
Wyjście: models/features_spec.json (lista kolumn + column price 'close_norm').
"""

from __future__ import annotations
import json
from pathlib import Path
import pandas as pd, numpy as np
from scipy.stats import spearmanr

try:
    from sklearn.linear_model import LassoCV  # type: ignore
    HAVE_SKL = True
except Exception:
    HAVE_SKL = False

BASE_COLS = ['time','open','high','low','close','tick_volume','spread']

def compute_targets(df: pd.DataFrame) -> pd.Series:
    ret1 = np.log(df['close']).diff().shift(-1)  # target = kolejna świeca
    return ret1

def rolling_ic(df_feat: pd.DataFrame, target: pd.Series, win: int = 5000) -> pd.Series:
    scores = {}
    valid_cols = [c for c in df_feat.columns if c not in BASE_COLS + ['close_log','close_norm']]
    # prosty IC na całym okresie (dla szybkości); wariant rolling można dodać później
    for c in valid_cols:
        a = df_feat[c].values
        b = target.values
        ok = np.isfinite(a) & np.isfinite(b)
        if ok.sum() < 500:  # min próbek
            scores[c] = 0.0
            continue
        rho, _ = spearmanr(a[ok], b[ok])
        scores[c] = 0.0 if (rho is None or np.isnan(rho)) else float(rho)
    return pd.Series(scores).sort_values(ascending=False)

def model_importance(df_feat: pd.DataFrame, target: pd.Series, top_n: int = 128) -> pd.Series:
    if not HAVE_SKL:
        return pd.Series(dtype=float)
    cols = [c for c in df_feat.columns if c not in BASE_COLS + ['close_log','close_norm']]
    X = df_feat[cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).values
    y = target.replace([np.inf,-np.inf], np.nan).fillna(0.0).values
    if len(cols) == 0 or len(df_feat) < 1000:
        return pd.Series(dtype=float)
    model = LassoCV(cv=5, n_jobs=None, max_iter=10000).fit(X, y)
    imp = pd.Series(np.abs(model.coef_), index=cols)
    return imp.sort_values(ascending=False).head(top_n)

def select_topK(ic: pd.Series, imp: pd.Series | None, K: int = 96) -> list[str]:
    if imp is None or imp.empty:
        return list(ic.head(K).index)
    # fuzja rankingów (simple rank sum)
    ic_rank = ic.rank(ascending=False, method='dense')
    imp = imp.reindex(ic.index).fillna(0.0)
    imp_rank = imp.rank(ascending=False, method='dense')
    score = 1.0/ic_rank + 1.0/(imp_rank.replace(0, np.nan))
    return list(score.sort_values(ascending=False).head(K).index)

def apply_change_budget(prev_cols: list[str], new_cols: list[str], change_frac: float = 0.1) -> list[str]:
    if not prev_cols:
        return new_cols
    K = len(prev_cols)
    max_changes = max(1, int(round(K * change_frac)))
    keep = [c for c in new_cols if c in prev_cols]
    add  = [c for c in new_cols if c not in prev_cols][:max_changes]
    # Usuń najgorsze z poprzednich, żeby zwolnić miejsca
    drop_candidates = [c for c in prev_cols if c not in new_cols]
    drop = drop_candidates[:max_changes]
    # Final: (prev - drop) + add (z zachowaniem kolejności now_cols)
    result = [c for c in prev_cols if c not in drop]
    for c in new_cols:
        if c not in result and c in add:
            result.append(c)
    # Zabezpieczenie na dokładny rozmiar
    if len(result) > K:
        result = result[:K]
    elif len(result) < K:
        # dopełnij z 'keep' na wszelki wypadek
        for c in keep:
            if c not in result:
                result.append(c)
            if len(result) >= K:
                break
    return result

def write_features_spec(path_json: str, feat_cols: list[str], price_col: str = "close_norm") -> None:
    Path(path_json).parent.mkdir(parents=True, exist_ok=True)
    with open(path_json, "w", encoding="utf-8") as f:
        json.dump({"feature_columns": feat_cols, "price_column": price_col}, f, ensure_ascii=False, indent=2)

def main_discover(features_csv: str,
                  out_json: str = "models/features_spec.json",
                  topK: int = 96,
                  change_frac: float = 0.10):
    df = pd.read_csv(features_csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
    target = compute_targets(df)
    ic = rolling_ic(df, target)
    imp = model_importance(df, target, top_n=topK*2)  # opcjonalne
    proposal = select_topK(ic, imp, K=topK)

    prev = []
    p = Path(out_json)
    if p.exists():
        prev = json.loads(p.read_text(encoding='utf-8')).get("feature_columns", [])
        # nie bierzemy kolumn, których już fizycznie nie ma:
        prev = [c for c in prev if c in df.columns]

    final_cols = apply_change_budget(prev, proposal, change_frac=change_frac)
    write_features_spec(out_json, final_cols, "close_norm")
    print(f"[discover] Wrote {out_json} with {len(final_cols)} features.")

=== FILE: features\fundamentals.py ===

# -*- coding: utf-8 -*-
"""
features/fundamentals.py
Fundamenty dla XAUUSD: realne stopy (DGS10 - T10YIE), USD index (np. FRED),
GPR (Caldara & Iacoviello), COT Gold (CFTC). As-of alignment i FFill -> M5.

Wymaga: pandas, numpy, (opcjonalnie) fredapi, requests, python-dotenv.
Źródła:
- FRED: DGS10, T10YIE (real 10y = DGS10 - T10YIE)      [St. Louis Fed]  # cite: turn2search20
- CFTC COT (COMEX Gold) weekly                          # cite: turn2search9
- GPR (Caldara & Iacoviello) monthly                    # cite: turn2search28
"""

from __future__ import annotations
import os, json
import pandas as pd, numpy as np
from datetime import datetime, timezone, timedelta
from pathlib import Path

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# ---------- Utils ----------

def _zscore(s: pd.Series, win: int = 252) -> pd.Series:
    mu = s.rolling(win, min_periods=max(10, win//10)).mean()
    sd = s.rolling(win, min_periods=max(10, win//10)).std().replace(0, np.nan)
    return (s - mu) / (sd + 1e-12)

def _ensure_dt_utc(s: pd.Series) -> pd.Series:
    if not isinstance(s.index, pd.DatetimeIndex):
        raise ValueError("Index must be DatetimeIndex")
    if s.index.tz is None:
        s.index = s.index.tz_localize("UTC")
    else:
        s.index = s.index.tz_convert("UTC")
    return s

def _asof_ffill_daily_to_m5(daily: pd.Series,
                            bars_m5: pd.DataFrame,
                            available_hour_utc: int = 23,
                            available_minute_utc: int = 59) -> pd.Series:
    """
    Dla wartości dziennych zakładamy, że 'stają się znane' o available_* UTC.
    Każda wartość jest użyta od (data, available_time) do kolejnej publikacji.
    To zachowuje konserwatywność (eliminuje look-ahead).
    """
    s = daily.copy()
    s = _ensure_dt_utc(s)
    # Utnij do zakresu świec
    s = s[(s.index <= bars_m5['time'].max())]
    if s.empty:
        return pd.Series(index=bars_m5.index, dtype=float)

    # Tworzymy serię znaczników as-of
    idx_asof = []
    vals = []
    for ts, val in s.items():
        ts_asof = pd.Timestamp(ts.date(), tz=timezone.utc) + timedelta(
            hours=available_hour_utc, minutes=available_minute_utc)
        idx_asof.append(ts_asof)
        vals.append(val)
    asof_series = pd.Series(vals, index=pd.DatetimeIndex(idx_asof, tz="UTC")).sort_index()

    # Reindeks do M5 time i ffill
    m5_idx = pd.DatetimeIndex(bars_m5['time']).tz_convert("UTC")
    out = asof_series.reindex(m5_idx, method="ffill")
    out.name = daily.name
    return out

# ---------- FRED ----------

def load_fred_series(series_id: str, api_key: str | None) -> pd.Series:
    """
    Ładuje serię z FRED (jeśli brak fredapi/klucza -> spodziewa się CSV w external/fred/{series_id}.csv)
    CSV format: DATE,VALUE
    """
    try:
        from fredapi import Fred  # type: ignore
    except Exception:
        Fred = None

    if Fred is not None and api_key:
        fred = Fred(api_key=api_key)
        s = fred.get_series(series_id)
        s = s.dropna().astype(float)
        s.index = pd.to_datetime(s.index, utc=True)
        s.name = series_id
        return s

    # Fallback: lokalny CSV
    p = Path("external/fred") / f"{series_id}.csv"
    if not p.exists():
        raise FileNotFoundError(f"Brak {p}. Zainstaluj fredapi lub umieść CSV.")
    df = pd.read_csv(p)
    df['DATE'] = pd.to_datetime(df['DATE'], utc=True)
    s = pd.Series(df.iloc[:, 1].astype(float).values, index=df['DATE'], name=series_id)
    return s

def build_real10y_and_usd(bars_m5: pd.DataFrame,
                          fred_key: str | None,
                          usd_series_id: str = "DTWEXBGS",
                          real_asof_hh: int = 23,
                          real_asof_mm: int = 59) -> pd.DataFrame:
    """
    Zwraca DataFrame z kolumnami: real10y, real10y_z, d_real10y, usd, usd_z
    - real10y = DGS10 - T10YIE (oba dzienne)  # cite: turn2search20
    - usd = indeks USD (np. DTWEXBGS z FRED)  # cite: turn2search25
    """
    dgs10 = load_fred_series("DGS10", fred_key)
    t10yie = load_fred_series("T10YIE", fred_key)
    real10y_daily = (dgs10 - t10yie).dropna()
    usd_daily = load_fred_series(usd_series_id, fred_key).dropna()

    real10y_m5 = _asof_ffill_daily_to_m5(real10y_daily, bars_m5, real_asof_hh, real_asof_mm)
    usd_m5     = _asof_ffill_daily_to_m5(usd_daily,     bars_m5, 23, 59)

    # pochodne
    real10y_z = _zscore(real10y_m5, 252*24*12//5)  # ~252 dni w M5 ≈ 12*24*252/5
    d_real10y = real10y_m5.diff()

    usd_z = _zscore(usd_m5, 252*24*12//5)

    out = pd.DataFrame({
        'real10y': real10y_m5.values,
        'real10y_z': real10y_z.values,
        'd_real10y': d_real10y.values,
        'usd': usd_m5.values,
        'usd_z': usd_z.values
    }, index=bars_m5.index)
    return out

# ---------- GPR ----------

def load_gpr_local_csv(path: str = "external/gpr/gpr.csv") -> pd.Series:
    """
    Oczekuje miesięcznego GPR z kolumnami ['date','gpr'], gdzie date to YYYY-MM-01 UTC.
    Oryginał i metodologia: Caldara & Iacoviello (AER 2022)                      # cite: turn2search28
    Pobrane dane umieść offline – unikamy look-ahead przez FFill miesięczny.
    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Brak {p}. Pobierz GPR i zapisz jako CSV.")
    df = pd.read_csv(p)
    df['date'] = pd.to_datetime(df['date'], utc=True)
    s = pd.Series(df['gpr'].astype(float).values, index=df['date'], name='gpr')
    return s

def gpr_to_m5(bars_m5: pd.DataFrame, gpr_monthly: pd.Series) -> pd.Series:
    gpr_monthly = _ensure_dt_utc(gpr_monthly)
    # As-of: konserwatywnie początek kolejnego miesiąca 00:00 UTC
    idx_asof = [pd.Timestamp(ts.year + (1 if ts.month==12 else 0),
                             1 if ts.month==12 else ts.month+1, 1, tz="UTC") for ts in gpr_monthly.index]
    s_asof = pd.Series(gpr_monthly.values, index=pd.DatetimeIndex(idx_asof, tz="UTC")).sort_index()
    m5_idx = pd.DatetimeIndex(bars_m5['time']).tz_convert('UTC')
    gpr_m5 = s_asof.reindex(m5_idx, method='ffill')
    gpr_m5.name = 'gpr'
    return gpr_m5

# ---------- COT (Gold) ----------

def load_cot_gold_local_csv(path: str = "external/cot_gold.csv") -> pd.DataFrame:
    """
    Oczekuje tygodniowego COT dla Gold (COMEX): kolumny ['date','mm_long','mm_short'] dla Managed Money.
    Dane z CFTC (Disaggregated report).                               # cite: turn2search9
    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Brak {p}. Przygotuj eksport z CFTC i zapisz CSV.")
    df = pd.read_csv(p)
    df['date'] = pd.to_datetime(df['date'], utc=True)
    return df[['date','mm_long','mm_short']].sort_values('date')

def cot_to_m5(bars_m5: pd.DataFrame, cot_df: pd.DataFrame) -> pd.DataFrame:
    cot_df = cot_df.copy()
    cot_df['mm_net'] = cot_df['mm_long'].astype(float) - cot_df['mm_short'].astype(float)
    # Publikacja: piątek (as-of), dane na wtorek – używamy konserwatywnie od piątku 21:00 UTC
    asof_ts = cot_df['date'] + pd.Timedelta(days=3)  # wtorek -> piątek
    asof_ts = asof_ts.dt.tz_localize('UTC') + pd.Timedelta(hours=21)
    ser = pd.Series(cot_df['mm_net'].values, index=asof_ts, name='cot_mm_net').sort_index()

    m5_idx = pd.DatetimeIndex(bars_m5['time']).tz_convert("UTC")
    mm_net_m5 = ser.reindex(m5_idx, method='ffill')
    mm_net_z  = _zscore(mm_net_m5, 52)  # roczny tygodniowy zscore
    out = pd.DataFrame({'cot_mm_net': mm_net_m5.values, 'cot_mm_net_z': mm_net_z.values}, index=bars_m5.index)
    return out

# ---------- Orkiestracja ----------

def build_fundamentals_features(bars_csv: str,
                                fred_api_key: str | None,
                                gpr_csv: str | None = None,
                                cot_csv: str | None = None,
                                usd_series_id: str = "DTWEXBGS") -> pd.DataFrame:
    """
    Ładuje bars M5, dokleja fundamenty (real10y/usd/gpr/cot) jako cechy na indeksie świec M5.
    """
    bars = pd.read_csv(bars_csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
    core = build_real10y_and_usd(bars, fred_api_key, usd_series_id)

    cols = [core]
    if gpr_csv:
        gpr = load_gpr_local_csv(gpr_csv)
        gpr_m5 = gpr_to_m5(bars, gpr)
        cols.append(pd.DataFrame({'gpr': gpr_m5.values, 'gpr_z': _zscore(gpr_m5, 36).values}, index=core.index))
    if cot_csv:
        cot = load_cot_gold_local_csv(cot_csv)
        cols.append(cot_to_m5(bars, cot))

    out = pd.concat(cols, axis=1)
    out.insert(0, 'time', bars['time'].values)
    return out

=== FILE: features\__init__.py ===



=== FILE: ops\feature_discovery_weekly.py ===

# -*- coding: utf-8 -*-
"""
ops/feature_discovery_weekly.py
Tygodniowy job: na podstawie aktualnego FEAT wybiera top-K cech (IC + opcjonalnie model),
respektuje budżet zmian (+/- change_frac), aktualizuje models/features_spec.json.

Uruchamiaj np. w niedzielę 21:00 UTC. Po sukcesie -> pełny nightly train (bo zmienia się obs_dim).
"""
import argparse, yaml, json
from pathlib import Path
import pandas as pd
from features.discover import main_discover

parser = argparse.ArgumentParser()
parser.add_argument('--features_csv', default=None, help='Ścieżka do CSV z cechami; domyślnie z config.yaml')
parser.add_argument('--topK', type=int, default=96)
parser.add_argument('--change_frac', type=float, default=0.10)
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = args.features_csv or cfg['files']['features_csv']
OUT_JSON = 'models/features_spec.json'

def main():
    if not Path(FEAT).exists():
        raise SystemExit(f"[feature_discovery] Brak {FEAT}. Najpierw uruchom features/build_features.py")
    main_discover(FEAT, OUT_JSON, topK=args.topK, change_frac=args.change_frac)

if __name__ == '__main__':
    main()
``

=== FILE: ops\mini_update_ppo.py ===

# -*- coding: utf-8 -*-
"""
ops/mini_update_ppo.py
Mini-update PPO co 6h: warm-start z championa, rolling window danych (ostatnie N dni),
krÃ³tszy trening, mniejszy LR/clip_range. Po treningu -> zapis modelu i vecnorm -> walidacja -> save_candidate/promote.
"""

import argparse, yaml, json
import pandas as pd
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor, sync_envs_normalization
from gymnasium.wrappers import TimeLimit
from env_xau import XauTradingEnv

parser = argparse.ArgumentParser()
parser.add_argument('--timesteps', type=int, default=300000)
parser.add_argument('--train_days', type=int, default=120)
parser.add_argument('--val_days', type=int, default=30)
parser.add_argument('--seed', type=int, default=42)
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = Path(cfg['files']['model_path'])
VECNORM_PATH = Path(cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl'))
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))

df = pd.read_csv(FEAT, parse_dates=['time']).sort_values('time').reset_index(drop=True)
end_time = df['time'].max()
train_start = end_time - pd.Timedelta(days=args.train_days + args.val_days)
train_end = end_time - pd.Timedelta(days=args.val_days)
val_start = train_end
train_df = df[(df['time']>=train_start)&(df['time']<train_end)].copy()
val_df = df[(df['time']>=val_start)&(df['time']<=end_time)].copy()

features_spec=None
fspec = Path('models/features_spec.json')
if fspec.exists():
    features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)

def make_env(sub_df):
    return TimeLimit(XauTradingEnv(sub_df, window=WINDOW,
                                   spread_abs=COSTS['spread_abs'],
                                   commission_rate=COSTS['commission_rate'],
                                   slippage_k=COSTS['slippage_k'],
                                   reward_mode=REWARD_MODE,
                                   use_close_norm=True,
                                   flip_penalty=FLIP_PENALTY,
                                   trade_hours_utc=TRADE_HOURS,
                                   enforce_flat_outside_hours=True,
                                   features_spec=features_spec,
                                   min_equity=MIN_EQ),
                     max_episode_steps=6000)

venv_train = DummyVecEnv([lambda: make_env(train_df)])
venv_train = VecMonitor(venv_train)
venv_train = VecNormalize(venv_train, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)

venv_eval = DummyVecEnv([lambda: make_env(val_df)])
venv_eval = VecMonitor(venv_eval)
venv_eval = VecNormalize(venv_eval, training=False, norm_obs=True, norm_reward=False)

# Warm-start z championa
model = PPO.load(str(MODEL_PATH), env=venv_train)
model.learning_rate = 3e-5
model.clip_range = 0.1

sync_envs_normalization(venv_eval, venv_train)

print(f"[mini-update] Training on {len(train_df)} rows (~{args.train_days}d), timesteps={args.timesteps}")
model.learn(total_timesteps=args.timesteps)
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
model.save(str(MODEL_PATH))
venv_train.save(str(VECNORM_PATH))
print(f"[mini-update] Saved model and VecNormalize.")

# Walidacja + raport
import subprocess
subprocess.run(["python","rl/evaluate.py","--out_dir","reports"])
subprocess.run(["python","utils/make_report.py"])
subprocess.run(["python","ops/save_candidate.py"])
subprocess.run(["python","ops/promote_challenger.py"])

=== FILE: ops\promote_challenger.py ===

# ops/promote_challenger.py
import json, shutil
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
REG  = ROOT / 'models' / 'registry'
REG.mkdir(parents=True, exist_ok=True)

CUR_TXT  = REG / 'current.txt'
MODEL_DST = ROOT / 'models' / 'ppo_xauusd_m5.zip'
VEC_DST   = ROOT / 'models' / 'vecnorm_xauusd_m5.pkl'

def load_metrics(d: Path):
    if not d: return None
    m = d / 'metrics.json'
    return json.loads(m.read_text('utf-8')) if m.exists() else None

def pick_latest_dir():
    dirs = [p for p in REG.iterdir() if p.is_dir()]
    return sorted(dirs, key=lambda p: p.name)[-1] if dirs else None

def better(m_new, m_old):
    if m_old is None: return True
    s_new, s_old = (m_new.get('sharpe') or 0.0), (m_old.get('sharpe') or 0.0)
    dd_new, dd_old = (m_new.get('max_dd') or -1.0), (m_old.get('max_dd') or -1.0)
    fe_new, fe_old = (m_new.get('final_equity') or 0.0), (m_old.get('final_equity') or 0.0)
    dd_ok = (dd_new >= dd_old * 1.2)
    s_ok  = (s_new >= s_old * 1.10) or (s_new >= 0.10 and s_new > s_old)
    if s_new == 0.0 and s_old == 0.0:
        return (fe_new > fe_old) and dd_ok
    return s_ok and dd_ok

def promote(new_dir: Path):
    REG.mkdir(parents=True, exist_ok=True)
    CUR_TXT.write_text(new_dir.name, encoding='utf-8')
    shutil.copy2(new_dir/'model.zip', MODEL_DST)
    if (new_dir/'vecnorm.pkl').exists():
        shutil.copy2(new_dir/'vecnorm.pkl', VEC_DST)
    print(f"[promote] Champion -> {new_dir.name}")

def main():
    latest = pick_latest_dir()
    if latest is None:
        print('[promote] No candidates.'); return
    m_new = load_metrics(latest)
    if m_new is None:
        print('[promote] Latest has no metrics.json'); return
    cur_dir = (REG / CUR_TXT.read_text('utf-8').strip()) if CUR_TXT.exists() else None
    m_old = load_metrics(cur_dir) if cur_dir and cur_dir.exists() else None
    if better(m_new, m_old):
        promote(latest)
    else:
        print('[promote] Challenger not better. No promotion.')

if __name__ == '__main__':
    main()

=== FILE: ops\save_candidate.py ===

# ops/save_candidate.py
import json, shutil
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
REG  = ROOT / 'models' / 'registry'
REG.mkdir(parents=True, exist_ok=True)

MODEL = ROOT / 'models' / 'ppo_xauusd_m5.zip'
VEC   = ROOT / 'models' / 'vecnorm_xauusd_m5.pkl'
METR  = ROOT / 'reports' / 'val_metrics.txt'
EQP   = ROOT / 'reports' / 'equity_val.png'

def parse_metrics_txt(p: Path):
    d = {}
    if p.exists():
        txt = p.read_text(encoding='utf-8')
        for line in txt.splitlines():
            if ':' in line:
                k, v = line.split(':', 1)
                k = k.strip().lower().replace(' ', '')
                try:
                    d[k] = float(v.strip())
                except:
                    pass
    return d

def main():
    m = parse_metrics_txt(METR)
    if not m:
        print('[save_candidate] No metrics.')
        return
    tag = 'cand_'
    n = 0
    while (REG / f"{tag}{n:02d}").exists():
        n += 1
    dst = REG / f"{tag}{n:02d}"
    dst.mkdir(parents=True, exist_ok=True)
    shutil.copy2(MODEL, dst/'model.zip')
    if VEC.exists():
        shutil.copy2(VEC, dst/'vecnorm.pkl')
    if EQP.exists():
        shutil.copy2(EQP, dst/'equity.png')
    (dst/'metrics.json').write_text(json.dumps({
        'final_equity': m.get('final_equity'),
        'max_dd': m.get('maxdrawdown'),
        'sharpe': m.get('sharpe_daily'),
    }, indent=2), encoding='utf-8')
    print(f'[save_candidate] Saved candidate -> {dst}')

if __name__ == '__main__':
    main()


=== FILE: paper_demo\paper_loop_mt5_demo.py ===

# -*- coding: utf-8 -*-
import MetaTrader5 as mt5
import pandas as pd, numpy as np, yaml, time, logging, json
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from utils.mt5_health import ensure_mt5_ready

logging.basicConfig(filename='paper_demo/paper_trading.log', level=logging.INFO,
                    format='%(asctime)s %(levelname)s %(message)s')

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
SYMBOL = cfg['symbol']; WINDOW=int(cfg.get('window',128))
MODEL_PATH = cfg['files']['model_path']; VECNORM_PATH = cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl')
TF_MAP={'M1': mt5.TIMEFRAME_M1,'M5': mt5.TIMEFRAME_M5,'M15': mt5.TIMEFRAME_M15}; TF_NAME = cfg.get('timeframe','M5'); TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)

COSTS = cfg['costs']; SPREAD_ABS=float(COSTS['spread_abs']); COMM=float(COSTS['commission_rate']); SLIP_K=float(COSTS['slippage_k'])
ENV_CFG = cfg.get('env',{}); TRADE_HOURS = ENV_CFG.get('trade_hours_utc', None); ENF_FLAT = bool(ENV_CFG.get('enforce_flat_outside_hours', True))

def in_trade_hours(ts_utc) -> bool:
    if not TRADE_HOURS or len(TRADE_HOURS)!=2: return True
    s = [int(x) for x in str(TRADE_HOURS[0]).split(':')]; e=[int(x) for x in str(TRADE_HOURS[1]).split(':')]
    from datetime import time as dtime
    start=dtime(s[0], s[1] if len(s)>1 else 0); end=dtime(e[0], e[1] if len(e)>1 else 0)
    t = ts_utc.to_pydatetime().time()
    if start <= end: return (t>=start) and (t<=end)
    return (t>=start) or (t<=end)

features_spec=None; fs=Path('models/features_spec.json')
if fs.exists(): features_spec=json.loads(fs.read_text(encoding='utf-8')).get('feature_columns', None)

def add_features_incremental(df: pd.DataFrame) -> pd.DataFrame:
    # tu możesz wywołać dokładnie to, co w build_features (wersja light, bez fundamentów)
    c=df['close']
    df['ret1']=np.log(c).diff()
    df['ema10']=c.ewm(span=10).mean(); df['ema50']=c.ewm(span=50).mean(); df['ema200']=c.ewm(span=200).mean()
    d=c.diff(); up=d.clip(lower=0).ewm(alpha=1/14,adjust=False).mean(); down=(-d.clip(upper=0)).ewm(alpha=1/14,adjust=False).mean(); rs=up/(down+1e-12)
    df['rsi14']=100-(100/(1+rs))
    h,l,cl=df['high'],df['low'],df['close']; tr=np.maximum(h-l, np.maximum(abs(h-cl.shift()), abs(l-cl.shift())))
    df['atr14']=tr.ewm(alpha=1/14,adjust=False).mean()
    ema_fast=c.ewm(span=12,adjust=False).mean(); ema_slow=c.ewm(span=26,adjust=False).mean(); macd_line=ema_fast-ema_slow; signal_line=macd_line.ewm(span=9,adjust=False).mean()
    df['macd']=macd_line; df['macd_signal']=signal_line; df['macd_hist']=macd_line-signal_line
    ma=c.rolling(20).mean(); sd=c.rolling(20).std()
    df['bb_ma']=ma; df['bb_up']=ma+2*sd; df['bb_lo']=ma-2*sd; df['bb_width']=(df['bb_up']-df['bb_lo'])/(ma.replace(0,np.nan).abs()+1e-12)
    df['minute']=df['time'].dt.hour*60+df['time'].dt.minute; df['tod_sin']=np.sin(2*np.pi*df['minute']/1440); df['tod_cos']=np.cos(2*np.pi*df['minute']/1440); df.drop(columns=['minute'], inplace=True)
    df['dow']=df['time'].dt.dayofweek; df['dow_sin']=np.sin(2*np.pi*df['dow']/7); df['dow_cos']=np.cos(2*np.pi*df['dow']/7); df.drop(columns=['dow'], inplace=True)
    df['close_log']=np.log(c.clip(lower=1e-12)); mu=df['close_log'].rolling(2000, min_periods=200).mean(); s=df['close_log'].rolling(2000, min_periods=200).std().replace(0,np.nan)
    df['close_norm']=(df['close_log']-mu)/(s+1e-8)
    norm=['ret1','ema10','ema50','ema200','rsi14','atr14','macd','macd_signal','macd_hist','bb_ma','bb_up','bb_lo','bb_width','tod_sin','tod_cos','dow_sin','dow_cos']
    for c_ in norm:
        mu=df[c_].rolling(2000,min_periods=200).mean(); s=df[c_].rolling(2000,min_periods=200).std().replace(0,np.nan); df[c_]=(df[c_]-mu)/(s+1e-8)
    return df.dropna().reset_index(drop=True)

def get_last_bars(symbol, timeframe, n:int):
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, n)
    if rates is None or len(rates)<n: return None
    df=pd.DataFrame(rates); df['time']=pd.to_datetime(df['time'], unit='s', utc=True); df.rename(columns={'real_volume':'tick_volume'}, inplace=True)
    return df[['time','open','high','low','close','tick_volume','spread']]

def build_obs(df_feat: pd.DataFrame, model_obs_dim: int, pos: int, entry: float) -> np.ndarray:
    per_step=(model_obs_dim-2)//WINDOW
    price_col='close_norm' if 'close_norm' in df_feat.columns else 'close'
    if features_spec is None:
        base=['open','high','low','close','tick_volume','spread','time']; feat_cols=[c for c in df_feat.columns if c not in base]
    else:
        feat_cols=[c for c in features_spec if c in df_feat.columns]
    tail=df_feat.tail(WINDOW); block=tail[[price_col]+feat_cols].to_numpy(dtype=np.float32)
    if block.shape!=(WINDOW, per_step): raise RuntimeError(f"Bad block {block.shape} vs {(WINDOW, per_step)}")
    flat=block.flatten()
    price=float(df_feat['close'].iloc[-1]); unreal=0.0
    if pos!=0 and not np.isnan(entry):
        dir_=1 if pos>0 else -1; unreal=dir_*(price-entry)
    return np.concatenate([flat, np.array([pos, unreal],dtype=np.float32)])

def main():
    if not ensure_mt5_ready(): raise RuntimeError("MT5 not ready (terminal/account). Start MT5 and login to a demo account.")
    try:
        model = PPO.load(MODEL_PATH); expected_dim=int(model.observation_space.shape[0]); assert (expected_dim-2)%WINDOW==0
        vecnorm=None
        if Path(VECNORM_PATH).exists():
            from gymnasium import spaces
            class _ObsOnlyEnv:
                def __init__(self, obs_dim): self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32); self.action_space = spaces.Discrete(3)
                def reset(self, *, seed=None, options=None): import numpy as np; return np.zeros(self.observation_space.shape, dtype=np.float32), {}
                def step(self, action): import numpy as np; return np.zeros(self.observation_space.shape, dtype=np.float32), 0.0, True, False, {}
            dummy=DummyVecEnv([lambda: _ObsOnlyEnv(expected_dim)])
            vecnorm=VecNormalize.load(VECNORM_PATH, dummy); vecnorm.training=False; vecnorm.norm_reward=False

        Path('paper_demo').mkdir(parents=True, exist_ok=True)
        csv=Path('paper_demo/decisions.csv'); 
        if not csv.exists(): csv.write_text('time_utc,price,action_id,action_label,pos,equity\n', encoding='utf-8')

        last_ts=None; hb_t=time.time()
        # Stan pozycji (jak w env)
        pos=0; entry=np.nan; equity=1.0; prev_price=np.nan

        while True:
            bars=get_last_bars(SYMBOL, TF, n=WINDOW+800)
            if bars is None or len(bars)<WINDOW+200: time.sleep(5); continue
            feat=add_features_incremental(bars)
            if len(feat)<WINDOW: time.sleep(5); continue

            cur_ts=feat['time'].iloc[-1]
            if last_ts is not None and cur_ts==last_ts: time.sleep(2); continue

            # Godziny handlu / enforce flat
            inside = in_trade_hours(cur_ts)
            desired_mask=True
            # Zbuduj obserwację (z aktualnym pos/unreal)
            obs=build_obs(feat, expected_dim, pos=pos, entry=entry)
            if vecnorm is not None:
                import numpy as np
                obs=vecnorm.normalize_obs(obs.reshape(1,-1)).reshape(-1)
            action,_=model.predict(obs, deterministic=True)
            desired={0:-1,1:0,2:1}[int(action)]
            if not inside and ENF_FLAT: desired=0

            price=float(feat['close'].iloc[-1]); prev=float(feat['close'].iloc[-2]); slip=SLIP_K*abs(price-prev)

            # Zmiana pozycji i koszty (jak w env)
            if desired!=pos:
                # koszt zamknięcia starej
                if pos!=0 and not np.isnan(entry):
                    cost=(SPREAD_ABS+slip)/max(price,1e-12)+COMM
                    equity=max(equity*(1.0-cost), 1e-6)
                pos=desired
                if pos!=0:
                    cost=(SPREAD_ABS+slip)/max(price,1e-12)+COMM
                    equity=max(equity*(1.0-cost), 1e-6)
                    entry=price
                else:
                    entry=np.nan

            # Zmiana equity w kroku (pct mode, jak w env)
            if pos!=0:
                dir_=1 if pos>0 else -1
                step_ret = dir_ * ((price / max(prev,1e-12)) - 1.0)
                equity = equity * (1.0 + step_ret)

            decision={-1:'SHORT',0:'FLAT',1:'LONG'}[int(pos)]
            msg=f"DECISION {decision} @ {price:.2f} (ts={cur_ts}) equity={equity:.6f}"
            print(msg); logging.info(msg)
            with open(csv,'a',encoding='utf-8') as f: f.write(f"{cur_ts},{price:.5f},{int(action)},{decision},{pos},{equity:.6f}\n")
            last_ts=cur_ts
            if time.time()-hb_t>600: logging.info(f"[HB] {SYMBOL} {TF_NAME} last_ts={cur_ts} price={price:.2f} pos={pos} equity={equity:.6f}"); hb_t=time.time()
            time.sleep(30)
    finally:
        mt5.shutdown()

if __name__ == '__main__':
    main()
``


=== FILE: paper_demo\simulate_execution.py ===

# -*- coding: utf-8 -*-
import yaml, pandas as pd
from env_xau import XauTradingEnv
from stable_baselines3 import PPO
from pathlib import Path

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = cfg['files']['model_path']
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']

df = pd.read_csv(FEAT, parse_dates=['time'])
cut = df['time'].max() - pd.Timedelta(days=120)
sim = df[df['time']>=cut].copy()

env = XauTradingEnv(sim, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'])
obs, info = env.reset(); model = PPO.load(MODEL_PATH)
rows=[]; done=False
while not done:
    action,_=model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    done = bool(terminated) or bool(truncated)
    rows.append({'time': info.get('time',None), 'price': info.get('price',None), 'action': int(action), 'reward': float(reward), 'equity': float(env.equity)})
Path('reports').mkdir(parents=True, exist_ok=True)
pd.DataFrame(rows).to_csv('reports/trace_simulated.csv', index=False)
pd.Series([r['equity'] for r in rows]).to_csv('reports/equity_simulated.csv', index=False)
print('Saved simulated trace/equity.')


=== FILE: paper_demo\__init__.py ===



=== FILE: reports\costs_suggestion.yaml ===

spread_abs_median: 0.42
spread_abs_p95: 0.62
slippage_k_suggested: 0.0


=== FILE: reports\report.txt ===

# RL Validation Report (XAUUSD M5)
Date: 2025-11-04 21:19:36

## Base metrics

Final equity: 1.000000
Min equity: 0.993288
Max drawdown: -0.067083


## Extra metrics from equity
- Steps: 3000
- Sharpe (daily): 0.010
- Sortino (daily): 0.007
- Daily vol: 0.0210
- CAGR (est.): -0.269%
- MaxDD: -6.71%
- Calmar: -0.040


=== FILE: reports\val_metrics.txt ===

Final equity: 1.000000
Min equity: 0.993288
Max drawdown: -0.067083


=== FILE: rl\evaluate.py ===

# -*- coding: utf-8 -*-
import argparse, yaml, json
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from env_xau import XauTradingEnv

parser = argparse.ArgumentParser()
parser.add_argument('--max_eval_steps', type=int, default=3000)
parser.add_argument('--out_dir', type=str, default='reports')
args = parser.parse_args()

OUT = Path(args.out_dir); OUT.mkdir(parents=True, exist_ok=True)

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = cfg['files']['model_path']
VECNORM_PATH = cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl')
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))

df = pd.read_csv(FEAT, parse_dates=['time'])
cut_val = df['time'].max() - pd.Timedelta(days=120)
val = df[df['time']>=cut_val].copy()

features_spec = None
fspec = Path('models/features_spec.json')
if fspec.exists():
    features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)

def make_eval():
    env = XauTradingEnv(val, window=WINDOW,
        spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
        reward_mode=REWARD_MODE, use_close_norm=True, features_spec=features_spec, min_equity=MIN_EQ)
    return TimeLimit(env, max_episode_steps=args.max_eval_steps)

venv = DummyVecEnv([make_eval]); venv = VecMonitor(venv)
if Path(VECNORM_PATH).exists():
    venv = VecNormalize.load(VECNORM_PATH, venv); venv.training=False; venv.norm_reward=False
model = PPO.load(MODEL_PATH, env=venv)

obs = venv.reset(); eq=[]; rows=[]; done=False
while not done:
    action,_ = model.predict(obs, deterministic=True)
    obs, rewards, dones, infos = venv.step(action)
    equity = venv.get_attr('equity', indices=0)[0]
    eq.append(float(equity))
    info0 = infos[0] if isinstance(infos,(list,tuple)) else infos
    rows.append({'time': str(info0.get('time','')), 'price': info0.get('price',np.nan), 'pos': info0.get('pos',np.nan),
                 'equity': equity, 'action': int(action[0]) if hasattr(action,'__len__') else int(action)})
    done = bool(dones[0])

s = pd.Series(eq)
final_eq = float(s.iloc[-1]); min_eq=float(s.min()); peak=s.cummax(); max_dd=float((s/peak-1.0).min())
plt.figure(figsize=(10,4)); plt.plot(s.values); plt.title('Equity (validation)'); plt.tight_layout()
plt.savefig((OUT/'equity_val.png').as_posix(), dpi=150)
(OUT/'val_metrics.txt').write_text(
    f"Final equity: {final_eq:.6f}\nMin equity: {min_eq:.6f}\nMax drawdown: {max_dd:.6f}\n", encoding='utf-8')
pd.DataFrame(rows).to_csv(OUT/'eval_trace.csv', index=False)
print("Saved validation results.")


=== FILE: rl\train_ppo.py ===

# -*- coding: utf-8 -*-
import argparse, yaml, json
import pandas as pd
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor, sync_envs_normalization
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.utils import set_random_seed
from gymnasium.wrappers import TimeLimit
from env_xau import XauTradingEnv

parser = argparse.ArgumentParser()
parser.add_argument('--timesteps', type=int, default=1500000)
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--eval_freq', type=int, default=100000)
parser.add_argument('--n_eval_episodes', type=int, default=1)
parser.add_argument('--max_train_steps', type=int, default=6000)
parser.add_argument('--max_eval_steps', type=int, default=3000)
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = Path(cfg['files']['model_path'])
VECNORM_PATH = Path(cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl'))
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))

df = pd.read_csv(FEAT, parse_dates=['time'])
cut_val = df['time'].max() - pd.Timedelta(days=30)
train_df = df[df['time'] < cut_val].copy()
val_df   = df[df['time'] >= cut_val].copy()

features_spec = None
fspec = Path('models/features_spec.json')
if fspec.exists():
    features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)

set_random_seed(args.seed)

def make_train():
    env = XauTradingEnv(train_df, window=WINDOW,
        spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
        reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
        enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
    return TimeLimit(env, max_episode_steps=args.max_train_steps)

def make_eval():
    env = XauTradingEnv(val_df, window=WINDOW,
        spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
        reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
        enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
    return TimeLimit(env, max_episode_steps=args.max_eval_steps)

venv_train = DummyVecEnv([make_train]); venv_train = VecMonitor(venv_train); venv_train = VecNormalize(venv_train, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)
venv_eval  = DummyVecEnv([make_eval]);  venv_eval  = VecMonitor(venv_eval);  venv_eval  = VecNormalize(venv_eval, training=False, norm_obs=True, norm_reward=False)

policy_kwargs = dict(net_arch=dict(pi=[128,128], vf=[128,128]))
try:
    import tensorboard as _tb
    tb_log_dir = 'logs/ppo_gold_m5'
except Exception:
    tb_log_dir = None

model = PPO('MlpPolicy', venv_train, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02,
            policy_kwargs=policy_kwargs, seed=args.seed, verbose=1, tensorboard_log=tb_log_dir)

sync_envs_normalization(venv_eval, venv_train)
eval_cb = EvalCallback(venv_eval, best_model_save_path=str(MODEL_PATH.parent), log_path='logs/eval',
                       eval_freq=max(args.eval_freq,1), n_eval_episodes=max(args.n_eval_episodes,1),
                       deterministic=True, render=False)

model.learn(total_timesteps=args.timesteps, callback=eval_cb)
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
model.save(str(MODEL_PATH))
venv_train.save(str(VECNORM_PATH))
print(f"Saved model: {MODEL_PATH}")
print(f"Saved VecNormalize: {VECNORM_PATH}")


=== FILE: rl\walk_forward.py ===

# -*- coding: utf-8 -*-
import argparse, yaml, json
import pandas as pd, numpy as np
from pathlib import Path
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from env_xau import XauTradingEnv

parser = argparse.ArgumentParser()
parser.add_argument('--segments', type=int, default=6)
parser.add_argument('--train_days', type=int, default=120)
parser.add_argument('--val_days', type=int, default=30)
parser.add_argument('--timesteps', type=int, default=500000)
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--out_dir', type=str, default='reports_wf')
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))

out = Path(args.out_dir); out.mkdir(parents=True, exist_ok=True)
df = pd.read_csv(FEAT, parse_dates=['time']).sort_values('time').reset_index(drop=True)
end_time = df['time'].max()

features_spec=None
fspec = Path('models/features_spec.json')
if fspec.exists(): features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)

rows=[]
for k in range(args.segments):
    val_end = end_time - pd.Timedelta(days=k*args.val_days)
    val_start = val_end - pd.Timedelta(days=args.val_days)
    train_end = val_start
    train_start = train_end - pd.Timedelta(days=args.train_days)
    tr = df[(df['time']>=train_start)&(df['time']<train_end)].copy()
    va = df[(df['time']>=val_start)&(df['time']<val_end)].copy()
    if len(tr)<=WINDOW or len(va)<=WINDOW: continue

    def make_train():
        env = XauTradingEnv(tr, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
            reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
            enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
        return TimeLimit(env, max_episode_steps=6000)

    def make_eval():
        env = XauTradingEnv(va, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
            reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
            enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
        return TimeLimit(env, max_episode_steps=3000)

    vt = DummyVecEnv([make_train]); vt = VecMonitor(vt); vt = VecNormalize(vt, norm_obs=True, norm_reward=True)
    ve = DummyVecEnv([make_eval]);  ve = VecMonitor(ve)

    model = PPO('MlpPolicy', vt, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02, seed=args.seed, verbose=0)
    model.learn(total_timesteps=args.timesteps)

    tmp = out/f'vecnorm_{k}.pkl'; vt.save(str(tmp))
    ve = VecNormalize.load(str(tmp), ve); ve.training=False; ve.norm_reward=False

    obs = ve.reset(); eq=[]; trace=[]; done=False
    while not done:
        action,_=model.predict(obs, deterministic=True)
        obs, rewards, dones, infos = ve.step(action)
        equity = ve.get_attr('equity', indices=0)[0]
        eq.append(float(equity))
        info0=infos[0] if isinstance(infos,(list,tuple)) else infos
        trace.append({'k':k,'time':str(info0.get('time','')),'price':info0.get('price',np.nan),'pos':info0.get('pos',np.nan),'equity':equity,
                      'action':int(action[0]) if hasattr(action,'__len__') else int(action)})
        done = bool(dones[0])

    s = pd.Series(eq); final_eq=float(s.iloc[-1]); min_eq=float(s.min()); peak=s.cummax(); max_dd=float((s/peak-1.0).min())
    rows.append({'k':k,'train_start':train_start,'train_end':train_end,'val_start':val_start,'val_end':val_end,'final_eq':final_eq,'min_eq':min_eq,'max_dd':max_dd,'n_steps':int(len(s))})
    pd.DataFrame(trace).to_csv(out/f'fold_{k}_trace.csv', index=False)

res = pd.DataFrame(rows).sort_values('k'); res.to_csv(out/'wf_results.csv', index=False)
if res.empty:
    txt=['# Walk-Forward Report','No results (windows too short?).']
else:
    agg={'folds':len(res),'final_eq_med':float(res['final_eq'].median()),'max_dd_med':float(res['max_dd'].median()),'n_steps_sum':int(res['n_steps'].sum())}
    txt=['# Walk-Forward Report',f"Folds: {agg['folds']}",f"Median Final Equity: {agg['final_eq_med']:.4f}",f"Median MaxDD: {agg['max_dd_med']:.2%}",f"Total steps: {agg['n_steps_sum']}"]
(out/'wf_report.txt').write_text('\n'.join(txt), encoding='utf-8')
print("Saved walk-forward results.")


=== FILE: rl\__init__.py ===



=== FILE: utils\atomic.py ===

from pathlib import Path

def atomic_write_csv(df, path: str):
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    tmp = p.with_suffix(p.suffix + ".tmp")
    df.to_csv(tmp, index=False)
    tmp.replace(p)


=== FILE: utils\calibration.py ===

# utils/calibration.py
# -*- coding: utf-8 -*-
import argparse, yaml, pandas as pd, numpy as np
from pathlib import Path

parser = argparse.ArgumentParser()
parser.add_argument('--apply', action='store_true', help='Nadpisz config.yaml sugerowanymi kosztami')
parser.add_argument('--ticks_csv', type=str, default=None, help='Ścieżka do CSV z tickami (domyślnie z config.yaml)')
parser.add_argument('--bars_csv', type=str, default=None, help='Ścieżka do CSV ze świecami (domyślnie z config.yaml)')
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
TICKS = args.ticks_csv or cfg['files']['ticks_csv']
BARS  = args.bars_csv  or cfg['files']['bars_csv']

ticks = pd.read_csv(TICKS, parse_dates=['time'])
ticks['spread'] = ticks['ask'] - ticks['bid']
mid = (ticks['ask'] + ticks['bid']) / 2.0
dmid = mid.diff().abs()

spread_median = float(ticks['spread'].median())
spread_p95    = float(ticks['spread'].quantile(0.95))
bars = pd.read_csv(BARS, parse_dates=['time'])
bar_move_med = float(bars['close'].diff().abs().median())

# Slippage ~ median|Δmid| jako proporcja do mediany ruchu barowego (ucięta do sensownych widełek)
slippage_k = float(np.clip(dmid.median() / max(bar_move_med, 1e-12), 0.0, 0.5))

Path('reports').mkdir(parents=True, exist_ok=True)
Path('reports/costs_suggestion.yaml').write_text(
    yaml.safe_dump(
        {
            'spread_abs_median': round(spread_median, 5),
            'spread_abs_p95': round(spread_p95, 5),
            'slippage_k_suggested': round(slippage_k, 3),
        },
        allow_unicode=True, sort_keys=False
    ),
    encoding='utf-8'
)
print("Saved: reports/costs_suggestion.yaml")

if args.apply:
    cfg.setdefault('costs', {})
    cfg['costs']['spread_abs']   = round(spread_median, 5)
    cfg['costs']['slippage_k']   = round(slippage_k, 3)
    Path('config.yaml').write_text(yaml.safe_dump(cfg, allow_unicode=True, sort_keys=False), encoding='utf-8')
    print("Applied suggestions to config.yaml")

=== FILE: utils\make_report.py ===

# -*- coding: utf-8 -*-
import argparse, base64
from pathlib import Path
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

parser = argparse.ArgumentParser()
parser.add_argument('--equity_csv', type=str, default='reports/eval_trace.csv')
parser.add_argument('--metrics', type=str, default='reports/val_metrics.txt')
parser.add_argument('--figure', type=str, default='reports/equity_val.png')
parser.add_argument('--out_html', type=str, default='reports/report.html')
parser.add_argument('--out_txt', type=str, default='reports/report.txt')
parser.add_argument('--bars_per_day', type=int, default=288)
parser.add_argument('--trading_days', type=int, default=252)
args = parser.parse_args()

out_html = Path(args.out_html)
out_txt  = Path(args.out_txt)
out_html.parent.mkdir(parents=True, exist_ok=True)
out_txt.parent.mkdir(parents=True, exist_ok=True)

eq = None
p = Path(args.equity_csv)
if p.exists():
    df = pd.read_csv(p)
    if 'equity' in df.columns:
        eq = df['equity'].astype(float).to_numpy()
    elif df.shape[1] == 1:
        eq = df.iloc[:, 0].astype(float).to_numpy()

metrics_text = Path(args.metrics).read_text(encoding='utf-8') if Path(args.metrics).exists() else ''

img_b64 = ''
if Path(args.figure).exists():
    img_b64 = base64.b64encode(Path(args.figure).read_bytes()).decode('ascii')

# Jeśli nie ma obrazka, wygeneruj go z danych equity
if not img_b64 and eq is not None and len(eq) > 1:
    fig, ax = plt.subplots(figsize=(9, 4))
    ax.plot(eq, color='#0057B8', lw=1.5)
    ax.set_title('Equity (validation)')
    ax.set_xlabel('Steps')
    ax.set_ylabel('Equity')
    fig.tight_layout()
    Path(args.figure).parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(args.figure, dpi=144)
    plt.close(fig)
    img_b64 = base64.b64encode(Path(args.figure).read_bytes()).decode('ascii')

# Dodatkowe metryki
extra = {}
if eq is not None and len(eq) > 2:
    ret = np.diff(eq) / np.maximum(eq[:-1], 1e-12)
    mu = float(np.nanmean(ret))
    sigma = float(np.nanstd(ret) + 1e-12)
    downside = ret[ret < 0]
    ds = float(np.nanstd(downside) + 1e-12)

    daily_mu = mu * args.bars_per_day
    daily_sigma = sigma * (args.bars_per_day ** 0.5)
    sharpe = daily_mu / (daily_sigma + 1e-12)
    sortino = daily_mu / (((args.bars_per_day ** 0.5) * ds) + 1e-12)

    n = len(eq)
    years = max(n / args.bars_per_day / args.trading_days, 1e-6)
    cagr = (eq[-1] / max(eq[0], 1e-12)) ** (1 / years) - 1 if years > 0 else 0.0
    peak = np.maximum.accumulate(eq)
    max_dd = float((eq / peak - 1.0).min())
    calmar = (cagr / abs(max_dd)) if max_dd < 0 else float('inf')

    extra = dict(
        n_steps=n,
        sharpe_daily=sharpe,
        sortino_daily=sortino,
        vol_daily=daily_sigma,
        cagr=cagr,
        max_dd=max_dd,
        calmar=calmar
    )

now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

md = [
    '# RL Validation Report (XAUUSD M5)',
    f'Date: {now}',
    '',
    '## Base metrics',
    '',
    metrics_text.strip() if metrics_text else '(no val_metrics.txt)',
    '',
    ''
]

if extra:
    md += [
        '## Extra metrics from equity',
        f"- Steps: {extra['n_steps']}",
        f"- Sharpe (daily): {extra['sharpe_daily']:.3f}",
        f"- Sortino (daily): {extra['sortino_daily']:.3f}",
        f"- Daily vol: {extra['vol_daily']:.4f}",
        f"- CAGR (est.): {extra['cagr']:.3%}",
        f"- MaxDD: {extra['max_dd']:.2%}",
        f"- Calmar: {extra['calmar']:.3f}",
    ]
else:
    md += ['No equity data.']

out_txt.write_text('\n'.join(md) + '\n', encoding='utf-8')

html = f"""RL Validation
RL Validation Report (XAUUSD M5)
Base metrics
{metrics_text if metrics_text else '(no val_metrics.txt)'}
Extra metrics
{('\n'.join(md)) if extra else 'No equity data.'}
Equity
{('data:image/png;base64,' + img_b64) if img_b64 else 'No plot.'}
Generated: {now}
"""
out_html.write_text(html, encoding='utf-8')
print('Saved report.')

=== FILE: utils\mt5_health.py ===

import MetaTrader5 as mt5
import time

def ensure_mt5_ready(retries=3, sleep_s=2):
    """Ponawia initialize i weryfikuje, że terminal + konto są gotowe."""
    for i in range(retries):
        if mt5.initialize():
            ti, ai = mt5.terminal_info(), mt5.account_info()
            if ti and ai and getattr(ai, "login", 0):
                return True
            mt5.shutdown()
        time.sleep(sleep_s * (i + 1))
    return False


=== FILE: utils\qc_bars.py ===

#!/usr/bin/env python
import pandas as pd, argparse
parser=argparse.ArgumentParser(); parser.add_argument('--csv', required=True); parser.add_argument('--tf_min', type=int, default=5)
args=parser.parse_args()
df=pd.read_csv(args.csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
print(f"Rows: {len(df)}\nFrom: {df['time'].iloc[0]} To: {df['time'].iloc[-1]}")
dt=(df['time'].diff().dt.total_seconds()/60).fillna(args.tf_min)
gaps=df.loc[dt>args.tf_min+0.1,['time']].copy(); gaps['gap_min']=dt[dt>args.tf_min+0.1].values
print(f"Gaps > {args.tf_min} min: {len(gaps)}");
if len(gaps): print(gaps.head(10))


=== FILE: utils\time_utils.py ===

from datetime import datetime, timezone
def now_utc(): return datetime.now(timezone.utc)


=== FILE: utils\__init__.py ===



=== FILE: _backup_py\env_xau.py ===

-- coding: utf-8 --
import numpy as np, pandas as pd
import gymnasium as gym
from gymnasium import spaces
from datetime import time as dtime
class XauTradingEnv(gym.Env):
metadata = {"render_modes": []}
def init(self, df: pd.DataFrame, window=128,
spread_abs=0.05, commission_rate=0.0001, slippage_k=0.10,
reward_mode: str = "pct", use_close_norm: bool = True,
flip_penalty: float = 0.0, trade_hours_utc=None,
enforce_flat_outside_hours: bool = True, features_spec: list | None = None,
min_equity: float = 0.8):
super().init()
self.df = df.reset_index(drop=True)
self.window = int(window)
self.spread_abs = float(spread_abs)
self.commission_rate = float(commission_rate)
self.slippage_k = float(slippage_k)
assert reward_mode in {"pct","points"}
self.reward_mode = reward_mode
self.use_close_norm = use_close_norm
self.flip_penalty = float(flip_penalty)
self.enforce_flat_outside = bool(enforce_flat_outside_hours)
self.min_equity = float(min_equity)
self.trade_hours = None
if trade_hours_utc and isinstance(trade_hours_utc,(list,tuple)) and len(trade_hours_utc)==2:
try:
s = [int(x) for x in str(trade_hours_utc[0]).split(":")]
e = [int(x) for x in str(trade_hours_utc[1]).split(":")]
self.trade_hours = (dtime(s[0], s[1] if len(s)>1 else 0), dtime(e[0], e[1] if len(e)>1 else 0))
except Exception:
self.trade_hours = None
base_cols = ['open','high','low','close','tick_volume','spread','time']
if features_spec is not None:
missing = [c for c in features_spec if c not in self.df.columns]
if missing: raise ValueError(f"Missing feature columns: {missing}")
self.feat_cols = list(features_spec)
else:
self.feat_cols = [c for c in self.df.columns if c not in base_cols]
self.price_col = 'close_norm' if (self.use_close_norm and 'close_norm' in self.df.columns) else 'close'
if len(self.df) <= self.window:
raise ValueError(f"Not enough rows: {len(self.df)} <= window {self.window}")
obs_dim = self.window * (1 + len(self.feat_cols)) + 2
self.action_space = spaces.Discrete(3)
self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
self._start = self.window
self._i = None
self.pos = 0
self.entry = None
self.equity = 1.0
self.prev_eq = 1.0
def _in_trade_hours(self, ts)->bool:
if self.trade_hours is None: return True
try: t = ts.to_pydatetime().time()
except Exception: t = ts
start, end = self.trade_hours
if start <= end: return (t>=start) and (t<=end)
return (t>=start) or (t<=end)
def _obs(self):
sl = slice(self._i - self.window, self.i)
block_df = self.df.iloc[sl][[self.price_col] + self.feat_cols]
block = block_df.to_numpy(dtype=np.float32)
expected = 1 + len(self.feat_cols)
if block.shape != (self.window, expected):
raise RuntimeError(f"Bad window shape: {block.shape} vs {(self.window, expected)}")
flat = block.flatten()
price = float(self.df.iloc[self.i]['close'])
unreal = 0.0
if self.pos != 0 and self.entry is not None:
dir = 1 if self.pos>0 else -1
unreal = dir * (price - self.entry)
import numpy as np
return np.concatenate([flat, np.array([self.pos, unreal], dtype=np.float32)])
def reset(self, seed=None, options=None):
super().reset(seed=seed)
self._i = self._start
self.pos = 0; self.entry = None
self.equity = 1.0; self.prev_eq = 1.0
return self._obs(), {}
def step(self, action):
import numpy as np
if isinstance(action,(np.ndarray,list,tuple)): action = int(action[0])
else: action = int(action)
if not self.action_space.contains(action): raise ValueError("Invalid action")
info = {}
ts = self.df.iloc[self._i]['time']
inside = self._in_trade_hours(ts)
price = float(self.df.iloc[self.i]['close'])
prev = float(self.df.iloc[self.i-1]['close'])
slip = self.slippage_k * abs(price - prev)
desired = [-1,0,1][action]
if not inside and self.enforce_flat_outside: desired = 0
if desired != self.pos:
if self.pos != 0 and desired != 0 and np.sign(self.pos) != np.sign(desired) and self.flip_penalty>0:
if self.reward_mode == 'pct': self.equity *= max(1.0 - self.flip_penalty, 1e-6)
else: self.equity -= self.flip_penalty
info['flip_penalty'] = float(self.flip_penalty)
if self.pos != 0 and self.entry is not None:
cost = (self.spread_abs + slip)/max(price,1e-12) + self.commission_rate
if self.reward_mode == 'pct': self.equity *= max(1.0 - cost, 1e-6)
else: self.equity -= cost
self.pos = desired
if self.pos != 0:
cost = (self.spread_abs + slip)/max(price,1e-12) + self.commission_rate
if self.reward_mode == 'pct': self.equity *= max(1.0 - cost, 1e-6)
else: self.equity -= cost
self.entry = price
else:
self.entry = None
if self.reward_mode == 'pct':
step_ret = 0.0
if self.pos != 0:
dir = 1 if self.pos>0 else -1
step_ret = dir * ((price/max(prev,1e-12)) - 1.0)
self.equity *= (1.0 + step_ret)
reward = float(self.equity - self.prev_eq)
else:
reward = float(self.equity - self.prev_eq)
self.prev_eq = self.equity
self._i += 1
terminated = bool(self._i >= len(self.df) - 1)
truncated = False
if self.equity <= self.min_equity:
truncated = True
info['early_stop'] = True
info.update({'time': ts, 'price': price, 'pos': int(self.pos), 'equity': float(self.equity), 'inside_hours': bool(inside)})
return self._obs(), reward, terminated, truncated, info


=== FILE: _backup_py\fetch__mt5_data.py ===

# -*- coding: utf-8 -*-
"""
Fetch bars & 24h tick sample from MT5 (demo/real). Incremental merge to maintain 720d.
Atomic CSV writes, Retries/backoff, Weekend-safe ticks (warn only)
"""
import MetaTrader5 as mt5
import pandas as pd
from datetime import datetime, timedelta, timezone
import yaml
from pathlib import Path
import numpy as np, time
from utils.atomic import atomic_write_csv
from utils.mt5_health import ensure_mt5_ready

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
SYMBOL = cfg['symbol']
BARS_CSV = cfg['files']['bars_csv']
TICKS_CSV = cfg['files']['ticks_csv']
HISTORY_DAYS = int(cfg.get('history_days', 720))
TF_MAP={'M1': mt5.TIMEFRAME_M1,'M5': mt5.TIMEFRAME_M5,'M15': mt5.TIMEFRAME_M15}
TF_NAME = cfg.get('timeframe','M5')
TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)
CHUNK = int(cfg.get('mt5',{}).get('bars_chunk', 20000))
UPDATE_DAYS = int(cfg.get('mt5',{}).get('update_days', 60))

def init_mt5():
    # inicjalizacja + czytelny komunikat, gdy terminal/konto nie są gotowe
    if not ensure_mt5_ready():
        last_err = mt5.last_error()
        raise RuntimeError(f"MT5 not ready (terminal/account). Start MT5 and login to a demo/real account. last_error={last_err}")
    term = mt5.terminal_info(); acc = mt5.account_info()
    parts=[]
    if getattr(term,"company",None): parts.append(f"Company={term.company}")
    if getattr(term,"name",None): parts.append(f"TerminalName={term.name}")
    if getattr(acc,"login",0): parts.append(f"Login={acc.login}")
    if getattr(acc,"server",None): parts.append(f"Server={acc.server}")
    if getattr(acc,"name",None): parts.append(f"AccountName={acc.name}")
    print("[MT5] " + " | ".join(parts) if parts else "[MT5] initialized")

def ensure_symbol(symbol: str)->bool:
    info = mt5.symbol_info(symbol)
    if info is None:
        mt5.symbol_select(symbol, True)
        info = mt5.symbol_info(symbol)
    if info is None: return False
    if not info.visible:
        if not mt5.symbol_select(symbol, True): return False
    return True

def with_retries(fn, attempts=3, sleep_s=2, *a, **kw):
    last=None
    for i in range(attempts):
        try:
            return fn(*a, **kw)
        except Exception as e:
            last = e
            time.sleep(sleep_s*(i+1))
    if last: raise last

def _to_df(rates):
    df = pd.DataFrame(rates)
    if df.empty: return df
    df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
    if 'real_volume' in df.columns:
        df.rename(columns={'real_volume':'tick_volume'}, inplace=True)
    cols=['time','open','high','low','close','tick_volume','spread']
    return df[cols].sort_values('time').drop_duplicates('time')

def fetch_range(symbol, timeframe, utc_from, utc_to):
    rates = mt5.copy_rates_range(symbol, timeframe, utc_from, utc_to)
    if rates is None or len(rates)==0:
        last_err = mt5.last_error()
        raise RuntimeError(f"copy_rates_range empty for '{symbol}'. last_error={last_err}")
    return _to_df(rates)

def fetch_from_pos_tail(symbol, timeframe, count):
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None or len(rates)==0:
        last_err = mt5.last_error()
        raise RuntimeError(f"copy_rates_from_pos empty for '{symbol}'. last_error={last_err}")
    return _to_df(rates)

def load_existing_bars(path: str):
    p = Path(path)
    if not p.exists(): return None
    try:
        df = pd.read_csv(p, parse_dates=['time'])
        if df.empty: return None
        return df.sort_values('time').drop_duplicates('time').reset_index(drop=True)
    except Exception:
        return None

def merge_clip(existing: pd.DataFrame | None, new_df: pd.DataFrame, keep_days:int)->pd.DataFrame:
    if existing is None or existing.empty:
        base = new_df.copy()
    else:
        base = (pd.concat([existing, new_df], ignore_index=True)
                .drop_duplicates('time').sort_values('time'))
    cutoff = datetime.now(timezone.utc) - timedelta(days=keep_days+1)
    base = base[base['time'] >= pd.Timestamp(cutoff)]
    return base.reset_index(drop=True)

def fetch_bars_incremental(symbol, timeframe, keep_days:int, update_days:int)->pd.DataFrame:
    existing = load_existing_bars(BARS_CSV)
    if existing is None:
        to = datetime.now(timezone.utc); frm = to - timedelta(days=keep_days+2)
        df = with_retries(lambda: fetch_range(symbol, timeframe, frm, to))
        if len(df) < 1000:
            tail = with_retries(lambda: fetch_from_pos_tail(symbol, timeframe, CHUNK))
            df = merge_clip(df, tail, keep_days)
        return df
    to = datetime.now(timezone.utc); frm = to - timedelta(days=max(2, update_days))
    fresh = with_retries(lambda: fetch_range(symbol, timeframe, frm, to))
    merged = merge_clip(existing, fresh, keep_days)
    return merged

def fetch_ticks(symbol: str, hours: int=24)->pd.DataFrame:
    utc_to = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(hours=hours)
    ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)
    if ticks is None or len(ticks)==0:
        last_err = mt5.last_error()
        raise RuntimeError(f"No MT5 ticks for '{symbol}'. last_error={last_err}")
    tdf = pd.DataFrame(ticks)
    tdf['time'] = pd.to_datetime(tdf['time'], unit='s', utc=True)
    return tdf[['time','bid','ask','last','volume']]

def suggest_costs(tdf: pd.DataFrame)->dict:
    spr = (tdf['ask'] - tdf['bid']).astype(float).replace([np.inf,-np.inf], np.nan).dropna()
    med_spread = float(np.median(spr)) if len(spr) else 0.0
    p75_spread = float(np.percentile(spr, 75)) if len(spr) else 0.0
    mid = (tdf['ask'] + tdf['bid'])/2.0
    dm = (mid.diff().abs()).replace([np.inf,-np.inf], np.nan).dropna()
    med_dm = float(np.median(dm)) if len(dm) else 0.0
    med_price = float(np.nanmedian(mid)) if len(mid) else 1.0
    slippage_k = float(min(max(med_dm / max(med_price, 1e-12), 0.0), 0.01))
    return {'spread_abs_median': round(med_spread,5), 'spread_abs_p75': round(p75_spread,5), 'slippage_k_suggested': round(slippage_k,4)}

def main():
    init_mt5()
    try:
        print(f"[CFG] symbol={SYMBOL} tf={TF_NAME} keep_days={HISTORY_DAYS} update_days={UPDATE_DAYS}")
        if not ensure_symbol(SYMBOL):
            raise SystemExit(
                "Symbol not found or not visible in Market Watch. "
                "Open Symbols window in MT5 and SHOW the instrument."
            )
        bars = fetch_bars_incremental(SYMBOL, TF, keep_days=HISTORY_DAYS, update_days=UPDATE_DAYS)
        assert {'time','open','high','low','close'}.issubset(bars.columns), "Bars frame malformed"
        assert len(bars) > 100, "Too few bars"
        atomic_write_csv(bars, BARS_CSV)
        print(f"Saved bars: {BARS_CSV} ({len(bars)})")
        # ticks są opcjonalne (weekend-safe)
        try:
            ticks = with_retries(lambda: fetch_ticks(SYMBOL, hours=24))
            atomic_write_csv(ticks, TICKS_CSV)
            print(f"Saved ticks: {TICKS_CSV} ({len(ticks)})")
            sugg = suggest_costs(ticks)
            Path('reports').mkdir(parents=True, exist_ok=True)
            Path('reports/costs_suggestion.yaml').write_text(
                yaml.safe_dump(sugg, allow_unicode=True, sort_keys=False), encoding='utf-8')
            print("Cost suggestions -> reports/costs_suggestion.yaml")
        except Exception as e:
            print(f"[warn] ticks unavailable ({e}); keeping previous {TICKS_CSV}")
    finally:
        mt5.shutdown()

if __name__ == '__main__':
    main()

=== FILE: _backup_py\quick_env_test.py ===

import yaml, pandas as pd
from env_xau import XauTradingEnv
cfg=yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
df=pd.read_csv(cfg['files']['features_csv'], parse_dates=['time']).sort_values('time').reset_index(drop=True)
df=df.tail(max(int(cfg.get('window',128))+256,512))
env=XauTradingEnv(df, window=int(cfg.get('window',128)),
spread_abs=cfg['costs']['spread_abs'], commission_rate=cfg['costs']['commission_rate'], slippage_k=cfg['costs']['slippage_k'],
reward_mode=cfg.get('env',{}).get('reward_mode', cfg.get('reward_mode','pct')), use_close_norm=True,
min_equity=float(cfg.get('env',{}).get('min_equity',0.8)))
obs,info=env.reset(); print('reset OK', len(obs))
obs,r,term,trunc,info=env.step(1); print('step OK', r, term, trunc)
