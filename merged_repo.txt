

=== FILE: bootstrap_pack.ps1 ===

# bootstrap_pack.ps1
$ErrorActionPreference = "Stop"

# =========================================
# Root + katalogi
# =========================================
$Root = Split-Path -Parent $MyInvocation.MyCommand.Path
$dirs = @("", "data", "features", "rl", "paper_demo", "utils", "models", "models/registry", "reports", "ops", "logs")
foreach($d in $dirs){ New-Item -ItemType Directory -Force -Path (Join-Path $Root $d) | Out-Null }

function Write-UTF8($RelPath, $Content){
  $Path = Join-Path $Root $RelPath
  $dir = Split-Path -Parent $Path
  if(!(Test-Path $dir)){ New-Item -ItemType Directory -Force -Path $dir | Out-Null }
  $Content | Out-File -FilePath $Path -Encoding utf8 -Force
}

# =========================================
# README
# =========================================
Write-UTF8 "README.md" @'
# XAUUSD RL PPO (M5) – Starter (Demo/Edu)
**Uwaga**: tylko do celów edukacyjnych, backtestów i *paper tradingu* (demo). Brak kodu wysyłającego realne zlecenia.

## Szybki start
```powershell
python -m venv .venv
# .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
# Na końcu zainstaluj TORCH odpowiedni dla CPU/GPU z pytorch.org (wheel).
python fetch__mt5_data.py
python utils\calibration.py
python utils\calibration.py --apply
python features\build_features.py
python rl\train_ppo.py --timesteps 1500000
python rl\evaluate.py --out_dir reports
python utils\make_report.py

24/7 na Windows
Użyj skryptów w ops\ oraz NSSM (patrz ops\install_services.ps1).
Dane i historia
Trzymamy 720 dni historii świec. Zbieracz działa inkrementalnie: dociąga ostatnie update_days (domyślnie 60), scala z istniejacym CSV i przycina do 720 dni. Zapisy CSV są atomowe.
'@
# =========================================
# requirements.txt
# =========================================
Write-UTF8 "requirements.txt" @'
pandas>=2.0
numpy>=1.24
pytz
PyYAML
joblib
matplotlib
tensorboard
MetaTrader5
gymnasium>=0.29
stable-baselines3>=2.2.1
torch: zainstaluj odpowiedni wheel (CPU/GPU) z pytorch.org
'@
# =========================================
# config.yaml
# =========================================
Write-UTF8 "config.yaml" @'
symbol: "XAUUSD"
timeframe: "M5"
history_days: 720  # wymagane okno historii
window: 128
mt5:
bars_chunk: 20000   # ile świec pobierać jednorazowo przy "from_pos" (~70-90 dni M5)
# update_days: 60     # inkrementalny zakres odświeżania (do scalenia z istniejacym CSV)
files:
bars_csv: "data/XAUUSD_M5.csv"
ticks_csv: "data/XAUUSD_ticks_sample.csv"
features_csv: "data/XAUUSD_M5_features.csv"
model_path: "models/ppo_xauusd_m5.zip"
vecnorm_path: "models/vecnorm_xauusd_m5.pkl"
costs:
spread_abs: 0.05
commission_rate: 0.0001
slippage_k: 0.10
env:
reward_mode: "pct"
flip_penalty: 0.002
trade_hours_utc: ["06:00", "20:00"]
enforce_flat_outside_hours: true
min_equity: 0.8
'@
# =========================================
# utils/atomic.py
# =========================================
Write-UTF8 "utils/atomic.py" @'
from pathlib import Path
def atomic_write_csv(df, path: str):
p = Path(path)
p.parent.mkdir(parents=True, exist_ok=True)
tmp = p.with_suffix(p.suffix + ".tmp")
df.to_csv(tmp, index=False)
tmp.replace(p)
'@
# =========================================
# utils/mt5_health.py
# =========================================
Write-UTF8 "utils/mt5_health.py" @'
import MetaTrader5 as mt5, time
def ensure_mt5_ready(retries=3, sleep_s=2):
"""Ponawia initialize i weryfikuje, że terminal + konto są gotowe."""
for i in range(retries):
if mt5.initialize():
ti, ai = mt5.terminal_info(), mt5.account_info()
if ti and ai and getattr(ai, "login", 0):
return True
mt5.shutdown()
time.sleep(sleep_s * (i + 1))
return False
'@
# =========================================
# env_xau.py
# =========================================
Write-UTF8 "env_xau.py" @'
-- coding: utf-8 --
import numpy as np, pandas as pd
import gymnasium as gym
from gymnasium import spaces
from datetime import time as dtime
class XauTradingEnv(gym.Env):
metadata = {"render_modes": []}
def init(self, df: pd.DataFrame, window=128,
spread_abs=0.05, commission_rate=0.0001, slippage_k=0.10,
reward_mode: str = "pct", use_close_norm: bool = True,
flip_penalty: float = 0.0, trade_hours_utc=None,
enforce_flat_outside_hours: bool = True, features_spec: list | None = None,
min_equity: float = 0.8):
super().init()
self.df = df.reset_index(drop=True)
self.window = int(window)
self.spread_abs = float(spread_abs)
self.commission_rate = float(commission_rate)
self.slippage_k = float(slippage_k)
assert reward_mode in {"pct","points"}
self.reward_mode = reward_mode
self.use_close_norm = use_close_norm
self.flip_penalty = float(flip_penalty)
self.enforce_flat_outside = bool(enforce_flat_outside_hours)
self.min_equity = float(min_equity)
self.trade_hours = None
if trade_hours_utc and isinstance(trade_hours_utc,(list,tuple)) and len(trade_hours_utc)==2:
try:
s = [int(x) for x in str(trade_hours_utc[0]).split(":")]
e = [int(x) for x in str(trade_hours_utc[1]).split(":")]
self.trade_hours = (dtime(s[0], s[1] if len(s)>1 else 0), dtime(e[0], e[1] if len(e)>1 else 0))
except Exception:
self.trade_hours = None
base_cols = ['open','high','low','close','tick_volume','spread','time']
if features_spec is not None:
missing = [c for c in features_spec if c not in self.df.columns]
if missing: raise ValueError(f"Missing feature columns: {missing}")
self.feat_cols = list(features_spec)
else:
self.feat_cols = [c for c in self.df.columns if c not in base_cols]
self.price_col = 'close_norm' if (self.use_close_norm and 'close_norm' in self.df.columns) else 'close'
if len(self.df) <= self.window:
raise ValueError(f"Not enough rows: {len(self.df)} <= window {self.window}")
obs_dim = self.window * (1 + len(self.feat_cols)) + 2
self.action_space = spaces.Discrete(3)
self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
self._start = self.window
self._i = None
self.pos = 0
self.entry = None
self.equity = 1.0
self.prev_eq = 1.0
def _in_trade_hours(self, ts)->bool:
if self.trade_hours is None: return True
try: t = ts.to_pydatetime().time()
except Exception: t = ts
start, end = self.trade_hours
if start <= end: return (t>=start) and (t<=end)
return (t>=start) or (t<=end)
def _obs(self):
sl = slice(self._i - self.window, self.i)
block_df = self.df.iloc[sl][[self.price_col] + self.feat_cols]
block = block_df.to_numpy(dtype=np.float32)
expected = 1 + len(self.feat_cols)
if block.shape != (self.window, expected):
raise RuntimeError(f"Bad window shape: {block.shape} vs {(self.window, expected)}")
flat = block.flatten()
price = float(self.df.iloc[self.i]['close'])
unreal = 0.0
if self.pos != 0 and self.entry is not None:
dir = 1 if self.pos>0 else -1
unreal = dir * (price - self.entry)
import numpy as np
return np.concatenate([flat, np.array([self.pos, unreal], dtype=np.float32)])
def reset(self, seed=None, options=None):
super().reset(seed=seed)
self._i = self._start
self.pos = 0; self.entry = None
self.equity = 1.0; self.prev_eq = 1.0
return self._obs(), {}
def step(self, action):
import numpy as np
if isinstance(action,(np.ndarray,list,tuple)): action = int(action[0])
else: action = int(action)
if not self.action_space.contains(action): raise ValueError("Invalid action")
info = {}
ts = self.df.iloc[self._i]['time']
inside = self._in_trade_hours(ts)
price = float(self.df.iloc[self.i]['close'])
prev = float(self.df.iloc[self.i-1]['close'])
slip = self.slippage_k * abs(price - prev)
desired = [-1,0,1][action]
if not inside and self.enforce_flat_outside: desired = 0
if desired != self.pos:
if self.pos != 0 and desired != 0 and np.sign(self.pos) != np.sign(desired) and self.flip_penalty>0:
if self.reward_mode == 'pct': self.equity *= max(1.0 - self.flip_penalty, 1e-6)
else: self.equity -= self.flip_penalty
info['flip_penalty'] = float(self.flip_penalty)
if self.pos != 0 and self.entry is not None:
cost = (self.spread_abs + slip)/max(price,1e-12) + self.commission_rate
if self.reward_mode == 'pct': self.equity *= max(1.0 - cost, 1e-6)
else: self.equity -= cost
self.pos = desired
if self.pos != 0:
cost = (self.spread_abs + slip)/max(price,1e-12) + self.commission_rate
if self.reward_mode == 'pct': self.equity *= max(1.0 - cost, 1e-6)
else: self.equity -= cost
self.entry = price
else:
self.entry = None
if self.reward_mode == 'pct':
step_ret = 0.0
if self.pos != 0:
dir = 1 if self.pos>0 else -1
step_ret = dir * ((price/max(prev,1e-12)) - 1.0)
self.equity *= (1.0 + step_ret)
reward = float(self.equity - self.prev_eq)
else:
reward = float(self.equity - self.prev_eq)
self.prev_eq = self.equity
self._i += 1
terminated = bool(self._i >= len(self.df) - 1)
truncated = False
if self.equity <= self.min_equity:
truncated = True
info['early_stop'] = True
info.update({'time': ts, 'price': price, 'pos': int(self.pos), 'equity': float(self.equity), 'inside_hours': bool(inside)})
return self._obs(), reward, terminated, truncated, info
'@
# =========================================
# rl/train_ppo.py
# =========================================
Write-UTF8 "rl/train_ppo.py" @'
-- coding: utf-8 --
"""PPO training with VecNormalize/Monitor/TimeLimit and features_spec."""
import argparse, yaml, json
import pandas as pd
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor, sync_envs_normalization
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.utils import set_random_seed
from gymnasium.wrappers import TimeLimit
from env_xau import XauTradingEnv
parser = argparse.ArgumentParser()
parser.add_argument('--timesteps', type=int, default=1500000)
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--eval_freq', type=int, default=100000)
parser.add_argument('--n_eval_episodes', type=int, default=1)
parser.add_argument('--max_train_steps', type=int, default=6000)
parser.add_argument('--max_eval_steps', type=int, default=3000)
args = parser.parse_args()
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = Path(cfg['files']['model_path'])
VECNORM_PATH = Path(cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl'))
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))
df = pd.read_csv(FEAT, parse_dates=['time'])
cut_val = df['time'].max() - pd.Timedelta(days=120)
train_df = df[df['time'] < cut_val].copy()
val_df   = df[df['time'] >= cut_val].copy()
features_spec = None
fspec = Path('models/features_spec.json')
if fspec.exists():
features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)
set_random_seed(args.seed)
def make_train():
env = XauTradingEnv(train_df, window=WINDOW,
spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=args.max_train_steps)
def make_eval():
env = XauTradingEnv(val_df, window=WINDOW,
spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=args.max_eval_steps)
venv_train = DummyVecEnv([make_train]); venv_train = VecMonitor(venv_train); venv_train = VecNormalize(venv_train, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)
venv_eval  = DummyVecEnv([make_eval]);  venv_eval  = VecMonitor(venv_eval);  venv_eval  = VecNormalize(venv_eval, training=False, norm_obs=True, norm_reward=False)
policy_kwargs = dict(net_arch=dict(pi=[128,128], vf=[128,128]))
try:
import tensorboard as _tb
tb_log_dir = 'logs/ppo_gold_m5'
except Exception:
tb_log_dir = None
model = PPO('MlpPolicy', venv_train, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02,
policy_kwargs=policy_kwargs, seed=args.seed, verbose=1, tensorboard_log=tb_log_dir)
sync_envs_normalization(venv_eval, venv_train)
eval_cb = EvalCallback(venv_eval, best_model_save_path=str(MODEL_PATH.parent), log_path='logs/eval',
eval_freq=max(args.eval_freq,1), n_eval_episodes=max(args.n_eval_episodes,1),
deterministic=True, render=False)
model.learn(total_timesteps=args.timesteps, callback=eval_cb)
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
model.save(str(MODEL_PATH))
venv_train.save(str(VECNORM_PATH))
print(f"Saved model: {MODEL_PATH}")
print(f"Saved VecNormalize: {VECNORM_PATH}")
'@
# =========================================
# rl/evaluate.py
# =========================================
Write-UTF8 "rl/evaluate.py" @'
-- coding: utf-8 --
"""Validate PPO on last ~120 days, save equity plot and trace CSV."""
import argparse, yaml, json
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from env_xau import XauTradingEnv
parser = argparse.ArgumentParser()
parser.add_argument('--max_eval_steps', type=int, default=3000)
parser.add_argument('--out_dir', type=str, default='reports')
args = parser.parse_args()
OUT = Path(args.out_dir); OUT.mkdir(parents=True, exist_ok=True)
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = cfg['files']['model_path']
VECNORM_PATH = cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl')
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))
df = pd.read_csv(FEAT, parse_dates=['time'])
cut_val = df['time'].max() - pd.Timedelta(days=120)
val = df[df['time']>=cut_val].copy()
features_spec = None
fspec = Path('models/features_spec.json')
if fspec.exists():
features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)
def make_eval():
env = XauTradingEnv(val, window=WINDOW,
spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=args.max_eval_steps)
venv = DummyVecEnv([make_eval]); venv = VecMonitor(venv)
if Path(VECNORM_PATH).exists():
venv = VecNormalize.load(VECNORM_PATH, venv)
venv.training=False; venv.norm_reward=False
model = PPO.load(MODEL_PATH, env=venv)
obs = venv.reset(); eq=[]; rows=[]; done=False
while not done:
action,_ = model.predict(obs, deterministic=True)
obs, rewards, dones, infos = venv.step(action)
equity = venv.get_attr('equity', indices=0)[0]
eq.append(float(equity))
info0 = infos[0] if isinstance(infos,(list,tuple)) else infos
rows.append({'time': str(info0.get('time','')), 'price': info0.get('price',np.nan), 'pos': info0.get('pos',np.nan),
'equity': equity, 'action': int(action[0]) if hasattr(action,'len') else int(action)})
done = bool(dones[0])
s = pd.Series(eq)
final_eq = float(s.iloc[-1]); min_eq=float(s.min()); peak=s.cummax(); max_dd=float((s/peak-1.0).min())
plt.figure(figsize=(10,4)); plt.plot(s.values); plt.title('Equity (validation)'); plt.tight_layout()
plt.savefig((OUT/'equity_val.png').as_posix(), dpi=150)
(OUT/'val_metrics.txt').write_text(
f"Final equity: {final_eq:.6f}\nMin equity: {min_eq:.6f}\nMax drawdown: {max_dd:.6f}\n", encoding='utf-8')
pd.DataFrame(rows).to_csv(OUT/'eval_trace.csv', index=False)
print("Saved validation results.")
'@
# =========================================
# rl/walk_forward.py
# =========================================
Write-UTF8 "rl/walk_forward.py" @'
-- coding: utf-8 --
"""Walk-forward evaluation with per-fold traces and CSV/TXT outputs."""
import argparse, yaml, json
import pandas as pd, numpy as np
from pathlib import Path
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from env_xau import XauTradingEnv
parser = argparse.ArgumentParser()
parser.add_argument('--segments', type=int, default=6)
parser.add_argument('--train_days', type=int, default=120)
parser.add_argument('--val_days', type=int, default=30)
parser.add_argument('--timesteps', type=int, default=500000)
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--out_dir', type=str, default='reports_wf')
args = parser.parse_args()
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))
out = Path(args.out_dir); out.mkdir(parents=True, exist_ok=True)
df = pd.read_csv(FEAT, parse_dates=['time']).sort_values('time').reset_index(drop=True)
end_time = df['time'].max()
features_spec=None
fspec = Path('models/features_spec.json')
if fspec.exists(): features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)
rows=[]
for k in range(args.segments):
val_end = end_time - pd.Timedelta(days=k*args.val_days)
val_start = val_end - pd.Timedelta(days=args.val_days)
train_end = val_start
train_start = train_end - pd.Timedelta(days=args.train_days)
tr = df[(df['time']>=train_start)&(df['time']<train_end)].copy()
va = df[(df['time']>=val_start)&(df['time']<val_end)].copy()
if len(tr)<=WINDOW or len(va)<=WINDOW: continue
def make_train():
env = XauTradingEnv(tr, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=6000)
def make_eval():
env = XauTradingEnv(va, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=3000)
vt = DummyVecEnv([make_train]); vt = VecMonitor(vt); vt = VecNormalize(vt, norm_obs=True, norm_reward=True)
ve = DummyVecEnv([make_eval]);  ve = VecMonitor(ve)
model = PPO('MlpPolicy', vt, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02, seed=args.seed, verbose=0)
model.learn(total_timesteps=args.timesteps)
tmp = out/f'vecnorm_{k}.pkl'; vt.save(str(tmp))
ve = VecNormalize.load(str(tmp), ve); ve.training=False; ve.norm_reward=False
obs = ve.reset(); eq=[]; trace=[]; done=False
while not done:
action,=model.predict(obs, deterministic=True)
obs, rewards, dones, infos = ve.step(action)
equity = ve.get_attr('equity', indices=0)[0]
eq.append(float(equity))
info0=infos[0] if isinstance(infos,(list,tuple)) else infos
trace.append({'k':k,'time':str(info0.get('time','')),'price':info0.get('price',np.nan),'pos':info0.get('pos',np.nan),'equity':equity,
'action':int(action[0]) if hasattr(action,'len') else int(action)})
done = bool(dones[0])
s = pd.Series(eq); final_eq=float(s.iloc[-1]); min_eq=float(s.min()); peak=s.cummax(); max_dd=float((s/peak-1.0).min())
rows.append({'k':k,'train_start':train_start,'train_end':train_end,'val_start':val_start,'val_end':val_end,'final_eq':final_eq,'min_eq':min_eq,'max_dd':max_dd,'n_steps':int(len(s))})
pd.DataFrame(trace).to_csv(out/f'fold{k}_trace.csv', index=False)
res = pd.DataFrame(rows).sort_values('k'); res.to_csv(out/'wf_results.csv', index=False)
if res.empty:
txt=['# Walk-Forward Report','No results (windows too short?).']
else:
agg={'folds':len(res),'final_eq_med':float(res['final_eq'].median()),'max_dd_med':float(res['max_dd'].median()),'n_steps_sum':int(res['n_steps'].sum())}
txt=['# Walk-Forward Report',f"Folds: {agg['folds']}",f"Median Final Equity: {agg['final_eq_med']:.4f}",f"Median MaxDD: {agg['max_dd_med']:.2%}",f"Total steps: {agg['n_steps_sum']}"]
(out/'wf_report.txt').write_text('`n'.join(txt), encoding='utf-8')
print("Saved walk-forward results.")
'@
# =========================================
# paper_demo/paper_loop_mt5_demo.py (poprawiony)
# =========================================
Write-UTF8 "paper_demo/paper_loop_mt5_demo.py" @'
-- coding: utf-8 --
"""Paper trading (DEMO) - pull M5 bars from MT5, build features incrementally, log decisions."""
import MetaTrader5 as mt5
import pandas as pd, numpy as np, yaml, time, logging, json
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from utils.mt5_health import ensure_mt5_ready
logging.basicConfig(filename='paper_demo/paper_trading.log', level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
SYMBOL = cfg['symbol']; WINDOW=int(cfg.get('window',128))
MODEL_PATH = cfg['files']['model_path']; VECNORM_PATH = cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl')
TF_MAP={'M1': mt5.TIMEFRAME_M1,'M5': mt5.TIMEFRAME_M5,'M15': mt5.TIMEFRAME_M15}; TF_NAME = cfg.get('timeframe','M5'); TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)
features_spec=None; fs=Path('models/features_spec.json')
if fs.exists(): features_spec=json.loads(fs.read_text(encoding='utf-8')).get('feature_columns', None)
def add_features_incremental(df: pd.DataFrame)->pd.DataFrame:
df=df.sort_values('time').reset_index(drop=True)
df['ret1']=np.log(df['close']).diff()
df['ema10']=df['close'].ewm(span=10).mean(); df['ema50']=df['close'].ewm(span=50).mean(); df['ema200']=df['close'].ewm(span=200).mean()
d=df['close'].diff(); up=d.clip(lower=0).ewm(alpha=1/14,adjust=False).mean(); down=(-d.clip(upper=0)).ewm(alpha=1/14,adjust=False).mean(); rs=up/(down+1e-12)
df['rsi14']=100-(100/(1+rs))
h,l,c=df['high'],df['low'],df['close']; tr=np.maximum(h-l, np.maximum(abs(h-c.shift()), abs(l-c.shift())))
df['atr14']=tr.ewm(alpha=1/14,adjust=False).mean()
ema_fast=df['close'].ewm(span=12,adjust=False).mean(); ema_slow=df['close'].ewm(span=26,adjust=False).mean(); macd_line=ema_fast-ema_slow; signal_line=macd_line.ewm(span=9,adjust=False).mean()
df['macd']=macd_line; df['macd_signal']=signal_line; df['macd_hist']=macd_line-signal_line
ma=df['close'].rolling(20).mean(); sd=df['close'].rolling(20).std()
df['bb_ma']=ma; df['bb_up']=ma+2sd; df['bb_lo']=ma-2sd; df['bb_width']=(df['bb_up']-df['bb_lo'])/(ma.replace(0,np.nan).abs()+1e-12)
df['minute']=df['time'].dt.hour60+df['time'].dt.minute; df['tod_sin']=np.sin(2np.pidf['minute']/1440); df['tod_cos']=np.cos(2np.pidf['minute']/1440); df.drop(columns=['minute'], inplace=True)
df['dow']=df['time'].dt.dayofweek; df['dow_sin']=np.sin(2np.pidf['dow']/7); df['dow_cos']=np.cos(2np.pi*df['dow']/7); df.drop(columns=['dow'], inplace=True)
df['close_log']=np.log(df['close'].clip(lower=1e-12)); mu=df['close_log'].rolling(2000,min_periods=200).mean(); s=df['close_log'].rolling(2000,min_periods=200).std().replace(0,np.nan)
df['close_norm']=(df['close_log']-mu)/(s+1e-8)
norm=['ret1','ema10','ema50','ema200','rsi14','atr14','macd','macd_signal','macd_hist','bb_ma','bb_up','bb_lo','bb_width','tod_sin','tod_cos','dow_sin','dow_cos']
for c in norm:
mu=df[c].rolling(2000,min_periods=200).mean(); s=df[c].rolling(2000,min_periods=200).std().replace(0,np.nan); df[c]=(df[c]-mu)/(s+1e-8)
return df.dropna().reset_index(drop=True)
def get_last_bars(symbol, timeframe, n:int):
rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, n)
if rates is None or len(rates)<n: return None
df=pd.DataFrame(rates); df['time']=pd.to_datetime(df['time'], unit='s', utc=True); df.rename(columns={'real_volume':'tick_volume'}, inplace=True)
return df[['time','open','high','low','close','tick_volume','spread']]
def build_obs(df_feat: pd.DataFrame, model_obs_dim: int)->np.ndarray:
per_step=(model_obs_dim-2)//WINDOW
price_col='close_norm' if 'close_norm' in df_feat.columns else 'close'
if features_spec is None:
base=['open','high','low','close','tick_volume','spread','time']; feat_cols=[c for c in df_feat.columns if c not in base]
else:
feat_cols=[c for c in features_spec if c in df_feat.columns]
tail=df_feat.tail(WINDOW); block=tail[[price_col]+feat_cols].to_numpy(dtype=np.float32)
if block.shape!=(WINDOW, per_step): raise RuntimeError(f"Bad block {block.shape} vs {(WINDOW, per_step)}")
flat=block.flatten(); import numpy as np
return np.concatenate([flat, np.array([0.0,0.0],dtype=np.float32)])
def main():
if not ensure_mt5_ready(): raise RuntimeError("MT5 not ready (terminal/account). Start MT5 and login to a demo account.")
try:
model = PPO.load(MODEL_PATH); expected_dim=int(model.observation_space.shape[0]); assert (expected_dim-2)%WINDOW==0
vecnorm=None
if Path(VECNORM_PATH).exists():
from gymnasium import spaces
class _ObsOnlyEnv:
def init(self, obs_dim):
self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
self.action_space = spaces.Discrete(3)
def reset(self, *, seed=None, options=None):
import numpy as np; return np.zeros(self.observation_space.shape, dtype=np.float32), {}
def step(self, action):
import numpy as np; return np.zeros(self.observation_space.shape, dtype=np.float32), 0.0, True, False, {}
dummy=DummyVecEnv([lambda: _ObsOnlyEnv(expected_dim)])
vecnorm=VecNormalize.load(VECNORM_PATH, dummy); vecnorm.training=False; vecnorm.norm_reward=False
Path('paper_demo').mkdir(parents=True, exist_ok=True)
csv=Path('paper_demo/decisions.csv'); if not csv.exists(): csv.write_text('time_utc,price,action_id,action_labeln', encoding='utf-8')         last_ts=None; hb_t=time.time()         while True:             bars=get_last_bars(SYMBOL, TF, n=WINDOW+800)             if bars is None or len(bars)<WINDOW+200: time.sleep(5); continue             feat=add_features_incremental(bars)             if len(feat)<WINDOW: time.sleep(5); continue             cur_ts=feat['time'].iloc[-1]             if last_ts is not None and cur_ts==last_ts: time.sleep(2); continue             obs=build_obs(feat, expected_dim)             if vecnorm is not None:                 import numpy as np; obs=vecnorm.normalize_obs(obs.reshape(1,-1)).reshape(-1)             action,_=model.predict(obs, deterministic=True)             decision={0:'SHORT',1:'FLAT',2:'LONG'}[int(action)]             price=float(feat['close'].iloc[-1]); msg=f"DECISION {decision} @ {price:.2f} (ts={cur_ts})"             print(msg); logging.info(msg)             with open(csv,'a',encoding='utf-8') as f: f.write(f"{cur_ts},{price:.5f},{int(action)},{decision}n")
last_ts=cur_ts
if time.time()-hb_t>600: logging.info(f"[HB] {SYMBOL} {TF_NAME} last_ts={cur_ts} price={price:.2f}"); hb_t=time.time()
time.sleep(30)
finally:
mt5.shutdown()
if name == 'main': main()
'@
# =========================================
# paper_demo/simulate_execution.py
# =========================================
Write-UTF8 "paper_demo/simulate_execution.py" @'
-- coding: utf-8 --
import yaml, pandas as pd
from env_xau import XauTradingEnv
from stable_baselines3 import PPO
from pathlib import Path
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']; MODEL_PATH = cfg['files']['model_path']; WINDOW = int(cfg.get('window',128)); COSTS = cfg['costs']
df = pd.read_csv(FEAT, parse_dates=['time']); cut = df['time'].max() - pd.Timedelta(days=120); sim = df[df['time']>=cut].copy()
env = XauTradingEnv(sim, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'])
obs, info = env.reset(); model = PPO.load(MODEL_PATH)
rows=[]; done=False
while not done:
action,_=model.predict(obs, deterministic=True)
obs, reward, terminated, truncated, info = env.step(action)
done = bool(terminated) or bool(truncated)
rows.append({'time': info.get('time',None), 'price': info.get('price',None), 'action': int(action), 'reward': float(reward), 'equity': float(env.equity)})
Path('reports').mkdir(parents=True, exist_ok=True)
pd.DataFrame(rows).to_csv('reports/trace_simulated.csv', index=False)
pd.Series([r['equity'] for r in rows]).to_csv('reports/equity_simulated.csv', index=False)
print('Saved simulated trace/equity.')
'@
# =========================================
# features/build_features.py (POPRAWIONY)
# =========================================
Write-UTF8 "features/build_features.py" @'
-- coding: utf-8 --
"""Feature builder: EMA/RSI/ATR + MACD + Bollinger + time cycles + rolling z-scores."""
import pandas as pd, numpy as np, yaml, json
from pathlib import Path
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
BARS = cfg['files']['bars_csv']; FEAT = cfg['files']['features_csv']
def macd(series, fast=12, slow=26, signal=9):
ema_fast = series.ewm(span=fast, adjust=False).mean()
ema_slow = series.ewm(span=slow, adjust=False).mean()
macd_line = ema_fast - ema_slow
signal_line = macd_line.ewm(span=signal, adjust=False).mean()
return macd_line, signal_line, macd_line - signal_line
def bbands(series, period=20, n_std=2.0):
ma = series.rolling(period).mean(); sd = series.rolling(period).std()
upper = ma + n_stdsd; lower = ma - n_stdsd
width = (upper - lower) / (ma.replace(0,np.nan).abs() + 1e-12)
return ma, upper, lower, width
def add_features(df):
df = df.sort_values('time').reset_index(drop=True)
c = df['close']
df['ret1'] = np.log(c).diff()
df['ema10'] = c.ewm(span=10).mean(); df['ema50']=c.ewm(span=50).mean(); df['ema200']=c.ewm(span=200).mean()
d = c.diff(); up=d.clip(lower=0).ewm(alpha=1/14,adjust=False).mean(); down=(-d.clip(upper=0)).ewm(alpha=1/14,adjust=False).mean(); rs=up/(down+1e-12)
df['rsi14'] = 100 - (100/(1+rs))
h,l,cl = df['high'], df['low'], df['close']
tr = np.maximum(h-l, np.maximum((h-cl.shift()).abs(), (l-cl.shift()).abs()))
df['atr14'] = tr.ewm(alpha=1/14, adjust=False).mean()
m,s,hst = macd(c); df['macd']=m; df['macd_signal']=s; df['macd_hist']=hst
bb_ma, bb_up, bb_lo, bb_w = bbands(c); df['bb_ma']=bb_ma; df['bb_up']=bb_up; df['bb_lo']=bb_lo; df['bb_width']=bb_w
df['minute']=df['time'].dt.hour60+df['time'].dt.minute
df['tod_sin']=np.sin(2np.pidf['minute']/1440); df['tod_cos']=np.cos(2np.pidf['minute']/1440); df.drop(columns=['minute'], inplace=True)
df['dow']=df['time'].dt.dayofweek
df['dow_sin']=np.sin(2np.pidf['dow']/7); df['dow_cos']=np.cos(2np.pi*df['dow']/7); df.drop(columns=['dow'], inplace=True)
df['close_log']=np.log(c.clip(lower=1e-12))
mu = df['close_log'].rolling(2000, min_periods=200).mean(); sd = df['close_log'].rolling(2000, min_periods=200).std().replace(0,np.nan)
df['close_norm']=(df['close_log']-mu)/(sd+1e-8)
feat_cols=['ret1','ema10','ema50','ema200','rsi14','atr14','macd','macd_signal','macd_hist','bb_ma','bb_up','bb_lo','bb_width','tod_sin','tod_cos','dow_sin','dow_cos']
for col in feat_cols:
mu = df[col].rolling(2000, min_periods=200).mean(); sd = df[col].rolling(2000, min_periods=200).std().replace(0,np.nan)
df[col]=(df[col]-mu)/(sd+1e-8)
return df.dropna().reset_index(drop=True)
df = pd.read_csv(BARS, parse_dates=['time'])
df = add_features(df)
assert len(df) > int(cfg.get('window',128)) + 10, "Too few rows for features"
Path(FEAT).parent.mkdir(parents=True, exist_ok=True)
df.to_csv(FEAT, index=False)
base_cols = ['time','open','high','low','close','tick_volume','spread']
feature_columns = [c for c in df.columns if c not in base_cols]
spec = {"feature_columns": feature_columns, "price_column":"close_norm"}
Path('models').mkdir(parents=True, exist_ok=True)
Path('models/features_spec.json').write_text(json.dumps(spec, ensure_ascii=False, indent=2), encoding='utf-8')
print("Saved features and features_spec.json")
'@
# =========================================
# fetch__mt5_data.py (inkrementalny, 720 dni, atomowy zapis, weekend-safe)
# =========================================
Write-UTF8 "fetch__mt5_data.py" @'
-- coding: utf-8 --
"""
Fetch bars & 24h tick sample from MT5 (demo/real). Incremental merge to maintain 720d.
# Atomic CSV writes, Retries/backoff, Weekend-safe ticks (warn only)
"""
import MetaTrader5 as mt5
import pandas as pd
from datetime import datetime, timedelta, timezone
import yaml
from pathlib import Path
import numpy as np, time
from utils.atomic import atomic_write_csv
from utils.mt5_health import ensure_mt5_ready
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
SYMBOL = cfg['symbol']
BARS_CSV = cfg['files']['bars_csv']
TICKS_CSV = cfg['files']['ticks_csv']
HISTORY_DAYS = int(cfg.get('history_days', 720))
TF_MAP={'M1': mt5.TIMEFRAME_M1,'M5': mt5.TIMEFRAME_M5,'M15': mt5.TIMEFRAME_M15}
TF_NAME = cfg.get('timeframe','M5'); TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)
CHUNK = int(cfg.get('mt5',{}).get('bars_chunk', 20000))
UPDATE_DAYS = int(cfg.get('mt5',{}).get('update_days', 60))
def init_mt5():
if not ensure_mt5_ready(): raise RuntimeError("MT5 not ready (terminal/account). Please login in MT5 terminal.")
term = mt5.terminal_info(); acc = mt5.account_info()
parts=[]
if getattr(term,"company",None): parts.append(f"Company={term.company}")
if getattr(term,"name",None): parts.append(f"TerminalName={term.name}")
if getattr(acc,"login",0): parts.append(f"Login={acc.login}")
if getattr(acc,"server",None): parts.append(f"Server={acc.server}")
if getattr(acc,"name",None): parts.append(f"AccountName={acc.name}")
print("[MT5] " + " | ".join(parts) if parts else "[MT5] initialized")
def ensure_symbol(symbol: str)->bool:
info = mt5.symbol_info(symbol)
if info is None:
mt5.symbol_select(symbol, True)
info = mt5.symbol_info(symbol)
if info is None: return False
if not info.visible:
if not mt5.symbol_select(symbol, True): return False
return True
def suggest_similar(symbol: str, limit=10):
cands=[]
for s in mt5.symbols_get():
name=s.name.upper()
if ("GOLD" in name) or ("XAU" in name): cands.append(s.name)
if not cands:
cands = [s.name for s in mt5.symbols_get()[:limit]]
return sorted(set(cands))[:limit]
def with_retries(fn, attempts=3, sleep_s=2, *a, **kw):
last=None
for i in range(attempts):
try:
return fn(*a, *kw)
except Exception as e:
last = e; time.sleep(sleep_s(i+1))
if last: raise last
def _to_df(rates):
df = pd.DataFrame(rates)
if df.empty: return df
df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
if 'real_volume' in df.columns:
df.rename(columns={'real_volume':'tick_volume'}, inplace=True)
cols=['time','open','high','low','close','tick_volume','spread']
return df[cols].sort_values('time').drop_duplicates('time')
def fetch_range(symbol, timeframe, utc_from, utc_to):
rates = mt5.copy_rates_range(symbol, timeframe, utc_from, utc_to)
if rates is None or len(rates)==0:
last_err = mt5.last_error()
raise RuntimeError(f"copy_rates_range empty for '{symbol}'. last_error={last_err}")
return _to_df(rates)
def fetch_from_pos_tail(symbol, timeframe, count):
rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
if rates is None or len(rates)==0:
last_err = mt5.last_error()
raise RuntimeError(f"copy_rates_from_pos empty for '{symbol}'. last_error={last_err}")
return _to_df(rates)
def load_existing_bars(path: str):
p = Path(path)
if not p.exists(): return None
try:
df = pd.read_csv(p, parse_dates=['time'])
if df.empty: return None
return df.sort_values('time').drop_duplicates('time').reset_index(drop=True)
except Exception:
return None
def merge_clip(existing: pd.DataFrame | None, new_df: pd.DataFrame, keep_days:int)->pd.DataFrame:
if existing is None or existing.empty:
base = new_df.copy()
else:
base = (pd.concat([existing, new_df], ignore_index=True)
.drop_duplicates('time').sort_values('time'))
cutoff = datetime.now(timezone.utc) - timedelta(days=keep_days+1)
base = base[base['time'] >= pd.Timestamp(cutoff)]
return base.reset_index(drop=True)
def fetch_bars_incremental(symbol, timeframe, keep_days:int, update_days:int)->pd.DataFrame:
existing = load_existing_bars(BARS_CSV)
if existing is None:
to = datetime.now(timezone.utc); frm = to - timedelta(days=keep_days+2)
df = with_retries(lambda: fetch_range(symbol, timeframe, frm, to))
if len(df) < 1000:
tail = with_retries(lambda: fetch_from_pos_tail(symbol, timeframe, CHUNK))
df = merge_clip(df, tail, keep_days)
return df
to = datetime.now(timezone.utc); frm = to - timedelta(days=max(2, update_days))
fresh = with_retries(lambda: fetch_range(symbol, timeframe, frm, to))
merged = merge_clip(existing, fresh, keep_days)
return merged
def fetch_ticks(symbol: str, hours: int=24)->pd.DataFrame:
utc_to = datetime.now(timezone.utc); utc_from = utc_to - timedelta(hours=hours)
ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)
if ticks is None or len(ticks)==0:
last_err = mt5.last_error()
raise RuntimeError(f"No MT5 ticks for '{symbol}'. last_error={last_err}")
tdf = pd.DataFrame(ticks); tdf['time']=pd.to_datetime(tdf['time'], unit='s', utc=True)
return tdf[['time','bid','ask','last','volume']]
def suggest_costs(tdf: pd.DataFrame)->dict:
spr = (tdf['ask'] - tdf['bid']).astype(float).replace([np.inf,-np.inf], np.nan).dropna()
med_spread = float(np.median(spr)) if len(spr) else 0.0
p75_spread = float(np.percentile(spr, 75)) if len(spr) else 0.0
mid = (tdf['ask'] + tdf['bid'])/2.0
dm = (mid.diff().abs()).replace([np.inf,-np.inf], np.nan).dropna()
med_dm = float(np.median(dm)) if len(dm) else 0.0
med_price = float(np.nanmedian(mid)) if len(mid) else 1.0
slippage_k = float(min(max(med_dm / max(med_price, 1e-12), 0.0), 0.01))
return {'spread_abs_median': round(med_spread,5), 'spread_abs_p75': round(p75_spread,5), 'slippage_k_suggested': round(slippage_k,4)}
def main():
init_mt5()
try:
print(f"[CFG] symbol={SYMBOL} tf={TF_NAME} keep_days={HISTORY_DAYS} update_days={UPDATE_DAYS}")
if not ensure_symbol(SYMBOL):
similar = suggest_similar(SYMBOL)
raise SystemExit(
"Symbol not found or not visible in Market Watch: '{}'n"                 "Try one of these (server-specific): {}n"
"Also open Symbols window and SHOW the instrument."
.format(SYMBOL, ", ".join(similar))
)
bars = fetch_bars_incremental(SYMBOL, TF, keep_days=HISTORY_DAYS, update_days=UPDATE_DAYS)
assert {'time','open','high','low','close'}.issubset(bars.columns), "Bars frame malformed"
assert len(bars) > 100, "Too few bars"
atomic_write_csv(bars, BARS_CSV)
print(f"Saved bars: {BARS_CSV} ({len(bars)})")
# ticks są opcjonalne (weekend-safe)
try:
ticks = with_retries(lambda: fetch_ticks(SYMBOL, hours=24))
atomic_write_csv(ticks, TICKS_CSV)
print(f"Saved ticks: {TICKS_CSV} ({len(ticks)})")
sugg = suggest_costs(ticks)
Path('reports').mkdir(parents=True, exist_ok=True)
Path('reports/costs_suggestion.yaml').write_text(
yaml.safe_dump(sugg, allow_unicode=True, sort_keys=False), encoding='utf-8')
print("Cost suggestions -> reports/costs_suggestion.yaml")
except Exception as e:
print(f"[warn] ticks unavailable ({e}); keeping previous {TICKS_CSV}")
finally:
mt5.shutdown()
if name == 'main':
main()
'@
# =========================================
# utils/calibration.py
# =========================================
Write-UTF8 "utils/calibration.py" @'
-- coding: utf-8 --
"""Calibrate costs: spread (median/95p) and slippage_k ratio. Save suggestions; with --apply update config.yaml."""
import argparse, yaml, pandas as pd, numpy as np
from pathlib import Path
parser = argparse.ArgumentParser(); parser.add_argument('--apply', action='store_true'); args=parser.parse_args()
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
TICKS = cfg['files']['ticks_csv']; BARS = cfg['files']['bars_csv']
ticks = pd.read_csv(TICKS, parse_dates=['time'])
ticks['spread']=ticks['ask']-ticks['bid']; mid=(ticks['ask']+ticks['bid'])/2.0; dmid=mid.diff().abs()
spread_median=float(ticks['spread'].median()); spread_p95=float(ticks['spread'].quantile(0.95)); dmid_median=float(dmid.median())
bars=pd.read_csv(BARS, parse_dates=['time']); bar_move_med=float(bars['close'].diff().abs().median())
slippage_k=float(np.clip(dmid_median/max(bar_move_med,1e-12), 0.0, 0.5))
Path('reports').mkdir(parents=True, exist_ok=True)
Path('reports/costs_suggestion.yaml').write_text(
yaml.safe_dump({'spread_abs_median':round(spread_median,5),'spread_abs_p95':round(spread_p95,5),'slippage_k_suggested':round(slippage_k,3)},
allow_unicode=True, sort_keys=False),
encoding='utf-8')
print('Suggestions -> reports/costs_suggestion.yaml')
if args.apply:
cfg['costs']['spread_abs']=float(round(spread_median,5)); cfg['costs']['slippage_k']=float(slippage_k)
Path('config.yaml').write_text(yaml.safe_dump(cfg, allow_unicode=True, sort_keys=False), encoding='utf-8')
print('Applied to config.yaml')
'@
# =========================================
# utils/make_report.py (POPRAWIONY)
# =========================================
Write-UTF8 "utils/make_report.py" @'
-- coding: utf-8 --
import argparse, base64
from pathlib import Path
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
parser=argparse.ArgumentParser()
parser.add_argument('--equity_csv', type=str, default='reports/eval_trace.csv')
parser.add_argument('--metrics', type=str, default='reports/val_metrics.txt')
parser.add_argument('--figure', type=str, default='reports/equity_val.png')
parser.add_argument('--out_html', type=str, default='reports/report.html')
parser.add_argument('--out_txt', type=str, default='reports/report.txt')
parser.add_argument('--bars_per_day', type=int, default=288)
parser.add_argument('--trading_days', type=int, default=252)
args=parser.parse_args()
out_html=Path(args.out_html); out_txt=Path(args.out_txt)
out_html.parent.mkdir(parents=True, exist_ok=True); out_txt.parent.mkdir(parents=True, exist_ok=True)
eq=None; p=Path(args.equity_csv)
if p.exists():
df=pd.read_csv(p)
if 'equity' in df.columns: eq=df['equity'].astype(float).to_numpy()
elif df.shape[1]==1: eq=df.iloc[:,0].astype(float).to_numpy()
metrics_text = Path(args.metrics).read_text(encoding='utf-8') if Path(args.metrics).exists() else ''
img_b64=''
if Path(args.figure).exists(): img_b64=base64.b64encode(Path(args.figure).read_bytes()).decode('ascii')
if not img_b64 and eq is not None and len(eq)>1:
fig,ax=plt.subplots(figsize=(9,4)); ax.plot(eq,color='#0057B8',lw=1.5)
ax.set_title('Equity (validation)'); ax.set_xlabel('Steps'); ax.set_ylabel('Equity')
fig.tight_layout(); Path(args.figure).parent.mkdir(parents=True, exist_ok=True); fig.savefig(args.figure,dpi=144); plt.close(fig)
img_b64=base64.b64encode(Path(args.figure).read_bytes()).decode('ascii')
extra={}
if eq is not None and len(eq)>2:
ret=np.diff(eq)/np.maximum(eq[:-1],1e-12); mu=float(np.nanmean(ret)); sigma=float(np.nanstd(ret)+1e-12)
downside=ret[ret<0]; ds=float(np.nanstd(downside)+1e-12)
daily_mu = mu * args.bars_per_day; daily_sigma = sigma * (args.bars_per_day0.5)
sharpe = daily_mu/(daily_sigma+1e-12); sortino = daily_mu/(((args.bars_per_day0.5)*ds)+1e-12)
n=len(eq); years=max(n/args.bars_per_day/args.trading_days,1e-6)
cagr=(eq[-1]/max(eq[0],1e-12))**(1/max(years,1e-6))-1 if years>0 else 0.0
peak=np.maximum.accumulate(eq); max_dd=float((eq/peak-1.0).min()); calmar=(cagr/abs(max_dd)) if max_dd<0 else float('inf')
extra=dict(n_steps=n, sharpe_daily=sharpe, sortino_daily=sortino, vol_daily=daily_sigma, cagr=cagr, max_dd=max_dd, calmar=calmar)
now=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
md=['# RL Validation Report (XAUUSD M5)', f'Date: {now}', '', '## Base metrics', '', metrics_text.strip() if metrics_text else '(no val_metrics.txt)', '', '']
if extra:
md += [
'## Extra metrics from equity',
f"- Steps: {extra['n_steps']}",
f"- Sharpe (daily): {extra['sharpe_daily']:.3f}",
f"- Sortino (daily): {extra['sortino_daily']:.3f}",
f"- Daily vol: {extra['vol_daily']:.4f}",
f"- CAGR (est.): {extra['cagr']:.3%}",
f"- MaxDD: {extra['max_dd']:.2%}",
f"- Calmar: {extra['calmar']:.3f}",
]
else:
md += ['No equity data.']
out_txt.write_text('\n'.join(md)+'\n', encoding='utf-8')
html=f"""RL Validation
RL Validation Report (XAUUSD M5)
Base metrics
{metrics_text if metrics_text else '(no val_metrics.txt)'}
Extra metrics
{('\n'.join(md)) if extra else 'No equity data.'}
Equity
{('data:image/png;base64,'+img_b64) if img_b64 else 'No plot.'}
Generated: {now}
"""
out_html.write_text(html, encoding='utf-8')
print('Saved report.')
'@
# =========================================
# utils/qc_bars.py & utils/time_utils.py
# =========================================
Write-UTF8 "utils/qc_bars.py" @'
#!/usr/bin/env python
import pandas as pd, argparse
parser=argparse.ArgumentParser(); parser.add_argument('--csv', required=True); parser.add_argument('--tf_min', type=int, default=5)
args=parser.parse_args()
df=pd.read_csv(args.csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
print(f"Rows: {len(df)}\nFrom: {df['time'].iloc[0]} To: {df['time'].iloc[-1]}")
dt=(df['time'].diff().dt.total_seconds()/60).fillna(args.tf_min)
gaps=df.loc[dt>args.tf_min+0.1,['time']].copy(); gaps['gap_min']=dt[dt>args.tf_min+0.1].values
print(f"Gaps > {args.tf_min} min: {len(gaps)}");
if len(gaps): print(gaps.head(10))
'@
Write-UTF8 "utils/time_utils.py" @'
from datetime import datetime, timezone
def now_utc(): return datetime.now(timezone.utc)
'@
# =========================================
# ops: env.ps1 & run scripts & promotion
# =========================================
Write-UTF8 "ops/env.ps1" @'
$ErrorActionPreference = "Stop"
$RepoRoot = Split-Path -Parent $MyInvocation.MyCommand.Path | Split-Path
$Py = Join-Path $RepoRoot ".venv\Scripts\python.exe"
$Cfg = Join-Path $RepoRoot "config.yaml"
$LogDir = Join-Path $RepoRoot "logs"
New-Item -ItemType Directory -Force -Path $LogDir | Out-Null
function RunPy([string]$script, [string]$args="") {
& $Py $script $args 2>&1 | Tee-Object -FilePath (Join-Path $LogDir ((Split-Path $script -Leaf) + ".log")) -Append
}
'@
Write-UTF8 "ops/run_collector.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) { try { RunPy "$RepoRoot\fetch__mt5_data.py" } catch { Write-Host "Collector error: $_" } Start-Sleep -Seconds 300 }
'@
Write-UTF8 "ops/run_features.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) { try { RunPy "$RepoRoot\features\build_features.py" } catch { Write-Host "Features error: $_" } Start-Sleep -Seconds 600 }
'@
Write-UTF8 "ops/run_paper.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) { try { RunPy "$RepoRoot\paper_demo\paper_loop_mt5_demo.py" } catch { Write-Host "Paper loop crash: $_"; Start-Sleep -Seconds 10 } Start-Sleep -Seconds 2 }
'@
Write-UTF8 "ops/run_train_short.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) {
try {
RunPy "$RepoRoot\rl\train_ppo.py" "--timesteps 500000 --seed 42 --eval_freq 100000"
RunPy "$RepoRoot\rl\evaluate.py" "--max_eval_steps 3000 --out_dir reports_live"
RunPy "$RepoRoot\ops\save_candidate.py"
& $Py "$RepoRoot\ops\promote_challenger.py"
} catch { Write-Host "Short training error: $_" }
Start-Sleep -Seconds 14400
}
'@
Write-UTF8 "ops/run_train_nightly.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) {
try {
while ( (Get-Date).ToUniversalTime().Hour -ne 23 ) { Start-Sleep -Seconds 600 }
Start-Sleep -Seconds 300
RunPy "$RepoRoot\rl\train_ppo.py" "--timesteps 2000000 --seed 42 --eval_freq 200000"
RunPy "$RepoRoot\rl\walk_forward.py" "--segments 6 --train_days 120 --val_days 30 --timesteps 500000 --out_dir reports_wf"
RunPy "$RepoRoot\rl\evaluate.py" "--max_eval_steps 3000 --out_dir reports_nightly"
RunPy "$RepoRoot\ops\save_candidate.py"
& $Py "$RepoRoot\ops\promote_challenger.py"
} catch { Write-Host "Nightly training error: $_" }
Start-Sleep -Seconds 3600
}
'@
Write-UTF8 "ops/save_candidate.py" @'
import json, shutil
from pathlib import Path
ROOT = Path(file).resolve().parents[1]
REG = ROOT / 'models' / 'registry'; REG.mkdir(parents=True, exist_ok=True)
MODEL = ROOT / 'models' / 'ppo_xauusd_m5.zip'
VEC   = ROOT / 'models' / 'vecnorm_xauusd_m5.pkl'
METR  = ROOT / 'reports' / 'val_metrics.txt'
EQP   = ROOT / 'reports' / 'equity_val.png'
def parse_metrics_txt(p: Path):
d = {}
if p.exists():
txt = p.read_text(encoding='utf-8')
for line in txt.splitlines():
if ':' in line:
k, v = line.split(':', 1)
k = k.strip().lower().replace(' ', '')
try: d[k] = float(v.strip())
except: pass
return d
def main():
m = parse_metrics_txt(METR)
if not m:
print('[save_candidate] No metrics.'); return
tag = 'cand' + Path.cwd().name + '_'
n=0
while (REG / f"{tag}{n:02d}").exists(): n+=1
dst = REG / f"{tag}{n:02d}"
dst.mkdir(parents=True, exist_ok=True)
shutil.copy2(MODEL, dst/'model.zip')
if VEC.exists(): shutil.copy2(VEC, dst/'vecnorm.pkl')
if EQP.exists(): shutil.copy2(EQP, dst/'equity.png')
(dst/'metrics.json').write_text(json.dumps({
'final_equity': m.get('final_equity'),
'max_dd': m.get('max_drawdown'),
'sharpe': m.get('sharpe_daily'),
}, indent=2), encoding='utf-8')
print(f'[save_candidate] Saved candidate -> {dst}')
if name == 'main': main()
'@
Write-UTF8 "ops/promote_challenger.py" @'
import json, shutil
from pathlib import Path
ROOT = Path(file).resolve().parents[1]
REG = ROOT / 'models' / 'registry'; REG.mkdir(parents=True, exist_ok=True)
CUR_TXT = REG / 'current.txt'
MODEL_DST = ROOT / 'models' / 'ppo_xauusd_m5.zip'
VEC_DST   = ROOT / 'models' / 'vecnorm_xauusd_m5.pkl'
def load_metrics(d: Path):
if not d: return None
m = d / 'metrics.json'
return json.loads(m.read_text('utf-8')) if m.exists() else None
def pick_latest_dir():
dirs = [p for p in REG.iterdir() if p.is_dir()]
return sorted(dirs, key=lambda p: p.name)[-1] if dirs else None
def better(m_new, m_old):
if m_old is None: return True
s_new, s_old = m_new.get('sharpe', 0.0) or 0.0, m_old.get('sharpe', 0.0) or 0.0
dd_new, dd_old = m_new.get('max_dd', -1.0) or -1.0, m_old.get('max_dd', -1.0) or -1.0
dd_ok = (dd_new >= dd_old * 1.2)
s_ok  = (s_new >= s_old * 1.10) or (s_new >= 0.10 and s_new > s_old)
fe_new, fe_old = m_new.get('final_equity', 0.0) or 0.0, m_old.get('final_equity', 0.0) or 0.0
if (s_new == 0.0 and s_old == 0.0):
return (fe_new > fe_old) and dd_ok
return s_ok and dd_ok
def promote(new_dir: Path):
REG.mkdir(parents=True, exist_ok=True)
(REG/'current.txt').write_text(new_dir.name, encoding='utf-8')
shutil.copy2(new_dir/'model.zip', MODEL_DST)
if (new_dir/'vecnorm.pkl').exists(): shutil.copy2(new_dir/'vecnorm.pkl', VEC_DST)
print(f"[promote] Champion -> {new_dir.name}")
def main():
latest = pick_latest_dir()
if latest is None: print('[promote] No candidates.'); return
new_m = load_metrics(latest)
if new_m is None: print('[promote] Latest has no metrics.json'); return
cur_dir = REG / CUR_TXT.read_text('utf-8').strip() if CUR_TXT.exists() else None
old_m = load_metrics(cur_dir) if cur_dir and cur_dir.exists() else None
if better(new_m, old_m): promote(latest)
else: print('[promote] Challenger not better. No promotion.')
if name == 'main': main()
'@
Write-UTF8 "ops/install_services.ps1" @'
$nssm = "C:\nssm\nssm.exe"
$root = Split-Path -Parent $MyInvocation.MyCommand.Path
$ps = "powershell.exe"
& $nssm install XAU_Collector   $ps "-ExecutionPolicy Bypass -File "$root\run_collector.ps1""
& $nssm set    XAU_Collector   AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_Collector   Start SERVICE_AUTO_START
& $nssm install XAU_Features    $ps "-ExecutionPolicy Bypass -File "$root\run_features.ps1""
& $nssm set    XAU_Features    AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_Features    Start SERVICE_AUTO_START
& $nssm install XAU_Paper       $ps "-ExecutionPolicy Bypass -File "$root\run_paper.ps1""
& $nssm set    XAU_Paper       AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_Paper       Start SERVICE_AUTO_START
& $nssm install XAU_TrainShort  $ps "-ExecutionPolicy Bypass -File "$root\run_train_short.ps1""
& $nssm set    XAU_TrainShort  AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_TrainShort  Start SERVICE_AUTO_START
& $nssm install XAU_TrainNightly $ps "-ExecutionPolicy Bypass -File "$root\run_train_nightly.ps1""
& $nssm set    XAU_TrainNightly AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_TrainNightly Start SERVICE_AUTO_START
'@
# =========================================
# quick_env_test + features_spec placeholder
# =========================================
Write-UTF8 "quick_env_test.py" @'
import yaml, pandas as pd
from env_xau import XauTradingEnv
cfg=yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
df=pd.read_csv(cfg['files']['features_csv'], parse_dates=['time']).sort_values('time').reset_index(drop=True)
df=df.tail(max(int(cfg.get('window',128))+256,512))
env=XauTradingEnv(df, window=int(cfg.get('window',128)),
spread_abs=cfg['costs']['spread_abs'], commission_rate=cfg['costs']['commission_rate'], slippage_k=cfg['costs']['slippage_k'],
reward_mode=cfg.get('env',{}).get('reward_mode', cfg.get('reward_mode','pct')), use_close_norm=True,
min_equity=float(cfg.get('env',{}).get('min_equity',0.8)))
obs,info=env.reset(); print('reset OK', len(obs))
obs,r,term,trunc,info=env.step(1); print('step OK', r, term, trunc)
'@
Write-UTF8 "models/features_spec.json" '{"feature_columns": [], "price_column": "close_norm"}'
Write-Host "OK – repo files created in $Root"









=== FILE: check.py ===

import pandas as pd
df = pd.read_csv("data/XAUUSD_M5.csv", parse_dates=["time"])
print("bars rows:", len(df), "from:", df["time"].min(), "to:", df["time"].max())
print(df.tail(3))

=== FILE: config.yaml ===

symbol: GOLD.pro
timeframe: M5
history_days: 720
window: 128
mt5:
  bars_chunk: 100000
  update_days: 60
files:
  bars_csv: data/XAUUSD_M5.csv
  ticks_csv: data/XAUUSD_ticks_sample.csv
  features_csv: data/XAUUSD_M5_features.csv
  model_path: models/ppo_xauusd_m5.zip
  vecnorm_path: models/vecnorm_xauusd_m5.pkl
costs:
  spread_abs: 0.42
  commission_rate: 0.0001
  slippage_k: 0.0
env:
  reward_mode: pct
  flip_penalty: 0.002
  trade_hours_utc:
  - 06:00
  - '20:00'
  enforce_flat_outside_hours: true
  min_equity: 0.8
fundamentals:
  usd_series_id: DTWEXBGS
  gpr_csv: external/gpr/gpr.csv
  cot_csv: external/cot_gold.csv
calendar:
  calendar_csv: external/calendar_us.csv
  events_whitelist:
  - Non Farm Payrolls
  - Unemployment Rate
  - CPI
  - Core CPI
  - Core PCE
  - ISM Manufacturing PMI
  - ISM Services PMI
  - FOMC Interest Rate Decision
  - Fed Press Conference


=== FILE: env_xau.py ===

# -*- coding: utf-8 -*-
import numpy as np, pandas as pd
import gymnasium as gym
from gymnasium import spaces
from datetime import time as dtime

class XauTradingEnv(gym.Env):
    metadata = {"render_modes": []}

    def __init__(self, df: pd.DataFrame, window=128,
                 spread_abs=0.05, commission_rate=0.0001, slippage_k=0.10,
                 reward_mode: str = "pct", use_close_norm: bool = True,
                 flip_penalty: float = 0.0, trade_hours_utc=None,
                 enforce_flat_outside_hours: bool = True, features_spec: list | None = None,
                 min_equity: float = 0.8):
        super().__init__()
        self.df = df.sort_values('time').reset_index(drop=True)
        self.window = int(window)
        self.spread_abs = float(spread_abs)
        self.commission_rate = float(commission_rate)
        self.slippage_k = float(slippage_k)
        assert reward_mode in {"pct","points"}
        self.reward_mode = reward_mode
        self.use_close_norm = use_close_norm
        self.flip_penalty = float(flip_penalty)
        self.enforce_flat_outside = bool(enforce_flat_outside_hours)
        self.min_equity = float(min_equity)

        # Okno godzin handlu (UTC)
        self.trade_hours = None
        if trade_hours_utc and isinstance(trade_hours_utc, (list, tuple)) and len(trade_hours_utc) == 2:
            try:
                s = [int(x) for x in str(trade_hours_utc[0]).split(":")]
                e = [int(x) for x in str(trade_hours_utc[1]).split(":")]
                self.trade_hours = (dtime(s[0], s[1] if len(s) > 1 else 0),
                                    dtime(e[0], e[1] if len(e) > 1 else 0))
            except Exception:
                self.trade_hours = None

        # Kolumny cech
        base_cols = ['open', 'high', 'low', 'close', 'tick_volume', 'spread', 'time']
        if features_spec is not None:
            missing = [c for c in features_spec if c not in self.df.columns]
            if missing:
                raise ValueError(f"Missing feature columns: {missing}")
            self.feat_cols = list(features_spec)
        else:
            self.feat_cols = [c for c in self.df.columns if c not in base_cols]

        self.price_col = 'close_norm' if (self.use_close_norm and 'close_norm' in self.df.columns) else 'close'
        if len(self.df) <= self.window:
            raise ValueError(f"Not enough rows: {len(self.df)} <= window {self.window}")

        # Observation: [window x (1 + n_features)] + [pos, unrealized]
        obs_dim = self.window * (1 + len(self.feat_cols)) + 2
        self.action_space = spaces.Discrete(3)  # 0=SHORT, 1=FLAT, 2=LONG
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)

        # Stan początkowy
        self._start = self.window
        self._i = None
        self.pos = 0
        self.entry = None
        self.equity = 1.0
        self.prev_eq = 1.0

    def _in_trade_hours(self, ts) -> bool:
        if self.trade_hours is None:
            return True
        try:
            t = ts.to_pydatetime().time()
        except Exception:
            t = ts
        start, end = self.trade_hours
        if start <= end:
            return (t >= start) and (t <= end)
        # okno nocne np. 22:00–06:00
        return (t >= start) or (t <= end)

    def _obs(self):
        sl = slice(self._i - self.window, self._i)
        block_df = self.df.iloc[sl][[self.price_col] + self.feat_cols]
        block = block_df.to_numpy(dtype=np.float32)
        expected = 1 + len(self.feat_cols)
        if block.shape != (self.window, expected):
            raise RuntimeError(f"Bad window shape: {block.shape} vs {(self.window, expected)}")
        flat = block.flatten()

        price = float(self.df.iloc[self._i]['close'])
        unreal = 0.0
        if self.pos != 0 and self.entry is not None:
            dir_ = 1 if self.pos > 0 else -1
            unreal = dir_ * (price - self.entry)

        return np.concatenate([flat, np.array([self.pos, unreal], dtype=np.float32)])

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self._i = self._start
        self.pos = 0
        self.entry = None
        self.equity = 1.0
        self.prev_eq = 1.0
        return self._obs(), {}

    def step(self, action):
        if isinstance(action, (np.ndarray, list, tuple)):
            action = int(action[0])
        else:
            action = int(action)
        if not self.action_space.contains(action):
            raise ValueError("Invalid action")

        info = {}
        ts = self.df.iloc[self._i]['time']
        inside = self._in_trade_hours(ts)
        price = float(self.df.iloc[self._i]['close'])
        prev = float(self.df.iloc[self._i - 1]['close'])
        slip = self.slippage_k * abs(price - prev)

        desired = [-1, 0, 1][action]
        if not inside and self.enforce_flat_outside:
            desired = 0

        # Zmiana pozycji (koszty + ewentualny flip penalty)
        if desired != self.pos:
            # flip penalty gdy odwracasz kierunek bez przejścia na 0
            if self.pos != 0 and desired != 0 and np.sign(self.pos) != np.sign(desired) and self.flip_penalty > 0:
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - self.flip_penalty, 1e-6)
                else:
                    self.equity -= self.flip_penalty
                info['flip_penalty'] = float(self.flip_penalty)

            # koszt zamknięcia starej pozycji
            if self.pos != 0 and self.entry is not None:
                cost = (self.spread_abs + slip) / max(price, 1e-12) + self.commission_rate
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - cost, 1e-6)
                else:
                    self.equity -= cost

            # otwórz nową (lub wyzeruj)
            self.pos = desired
            if self.pos != 0:
                cost = (self.spread_abs + slip) / max(price, 1e-12) + self.commission_rate
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - cost, 1e-6)
                else:
                    self.equity -= cost
                self.entry = price
            else:
                self.entry = None

        # Zmiana equity w kroku
        if self.reward_mode == 'pct':
            step_ret = 0.0
            if self.pos != 0:
                dir_ = 1 if self.pos > 0 else -1
                step_ret = dir_ * ((price / max(prev, 1e-12)) - 1.0)
            self.equity *= (1.0 + step_ret)
            reward = float(self.equity - self.prev_eq)
        else:
            reward = float(self.equity - self.prev_eq)

        self.prev_eq = self.equity
        self._i += 1

        terminated = bool(self._i >= len(self.df) - 1)
        truncated = False
        if self.equity <= self.min_equity:
            truncated = True
            info['early_stop'] = True

        info.update({
            'time': ts,
            'price': price,
            'pos': int(self.pos),
            'equity': float(self.equity),
            'inside_hours': bool(inside)
        })
        return self._obs(), reward, terminated, truncated, info

=== FILE: features_check.py ===

import pandas as pd

df = pd.read_csv("data/XAUUSD_M5_features.csv", parse_dates=["time"])
print("features rows:", len(df), "from:", df["time"].min(), "to:", df["time"].max())
print(df.tail(3))

=== FILE: fetch__mt5_data.py ===

# -*- coding: utf-8 -*-
"""
fetch__mt5_data.py
------------------
Pobiera świece M5 (inkrementalnie do 720 dni) oraz próbkę ticków z MT5.

• Inkrementalny merge do jednego CSV z 720 dni (przycinanie, deduplikacja po 'time').
• Obejścia brokerów: copy_rates_range (naive datetimes) z fallbackiem do copy_rates_from_pos (tail),
  retry/backoff, chunkowanie przy pierwszym pobraniu.
• Atomiczny zapis CSV.
• Ticki weekend-safe (ostrzeżenie, nie przerywa).
• Sugestie kosztów (median/p75 spread + heurystyka slippage_k).
• Czas wszędzie UTC-aware.

Użycie:
    python fetch__mt5_data.py
"""

from __future__ import annotations

import MetaTrader5 as mt5
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
from pathlib import Path
import time
import yaml

# nasze utilsy
from utils.atomic import atomic_write_csv
from utils.mt5_health import ensure_mt5_ready

# ============================================================
# Konfiguracja
# ============================================================
CFG_PATH = 'config.yaml'
cfg = yaml.safe_load(open(CFG_PATH, 'r', encoding='utf-8'))

if not isinstance(cfg, dict) or 'files' not in cfg:
    raise RuntimeError(f"{CFG_PATH} jest pusty lub ma złą strukturę (brak sekcji 'files').")

SYMBOL = cfg.get('symbol', 'XAUUSD')
BARS_CSV = cfg['files']['bars_csv']
TICKS_CSV = cfg['files']['ticks_csv']
HISTORY_DAYS = int(cfg.get('history_days', 720))

TF_MAP = {
    'M1':  mt5.TIMEFRAME_M1,
    'M5':  mt5.TIMEFRAME_M5,
    'M15': mt5.TIMEFRAME_M15,
    'H1':  mt5.TIMEFRAME_H1,
}
TF_NAME = str(cfg.get('timeframe', 'M5'))
TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)

MT5_CFG = cfg.get('mt5', {}) or {}
CHUNK = int(MT5_CFG.get('bars_chunk', 20000))   # 20k ~ 70–90 dni M5 (zależnie od brokera)
UPDATE_DAYS = int(MT5_CFG.get('update_days', 60))

# ============================================================
# MT5 init / symbol utils
# ============================================================
def init_mt5():
    """Initialize MT5 i wypisz krótkie info o terminalu/koncie."""
    if not ensure_mt5_ready():
        last_err = mt5.last_error()
        raise RuntimeError(f"MT5 not ready (terminal/account). Start MT5 and login. last_error={last_err}")
    term = mt5.terminal_info()
    acc = mt5.account_info()
    parts = []
    if getattr(term, "company", None): parts.append(f"Company={term.company}")
    if getattr(term, "name", None):    parts.append(f"TerminalName={term.name}")
    if getattr(acc, "login", 0):       parts.append(f"Login={acc.login}")
    if getattr(acc, "server", None):   parts.append(f"Server={acc.server}")
    if getattr(acc, "name", None):     parts.append(f"AccountName={acc.name}")
    print("[MT5] " + " | ".join(parts) if parts else "[MT5] initialized")

def ensure_symbol(symbol: str) -> bool:
    """Upewnij się, że symbol jest widoczny w Market Watch."""
    info = mt5.symbol_info(symbol)
    if info is None:
        mt5.symbol_select(symbol, True)
        info = mt5.symbol_info(symbol)
        if info is None:
            return False
    if not info.visible:
        if not mt5.symbol_select(symbol, True):
            return False
    return True

def suggest_similar(symbol: str, limit=12):
    """Podpowiedz nazwy symboli powiązanych (np. GOLD/XAU) – zależne od serwera."""
    out = []
    try:
        for s in mt5.symbols_get():
            nm = s.name.upper()
            if ("GOLD" in nm) or ("XAU" in nm):
                out.append(s.name)
        if not out:
            out = [s.name for s in mt5.symbols_get()][:limit]
        return sorted(set(out))[:limit]
    except Exception:
        return []

# ============================================================
# Helpers
# ============================================================
def with_retries(fn, attempts=3, sleep_s=2, *a, **kw):
    last = None
    for i in range(attempts):
        try:
            return fn(*a, **kw)
        except Exception as e:
            last = e
            time.sleep(sleep_s * (i + 1))
    if last:
        raise last

def _to_df(rates) -> pd.DataFrame:
    """Konwersja stawek MT5 -> kanoniczny DataFrame z sanity-checkiem kolumn i czasu."""
    df = pd.DataFrame(rates)
    if df.empty:
        return df
    df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
    if 'real_volume' in df.columns:
        df.rename(columns={'real_volume': 'tick_volume'}, inplace=True)
    cols = ['time', 'open', 'high', 'low', 'close', 'tick_volume', 'spread']
    # zachowaj tylko znane kolumny
    df = df[[c for c in cols if c in df.columns]].copy()
    # usuń zduplikowane nazwy i rekordy
    df = (df.loc[:, ~df.columns.duplicated()]
            .sort_values('time')
            .drop_duplicates('time')
            .reset_index(drop=True))
    return df

def _make_naive(dt: datetime) -> datetime:
    """Zwróć 'naive' datetime (bez tzinfo) – część brokerów tego wymaga dla copy_rates_range."""
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

# ============================================================
# Pobieranie danych
# ============================================================
def fetch_range(symbol, timeframe, utc_from: datetime, utc_to: datetime) -> pd.DataFrame:
    """
    Pobierz zakres 'range' z obejściami:
    • daty 'naive' (bez tzinfo),
    • fallback: tail od końca (copy_rates_from_pos) gdy range zwraca pustkę/Invalid params.
    """
    frm_n = _make_naive(utc_from)
    to_n  = _make_naive(utc_to)
    rates = mt5.copy_rates_range(symbol, timeframe, frm_n, to_n)
    if rates is None or len(rates) == 0:
        # fallback: tail od końca
        tail = mt5.copy_rates_from_pos(symbol, timeframe, 0, CHUNK)
        if tail is None or len(tail) == 0:
            last_err = mt5.last_error()
            raise RuntimeError(f"copy_rates_range empty for '{symbol}'. last_error={last_err}")
        return _to_df(tail)
    return _to_df(rates)

def fetch_from_pos_tail(symbol, timeframe, count) -> pd.DataFrame:
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None or len(rates) == 0:
        last_err = mt5.last_error()
        raise RuntimeError(f"copy_rates_from_pos empty for '{symbol}'. last_error={last_err}")
    return _to_df(rates)

def load_existing_bars(path: str):
    p = Path(path)
    if not p.exists():
        return None
    try:
        df = pd.read_csv(p, parse_dates=['time'])
        if df.empty:
            return None
        # Usuń kolumny techniczne i duplikaty nazw
        bad = [c for c in df.columns if str(c).startswith('Unnamed')]
        if bad:
            df = df.drop(columns=bad)
        df = df.loc[:, ~df.columns.duplicated()]
        keep = ['time','open','high','low','close','tick_volume','spread']
        df = df[[c for c in keep if c in df.columns]].copy()
        # Upewnij się, że 'time' jest UTC-aware
        if pd.api.types.is_datetime64_any_dtype(df['time']):
            if df['time'].dt.tz is None:
                df['time'] = df['time'].dt.tz_localize('UTC')
            else:
                df['time'] = df['time'].dt.tz_convert('UTC')
        else:
            df['time'] = pd.to_datetime(df['time'], utc=True)
        df = (df.sort_values('time')
                .drop_duplicates('time')
                .reset_index(drop=True))
        return df
    except Exception:
        return None

def merge_clip(existing: pd.DataFrame | None, new_df: pd.DataFrame, keep_days: int) -> pd.DataFrame:
    """Scal istniejące i nowe świece, deduplikuj po 'time', przytnij do keep_days i oczyść nagłówki."""
    def _sanitize(d):
        if d is None or d.empty:
            return d
        d = d.loc[:, ~d.columns.duplicated()]
        bad = [c for c in d.columns if str(c).startswith('Unnamed')]
        if bad:
            d = d.drop(columns=bad)
        return d

    existing = _sanitize(existing)
    new_df   = _sanitize(new_df)

    if existing is None or existing.empty:
        base = new_df.copy()
    else:
        common = [c for c in existing.columns if c in new_df.columns]
        base = (pd.concat([existing[common], new_df[common]], axis=0, ignore_index=True, copy=False)
                  .drop_duplicates('time')
                  .sort_values('time'))

    # time -> UTC-aware
    if pd.api.types.is_datetime64_any_dtype(base['time']):
        if base['time'].dt.tz is None:
            base['time'] = base['time'].dt.tz_localize('UTC')
        else:
            base['time'] = base['time'].dt.tz_convert('UTC')
    else:
        base['time'] = pd.to_datetime(base['time'], utc=True)

    # clip do okna czasowego
    cutoff = datetime.now(timezone.utc) - timedelta(days=keep_days + 1)
    base = base[base['time'] >= pd.Timestamp(cutoff)].reset_index(drop=True)
    return base

def fetch_bars_incremental(symbol, timeframe, keep_days: int, update_days: int) -> pd.DataFrame:
    """
    Strategia:
    • Gdy brak pliku – zbierz historię 'od końca' (copy_rates_from_pos) w 1–4 chunkach,
      aż pokryjesz ~keep_days (z marginesem). Jeśli to się nie uda – spróbuj krótkiego range.
    • Gdy plik istnieje – dociągnij ostatnie update_days (mały range z fallbackiem).
    """
    existing = load_existing_bars(BARS_CSV)
    if existing is None:
        wanted_days = keep_days + 2
        parts_df = None
        # 1–4 próby z narastającą liczbą świec (od końca)
        for i in range(1, 5):
            count = CHUNK * i
            try:
                dft = with_retries(lambda: fetch_from_pos_tail(symbol, timeframe, count))
            except Exception:
                time.sleep(1.0)
                continue
            if dft is not None and len(dft) > 0:
                parts_df = dft
                span_days = (parts_df['time'].max() - parts_df['time'].min()).days
                if span_days >= wanted_days:
                    break
        if parts_df is None or parts_df.empty:
            # Ostatnia próba – krótki range (np. 60 dni)
            to  = datetime.now(timezone.utc)
            frm = to - timedelta(days=min(60, wanted_days))
            parts_df = fetch_range(symbol, timeframe, frm, to)
        return merge_clip(None, parts_df, keep_days)

    # Inkrementalnie: dociągnij update_days (mały range z fallbackiem)
    to  = datetime.now(timezone.utc)
    frm = to - timedelta(days=max(2, update_days))
    fresh = fetch_range(symbol, timeframe, frm, to)
    merged = merge_clip(existing, fresh, keep_days)
    return merged
def fetch_ticks(symbol: str, hours: int = 24) -> pd.DataFrame:
    utc_to   = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(hours=hours)
    ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)
    if ticks is None or len(ticks) == 0:
        last_err = mt5.last_error()
        raise RuntimeError(f"No MT5 ticks for '{symbol}'. last_error={last_err}")
    tdf = pd.DataFrame(ticks)
    tdf['time'] = pd.to_datetime(tdf['time'], unit='s', utc=True)
    return tdf[['time', 'bid', 'ask', 'last', 'volume']]

def suggest_costs(tdf: pd.DataFrame) -> dict:
    """Sugeruj koszty na bazie ticków: median/p75 spread, heurystyczny slippage_k."""
    spr = (tdf['ask'] - tdf['bid']).astype(float).replace([np.inf, -np.inf], np.nan).dropna()
    med_spread = float(np.median(spr)) if len(spr) else 0.0
    p75_spread = float(np.percentile(spr, 75)) if len(spr) else 0.0
    mid = (tdf['ask'] + tdf['bid']) / 2.0
    dm = (mid.diff().abs()).replace([np.inf, -np.inf], np.nan).dropna()
    med_dm = float(np.median(dm)) if len(dm) else 0.0
    med_price = float(np.nanmedian(mid)) if len(mid) else 1.0
    # heurystyka: jaką część typowego ruchu stanowi przeciętny skok mida
    slippage_k = float(min(max(med_dm / max(med_price, 1e-12), 0.0), 0.01))
    return {
        'spread_abs_median': round(med_spread, 5),
        'spread_abs_p75':    round(p75_spread, 5),
        'slippage_k_suggested': round(slippage_k, 4)
    }

# ============================================================
# Main
# ============================================================
def main():
    init_mt5()
    try:
        print(f"[CFG] symbol={SYMBOL} tf={TF_NAME} keep_days={HISTORY_DAYS} update_days={UPDATE_DAYS}")

        if not ensure_symbol(SYMBOL):
            similar = suggest_similar(SYMBOL)
            hint = ", ".join(similar) if similar else "(brak propozycji)"
            raise SystemExit(
                "Symbol not found or not visible in Market Watch: '{}'\n"
                "Try one of these (server-specific): {}\n"
                "Also open Symbols window and SHOW the instrument."
                .format(SYMBOL, hint)
            )

        # ŚWIECE
        bars = fetch_bars_incremental(SYMBOL, TF, keep_days=HISTORY_DAYS, update_days=UPDATE_DAYS)
        assert {'time', 'open', 'high', 'low', 'close'}.issubset(bars.columns), "Bars frame malformed"
        assert len(bars) > 100, "Too few bars"
        atomic_write_csv(bars, BARS_CSV)
        print(f"Saved bars: {BARS_CSV} ({len(bars)})")

        # TICKI (opcjonalnie; weekend-safe)
        try:
            ticks = with_retries(lambda: fetch_ticks(SYMBOL, hours=24))
            atomic_write_csv(ticks, TICKS_CSV)
            print(f"Saved ticks: {TICKS_CSV} ({len(ticks)})")

            sugg = suggest_costs(ticks)
            rep = Path('reports'); rep.mkdir(parents=True, exist_ok=True)
            Path('reports/costs_suggestion.yaml').write_text(
                yaml.safe_dump(sugg, allow_unicode=True, sort_keys=False),
                encoding='utf-8'
            )
            print("Cost suggestions -> reports/costs_suggestion.yaml")
        except Exception as e:
            # np. weekend: brak ticków (OK)
            print(f"[warn] ticks unavailable ({e}); keeping previous {TICKS_CSV}")

    finally:
        mt5.shutdown()

if __name__ == '__main__':
    main()

=== FILE: merged_repo.txt ===



=== FILE: bootstrap_pack.ps1 ===

# bootstrap_pack.ps1
$ErrorActionPreference = "Stop"

# =========================================
# Root + katalogi
# =========================================
$Root = Split-Path -Parent $MyInvocation.MyCommand.Path
$dirs = @("", "data", "features", "rl", "paper_demo", "utils", "models", "models/registry", "reports", "ops", "logs")
foreach($d in $dirs){ New-Item -ItemType Directory -Force -Path (Join-Path $Root $d) | Out-Null }

function Write-UTF8($RelPath, $Content){
  $Path = Join-Path $Root $RelPath
  $dir = Split-Path -Parent $Path
  if(!(Test-Path $dir)){ New-Item -ItemType Directory -Force -Path $dir | Out-Null }
  $Content | Out-File -FilePath $Path -Encoding utf8 -Force
}

# =========================================
# README
# =========================================
Write-UTF8 "README.md" @'
# XAUUSD RL PPO (M5) – Starter (Demo/Edu)
**Uwaga**: tylko do celów edukacyjnych, backtestów i *paper tradingu* (demo). Brak kodu wysyłającego realne zlecenia.

## Szybki start
```powershell
python -m venv .venv
# .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
# Na końcu zainstaluj TORCH odpowiedni dla CPU/GPU z pytorch.org (wheel).
python fetch__mt5_data.py
python utils\calibration.py
python utils\calibration.py --apply
python features\build_features.py
python rl\train_ppo.py --timesteps 1500000
python rl\evaluate.py --out_dir reports
python utils\make_report.py

24/7 na Windows
Użyj skryptów w ops\ oraz NSSM (patrz ops\install_services.ps1).
Dane i historia
Trzymamy 720 dni historii świec. Zbieracz działa inkrementalnie: dociąga ostatnie update_days (domyślnie 60), scala z istniejacym CSV i przycina do 720 dni. Zapisy CSV są atomowe.
'@
# =========================================
# requirements.txt
# =========================================
Write-UTF8 "requirements.txt" @'
pandas>=2.0
numpy>=1.24
pytz
PyYAML
joblib
matplotlib
tensorboard
MetaTrader5
gymnasium>=0.29
stable-baselines3>=2.2.1
torch: zainstaluj odpowiedni wheel (CPU/GPU) z pytorch.org
'@
# =========================================
# config.yaml
# =========================================
Write-UTF8 "config.yaml" @'
symbol: "XAUUSD"
timeframe: "M5"
history_days: 720  # wymagane okno historii
window: 128
mt5:
bars_chunk: 20000   # ile świec pobierać jednorazowo przy "from_pos" (~70-90 dni M5)
# update_days: 60     # inkrementalny zakres odświeżania (do scalenia z istniejacym CSV)
files:
bars_csv: "data/XAUUSD_M5.csv"
ticks_csv: "data/XAUUSD_ticks_sample.csv"
features_csv: "data/XAUUSD_M5_features.csv"
model_path: "models/ppo_xauusd_m5.zip"
vecnorm_path: "models/vecnorm_xauusd_m5.pkl"
costs:
spread_abs: 0.05
commission_rate: 0.0001
slippage_k: 0.10
env:
reward_mode: "pct"
flip_penalty: 0.002
trade_hours_utc: ["06:00", "20:00"]
enforce_flat_outside_hours: true
min_equity: 0.8
'@
# =========================================
# utils/atomic.py
# =========================================
Write-UTF8 "utils/atomic.py" @'
from pathlib import Path
def atomic_write_csv(df, path: str):
p = Path(path)
p.parent.mkdir(parents=True, exist_ok=True)
tmp = p.with_suffix(p.suffix + ".tmp")
df.to_csv(tmp, index=False)
tmp.replace(p)
'@
# =========================================
# utils/mt5_health.py
# =========================================
Write-UTF8 "utils/mt5_health.py" @'
import MetaTrader5 as mt5, time
def ensure_mt5_ready(retries=3, sleep_s=2):
"""Ponawia initialize i weryfikuje, że terminal + konto są gotowe."""
for i in range(retries):
if mt5.initialize():
ti, ai = mt5.terminal_info(), mt5.account_info()
if ti and ai and getattr(ai, "login", 0):
return True
mt5.shutdown()
time.sleep(sleep_s * (i + 1))
return False
'@
# =========================================
# env_xau.py
# =========================================
Write-UTF8 "env_xau.py" @'
-- coding: utf-8 --
import numpy as np, pandas as pd
import gymnasium as gym
from gymnasium import spaces
from datetime import time as dtime
class XauTradingEnv(gym.Env):
metadata = {"render_modes": []}
def init(self, df: pd.DataFrame, window=128,
spread_abs=0.05, commission_rate=0.0001, slippage_k=0.10,
reward_mode: str = "pct", use_close_norm: bool = True,
flip_penalty: float = 0.0, trade_hours_utc=None,
enforce_flat_outside_hours: bool = True, features_spec: list | None = None,
min_equity: float = 0.8):
super().init()
self.df = df.reset_index(drop=True)
self.window = int(window)
self.spread_abs = float(spread_abs)
self.commission_rate = float(commission_rate)
self.slippage_k = float(slippage_k)
assert reward_mode in {"pct","points"}
self.reward_mode = reward_mode
self.use_close_norm = use_close_norm
self.flip_penalty = float(flip_penalty)
self.enforce_flat_outside = bool(enforce_flat_outside_hours)
self.min_equity = float(min_equity)
self.trade_hours = None
if trade_hours_utc and isinstance(trade_hours_utc,(list,tuple)) and len(trade_hours_utc)==2:
try:
s = [int(x) for x in str(trade_hours_utc[0]).split(":")]
e = [int(x) for x in str(trade_hours_utc[1]).split(":")]
self.trade_hours = (dtime(s[0], s[1] if len(s)>1 else 0), dtime(e[0], e[1] if len(e)>1 else 0))
except Exception:
self.trade_hours = None
base_cols = ['open','high','low','close','tick_volume','spread','time']
if features_spec is not None:
missing = [c for c in features_spec if c not in self.df.columns]
if missing: raise ValueError(f"Missing feature columns: {missing}")
self.feat_cols = list(features_spec)
else:
self.feat_cols = [c for c in self.df.columns if c not in base_cols]
self.price_col = 'close_norm' if (self.use_close_norm and 'close_norm' in self.df.columns) else 'close'
if len(self.df) <= self.window:
raise ValueError(f"Not enough rows: {len(self.df)} <= window {self.window}")
obs_dim = self.window * (1 + len(self.feat_cols)) + 2
self.action_space = spaces.Discrete(3)
self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
self._start = self.window
self._i = None
self.pos = 0
self.entry = None
self.equity = 1.0
self.prev_eq = 1.0
def _in_trade_hours(self, ts)->bool:
if self.trade_hours is None: return True
try: t = ts.to_pydatetime().time()
except Exception: t = ts
start, end = self.trade_hours
if start <= end: return (t>=start) and (t<=end)
return (t>=start) or (t<=end)
def _obs(self):
sl = slice(self._i - self.window, self.i)
block_df = self.df.iloc[sl][[self.price_col] + self.feat_cols]
block = block_df.to_numpy(dtype=np.float32)
expected = 1 + len(self.feat_cols)
if block.shape != (self.window, expected):
raise RuntimeError(f"Bad window shape: {block.shape} vs {(self.window, expected)}")
flat = block.flatten()
price = float(self.df.iloc[self.i]['close'])
unreal = 0.0
if self.pos != 0 and self.entry is not None:
dir = 1 if self.pos>0 else -1
unreal = dir * (price - self.entry)
import numpy as np
return np.concatenate([flat, np.array([self.pos, unreal], dtype=np.float32)])
def reset(self, seed=None, options=None):
super().reset(seed=seed)
self._i = self._start
self.pos = 0; self.entry = None
self.equity = 1.0; self.prev_eq = 1.0
return self._obs(), {}
def step(self, action):
import numpy as np
if isinstance(action,(np.ndarray,list,tuple)): action = int(action[0])
else: action = int(action)
if not self.action_space.contains(action): raise ValueError("Invalid action")
info = {}
ts = self.df.iloc[self._i]['time']
inside = self._in_trade_hours(ts)
price = float(self.df.iloc[self.i]['close'])
prev = float(self.df.iloc[self.i-1]['close'])
slip = self.slippage_k * abs(price - prev)
desired = [-1,0,1][action]
if not inside and self.enforce_flat_outside: desired = 0
if desired != self.pos:
if self.pos != 0 and desired != 0 and np.sign(self.pos) != np.sign(desired) and self.flip_penalty>0:
if self.reward_mode == 'pct': self.equity *= max(1.0 - self.flip_penalty, 1e-6)
else: self.equity -= self.flip_penalty
info['flip_penalty'] = float(self.flip_penalty)
if self.pos != 0 and self.entry is not None:
cost = (self.spread_abs + slip)/max(price,1e-12) + self.commission_rate
if self.reward_mode == 'pct': self.equity *= max(1.0 - cost, 1e-6)
else: self.equity -= cost
self.pos = desired
if self.pos != 0:
cost = (self.spread_abs + slip)/max(price,1e-12) + self.commission_rate
if self.reward_mode == 'pct': self.equity *= max(1.0 - cost, 1e-6)
else: self.equity -= cost
self.entry = price
else:
self.entry = None
if self.reward_mode == 'pct':
step_ret = 0.0
if self.pos != 0:
dir = 1 if self.pos>0 else -1
step_ret = dir * ((price/max(prev,1e-12)) - 1.0)
self.equity *= (1.0 + step_ret)
reward = float(self.equity - self.prev_eq)
else:
reward = float(self.equity - self.prev_eq)
self.prev_eq = self.equity
self._i += 1
terminated = bool(self._i >= len(self.df) - 1)
truncated = False
if self.equity <= self.min_equity:
truncated = True
info['early_stop'] = True
info.update({'time': ts, 'price': price, 'pos': int(self.pos), 'equity': float(self.equity), 'inside_hours': bool(inside)})
return self._obs(), reward, terminated, truncated, info
'@
# =========================================
# rl/train_ppo.py
# =========================================
Write-UTF8 "rl/train_ppo.py" @'
-- coding: utf-8 --
"""PPO training with VecNormalize/Monitor/TimeLimit and features_spec."""
import argparse, yaml, json
import pandas as pd
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor, sync_envs_normalization
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.utils import set_random_seed
from gymnasium.wrappers import TimeLimit
from env_xau import XauTradingEnv
parser = argparse.ArgumentParser()
parser.add_argument('--timesteps', type=int, default=1500000)
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--eval_freq', type=int, default=100000)
parser.add_argument('--n_eval_episodes', type=int, default=1)
parser.add_argument('--max_train_steps', type=int, default=6000)
parser.add_argument('--max_eval_steps', type=int, default=3000)
args = parser.parse_args()
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = Path(cfg['files']['model_path'])
VECNORM_PATH = Path(cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl'))
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))
df = pd.read_csv(FEAT, parse_dates=['time'])
cut_val = df['time'].max() - pd.Timedelta(days=120)
train_df = df[df['time'] < cut_val].copy()
val_df   = df[df['time'] >= cut_val].copy()
features_spec = None
fspec = Path('models/features_spec.json')
if fspec.exists():
features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)
set_random_seed(args.seed)
def make_train():
env = XauTradingEnv(train_df, window=WINDOW,
spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=args.max_train_steps)
def make_eval():
env = XauTradingEnv(val_df, window=WINDOW,
spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=args.max_eval_steps)
venv_train = DummyVecEnv([make_train]); venv_train = VecMonitor(venv_train); venv_train = VecNormalize(venv_train, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)
venv_eval  = DummyVecEnv([make_eval]);  venv_eval  = VecMonitor(venv_eval);  venv_eval  = VecNormalize(venv_eval, training=False, norm_obs=True, norm_reward=False)
policy_kwargs = dict(net_arch=dict(pi=[128,128], vf=[128,128]))
try:
import tensorboard as _tb
tb_log_dir = 'logs/ppo_gold_m5'
except Exception:
tb_log_dir = None
model = PPO('MlpPolicy', venv_train, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02,
policy_kwargs=policy_kwargs, seed=args.seed, verbose=1, tensorboard_log=tb_log_dir)
sync_envs_normalization(venv_eval, venv_train)
eval_cb = EvalCallback(venv_eval, best_model_save_path=str(MODEL_PATH.parent), log_path='logs/eval',
eval_freq=max(args.eval_freq,1), n_eval_episodes=max(args.n_eval_episodes,1),
deterministic=True, render=False)
model.learn(total_timesteps=args.timesteps, callback=eval_cb)
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
model.save(str(MODEL_PATH))
venv_train.save(str(VECNORM_PATH))
print(f"Saved model: {MODEL_PATH}")
print(f"Saved VecNormalize: {VECNORM_PATH}")
'@
# =========================================
# rl/evaluate.py
# =========================================
Write-UTF8 "rl/evaluate.py" @'
-- coding: utf-8 --
"""Validate PPO on last ~120 days, save equity plot and trace CSV."""
import argparse, yaml, json
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from env_xau import XauTradingEnv
parser = argparse.ArgumentParser()
parser.add_argument('--max_eval_steps', type=int, default=3000)
parser.add_argument('--out_dir', type=str, default='reports')
args = parser.parse_args()
OUT = Path(args.out_dir); OUT.mkdir(parents=True, exist_ok=True)
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = cfg['files']['model_path']
VECNORM_PATH = cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl')
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))
df = pd.read_csv(FEAT, parse_dates=['time'])
cut_val = df['time'].max() - pd.Timedelta(days=120)
val = df[df['time']>=cut_val].copy()
features_spec = None
fspec = Path('models/features_spec.json')
if fspec.exists():
features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)
def make_eval():
env = XauTradingEnv(val, window=WINDOW,
spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=args.max_eval_steps)
venv = DummyVecEnv([make_eval]); venv = VecMonitor(venv)
if Path(VECNORM_PATH).exists():
venv = VecNormalize.load(VECNORM_PATH, venv)
venv.training=False; venv.norm_reward=False
model = PPO.load(MODEL_PATH, env=venv)
obs = venv.reset(); eq=[]; rows=[]; done=False
while not done:
action,_ = model.predict(obs, deterministic=True)
obs, rewards, dones, infos = venv.step(action)
equity = venv.get_attr('equity', indices=0)[0]
eq.append(float(equity))
info0 = infos[0] if isinstance(infos,(list,tuple)) else infos
rows.append({'time': str(info0.get('time','')), 'price': info0.get('price',np.nan), 'pos': info0.get('pos',np.nan),
'equity': equity, 'action': int(action[0]) if hasattr(action,'len') else int(action)})
done = bool(dones[0])
s = pd.Series(eq)
final_eq = float(s.iloc[-1]); min_eq=float(s.min()); peak=s.cummax(); max_dd=float((s/peak-1.0).min())
plt.figure(figsize=(10,4)); plt.plot(s.values); plt.title('Equity (validation)'); plt.tight_layout()
plt.savefig((OUT/'equity_val.png').as_posix(), dpi=150)
(OUT/'val_metrics.txt').write_text(
f"Final equity: {final_eq:.6f}\nMin equity: {min_eq:.6f}\nMax drawdown: {max_dd:.6f}\n", encoding='utf-8')
pd.DataFrame(rows).to_csv(OUT/'eval_trace.csv', index=False)
print("Saved validation results.")
'@
# =========================================
# rl/walk_forward.py
# =========================================
Write-UTF8 "rl/walk_forward.py" @'
-- coding: utf-8 --
"""Walk-forward evaluation with per-fold traces and CSV/TXT outputs."""
import argparse, yaml, json
import pandas as pd, numpy as np
from pathlib import Path
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from env_xau import XauTradingEnv
parser = argparse.ArgumentParser()
parser.add_argument('--segments', type=int, default=6)
parser.add_argument('--train_days', type=int, default=120)
parser.add_argument('--val_days', type=int, default=30)
parser.add_argument('--timesteps', type=int, default=500000)
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--out_dir', type=str, default='reports_wf')
args = parser.parse_args()
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))
out = Path(args.out_dir); out.mkdir(parents=True, exist_ok=True)
df = pd.read_csv(FEAT, parse_dates=['time']).sort_values('time').reset_index(drop=True)
end_time = df['time'].max()
features_spec=None
fspec = Path('models/features_spec.json')
if fspec.exists(): features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)
rows=[]
for k in range(args.segments):
val_end = end_time - pd.Timedelta(days=k*args.val_days)
val_start = val_end - pd.Timedelta(days=args.val_days)
train_end = val_start
train_start = train_end - pd.Timedelta(days=args.train_days)
tr = df[(df['time']>=train_start)&(df['time']<train_end)].copy()
va = df[(df['time']>=val_start)&(df['time']<val_end)].copy()
if len(tr)<=WINDOW or len(va)<=WINDOW: continue
def make_train():
env = XauTradingEnv(tr, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=6000)
def make_eval():
env = XauTradingEnv(va, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
return TimeLimit(env, max_episode_steps=3000)
vt = DummyVecEnv([make_train]); vt = VecMonitor(vt); vt = VecNormalize(vt, norm_obs=True, norm_reward=True)
ve = DummyVecEnv([make_eval]);  ve = VecMonitor(ve)
model = PPO('MlpPolicy', vt, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02, seed=args.seed, verbose=0)
model.learn(total_timesteps=args.timesteps)
tmp = out/f'vecnorm_{k}.pkl'; vt.save(str(tmp))
ve = VecNormalize.load(str(tmp), ve); ve.training=False; ve.norm_reward=False
obs = ve.reset(); eq=[]; trace=[]; done=False
while not done:
action,=model.predict(obs, deterministic=True)
obs, rewards, dones, infos = ve.step(action)
equity = ve.get_attr('equity', indices=0)[0]
eq.append(float(equity))
info0=infos[0] if isinstance(infos,(list,tuple)) else infos
trace.append({'k':k,'time':str(info0.get('time','')),'price':info0.get('price',np.nan),'pos':info0.get('pos',np.nan),'equity':equity,
'action':int(action[0]) if hasattr(action,'len') else int(action)})
done = bool(dones[0])
s = pd.Series(eq); final_eq=float(s.iloc[-1]); min_eq=float(s.min()); peak=s.cummax(); max_dd=float((s/peak-1.0).min())
rows.append({'k':k,'train_start':train_start,'train_end':train_end,'val_start':val_start,'val_end':val_end,'final_eq':final_eq,'min_eq':min_eq,'max_dd':max_dd,'n_steps':int(len(s))})
pd.DataFrame(trace).to_csv(out/f'fold{k}_trace.csv', index=False)
res = pd.DataFrame(rows).sort_values('k'); res.to_csv(out/'wf_results.csv', index=False)
if res.empty:
txt=['# Walk-Forward Report','No results (windows too short?).']
else:
agg={'folds':len(res),'final_eq_med':float(res['final_eq'].median()),'max_dd_med':float(res['max_dd'].median()),'n_steps_sum':int(res['n_steps'].sum())}
txt=['# Walk-Forward Report',f"Folds: {agg['folds']}",f"Median Final Equity: {agg['final_eq_med']:.4f}",f"Median MaxDD: {agg['max_dd_med']:.2%}",f"Total steps: {agg['n_steps_sum']}"]
(out/'wf_report.txt').write_text('`n'.join(txt), encoding='utf-8')
print("Saved walk-forward results.")
'@
# =========================================
# paper_demo/paper_loop_mt5_demo.py (poprawiony)
# =========================================
Write-UTF8 "paper_demo/paper_loop_mt5_demo.py" @'
-- coding: utf-8 --
"""Paper trading (DEMO) - pull M5 bars from MT5, build features incrementally, log decisions."""
import MetaTrader5 as mt5
import pandas as pd, numpy as np, yaml, time, logging, json
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from utils.mt5_health import ensure_mt5_ready
logging.basicConfig(filename='paper_demo/paper_trading.log', level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
SYMBOL = cfg['symbol']; WINDOW=int(cfg.get('window',128))
MODEL_PATH = cfg['files']['model_path']; VECNORM_PATH = cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl')
TF_MAP={'M1': mt5.TIMEFRAME_M1,'M5': mt5.TIMEFRAME_M5,'M15': mt5.TIMEFRAME_M15}; TF_NAME = cfg.get('timeframe','M5'); TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)
features_spec=None; fs=Path('models/features_spec.json')
if fs.exists(): features_spec=json.loads(fs.read_text(encoding='utf-8')).get('feature_columns', None)
def add_features_incremental(df: pd.DataFrame)->pd.DataFrame:
df=df.sort_values('time').reset_index(drop=True)
df['ret1']=np.log(df['close']).diff()
df['ema10']=df['close'].ewm(span=10).mean(); df['ema50']=df['close'].ewm(span=50).mean(); df['ema200']=df['close'].ewm(span=200).mean()
d=df['close'].diff(); up=d.clip(lower=0).ewm(alpha=1/14,adjust=False).mean(); down=(-d.clip(upper=0)).ewm(alpha=1/14,adjust=False).mean(); rs=up/(down+1e-12)
df['rsi14']=100-(100/(1+rs))
h,l,c=df['high'],df['low'],df['close']; tr=np.maximum(h-l, np.maximum(abs(h-c.shift()), abs(l-c.shift())))
df['atr14']=tr.ewm(alpha=1/14,adjust=False).mean()
ema_fast=df['close'].ewm(span=12,adjust=False).mean(); ema_slow=df['close'].ewm(span=26,adjust=False).mean(); macd_line=ema_fast-ema_slow; signal_line=macd_line.ewm(span=9,adjust=False).mean()
df['macd']=macd_line; df['macd_signal']=signal_line; df['macd_hist']=macd_line-signal_line
ma=df['close'].rolling(20).mean(); sd=df['close'].rolling(20).std()
df['bb_ma']=ma; df['bb_up']=ma+2sd; df['bb_lo']=ma-2sd; df['bb_width']=(df['bb_up']-df['bb_lo'])/(ma.replace(0,np.nan).abs()+1e-12)
df['minute']=df['time'].dt.hour60+df['time'].dt.minute; df['tod_sin']=np.sin(2np.pidf['minute']/1440); df['tod_cos']=np.cos(2np.pidf['minute']/1440); df.drop(columns=['minute'], inplace=True)
df['dow']=df['time'].dt.dayofweek; df['dow_sin']=np.sin(2np.pidf['dow']/7); df['dow_cos']=np.cos(2np.pi*df['dow']/7); df.drop(columns=['dow'], inplace=True)
df['close_log']=np.log(df['close'].clip(lower=1e-12)); mu=df['close_log'].rolling(2000,min_periods=200).mean(); s=df['close_log'].rolling(2000,min_periods=200).std().replace(0,np.nan)
df['close_norm']=(df['close_log']-mu)/(s+1e-8)
norm=['ret1','ema10','ema50','ema200','rsi14','atr14','macd','macd_signal','macd_hist','bb_ma','bb_up','bb_lo','bb_width','tod_sin','tod_cos','dow_sin','dow_cos']
for c in norm:
mu=df[c].rolling(2000,min_periods=200).mean(); s=df[c].rolling(2000,min_periods=200).std().replace(0,np.nan); df[c]=(df[c]-mu)/(s+1e-8)
return df.dropna().reset_index(drop=True)
def get_last_bars(symbol, timeframe, n:int):
rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, n)
if rates is None or len(rates)<n: return None
df=pd.DataFrame(rates); df['time']=pd.to_datetime(df['time'], unit='s', utc=True); df.rename(columns={'real_volume':'tick_volume'}, inplace=True)
return df[['time','open','high','low','close','tick_volume','spread']]
def build_obs(df_feat: pd.DataFrame, model_obs_dim: int)->np.ndarray:
per_step=(model_obs_dim-2)//WINDOW
price_col='close_norm' if 'close_norm' in df_feat.columns else 'close'
if features_spec is None:
base=['open','high','low','close','tick_volume','spread','time']; feat_cols=[c for c in df_feat.columns if c not in base]
else:
feat_cols=[c for c in features_spec if c in df_feat.columns]
tail=df_feat.tail(WINDOW); block=tail[[price_col]+feat_cols].to_numpy(dtype=np.float32)
if block.shape!=(WINDOW, per_step): raise RuntimeError(f"Bad block {block.shape} vs {(WINDOW, per_step)}")
flat=block.flatten(); import numpy as np
return np.concatenate([flat, np.array([0.0,0.0],dtype=np.float32)])
def main():
if not ensure_mt5_ready(): raise RuntimeError("MT5 not ready (terminal/account). Start MT5 and login to a demo account.")
try:
model = PPO.load(MODEL_PATH); expected_dim=int(model.observation_space.shape[0]); assert (expected_dim-2)%WINDOW==0
vecnorm=None
if Path(VECNORM_PATH).exists():
from gymnasium import spaces
class _ObsOnlyEnv:
def init(self, obs_dim):
self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
self.action_space = spaces.Discrete(3)
def reset(self, *, seed=None, options=None):
import numpy as np; return np.zeros(self.observation_space.shape, dtype=np.float32), {}
def step(self, action):
import numpy as np; return np.zeros(self.observation_space.shape, dtype=np.float32), 0.0, True, False, {}
dummy=DummyVecEnv([lambda: _ObsOnlyEnv(expected_dim)])
vecnorm=VecNormalize.load(VECNORM_PATH, dummy); vecnorm.training=False; vecnorm.norm_reward=False
Path('paper_demo').mkdir(parents=True, exist_ok=True)
csv=Path('paper_demo/decisions.csv'); if not csv.exists(): csv.write_text('time_utc,price,action_id,action_labeln', encoding='utf-8')         last_ts=None; hb_t=time.time()         while True:             bars=get_last_bars(SYMBOL, TF, n=WINDOW+800)             if bars is None or len(bars)<WINDOW+200: time.sleep(5); continue             feat=add_features_incremental(bars)             if len(feat)<WINDOW: time.sleep(5); continue             cur_ts=feat['time'].iloc[-1]             if last_ts is not None and cur_ts==last_ts: time.sleep(2); continue             obs=build_obs(feat, expected_dim)             if vecnorm is not None:                 import numpy as np; obs=vecnorm.normalize_obs(obs.reshape(1,-1)).reshape(-1)             action,_=model.predict(obs, deterministic=True)             decision={0:'SHORT',1:'FLAT',2:'LONG'}[int(action)]             price=float(feat['close'].iloc[-1]); msg=f"DECISION {decision} @ {price:.2f} (ts={cur_ts})"             print(msg); logging.info(msg)             with open(csv,'a',encoding='utf-8') as f: f.write(f"{cur_ts},{price:.5f},{int(action)},{decision}n")
last_ts=cur_ts
if time.time()-hb_t>600: logging.info(f"[HB] {SYMBOL} {TF_NAME} last_ts={cur_ts} price={price:.2f}"); hb_t=time.time()
time.sleep(30)
finally:
mt5.shutdown()
if name == 'main': main()
'@
# =========================================
# paper_demo/simulate_execution.py
# =========================================
Write-UTF8 "paper_demo/simulate_execution.py" @'
-- coding: utf-8 --
import yaml, pandas as pd
from env_xau import XauTradingEnv
from stable_baselines3 import PPO
from pathlib import Path
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']; MODEL_PATH = cfg['files']['model_path']; WINDOW = int(cfg.get('window',128)); COSTS = cfg['costs']
df = pd.read_csv(FEAT, parse_dates=['time']); cut = df['time'].max() - pd.Timedelta(days=120); sim = df[df['time']>=cut].copy()
env = XauTradingEnv(sim, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'])
obs, info = env.reset(); model = PPO.load(MODEL_PATH)
rows=[]; done=False
while not done:
action,_=model.predict(obs, deterministic=True)
obs, reward, terminated, truncated, info = env.step(action)
done = bool(terminated) or bool(truncated)
rows.append({'time': info.get('time',None), 'price': info.get('price',None), 'action': int(action), 'reward': float(reward), 'equity': float(env.equity)})
Path('reports').mkdir(parents=True, exist_ok=True)
pd.DataFrame(rows).to_csv('reports/trace_simulated.csv', index=False)
pd.Series([r['equity'] for r in rows]).to_csv('reports/equity_simulated.csv', index=False)
print('Saved simulated trace/equity.')
'@
# =========================================
# features/build_features.py (POPRAWIONY)
# =========================================
Write-UTF8 "features/build_features.py" @'
-- coding: utf-8 --
"""Feature builder: EMA/RSI/ATR + MACD + Bollinger + time cycles + rolling z-scores."""
import pandas as pd, numpy as np, yaml, json
from pathlib import Path
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
BARS = cfg['files']['bars_csv']; FEAT = cfg['files']['features_csv']
def macd(series, fast=12, slow=26, signal=9):
ema_fast = series.ewm(span=fast, adjust=False).mean()
ema_slow = series.ewm(span=slow, adjust=False).mean()
macd_line = ema_fast - ema_slow
signal_line = macd_line.ewm(span=signal, adjust=False).mean()
return macd_line, signal_line, macd_line - signal_line
def bbands(series, period=20, n_std=2.0):
ma = series.rolling(period).mean(); sd = series.rolling(period).std()
upper = ma + n_stdsd; lower = ma - n_stdsd
width = (upper - lower) / (ma.replace(0,np.nan).abs() + 1e-12)
return ma, upper, lower, width
def add_features(df):
df = df.sort_values('time').reset_index(drop=True)
c = df['close']
df['ret1'] = np.log(c).diff()
df['ema10'] = c.ewm(span=10).mean(); df['ema50']=c.ewm(span=50).mean(); df['ema200']=c.ewm(span=200).mean()
d = c.diff(); up=d.clip(lower=0).ewm(alpha=1/14,adjust=False).mean(); down=(-d.clip(upper=0)).ewm(alpha=1/14,adjust=False).mean(); rs=up/(down+1e-12)
df['rsi14'] = 100 - (100/(1+rs))
h,l,cl = df['high'], df['low'], df['close']
tr = np.maximum(h-l, np.maximum((h-cl.shift()).abs(), (l-cl.shift()).abs()))
df['atr14'] = tr.ewm(alpha=1/14, adjust=False).mean()
m,s,hst = macd(c); df['macd']=m; df['macd_signal']=s; df['macd_hist']=hst
bb_ma, bb_up, bb_lo, bb_w = bbands(c); df['bb_ma']=bb_ma; df['bb_up']=bb_up; df['bb_lo']=bb_lo; df['bb_width']=bb_w
df['minute']=df['time'].dt.hour60+df['time'].dt.minute
df['tod_sin']=np.sin(2np.pidf['minute']/1440); df['tod_cos']=np.cos(2np.pidf['minute']/1440); df.drop(columns=['minute'], inplace=True)
df['dow']=df['time'].dt.dayofweek
df['dow_sin']=np.sin(2np.pidf['dow']/7); df['dow_cos']=np.cos(2np.pi*df['dow']/7); df.drop(columns=['dow'], inplace=True)
df['close_log']=np.log(c.clip(lower=1e-12))
mu = df['close_log'].rolling(2000, min_periods=200).mean(); sd = df['close_log'].rolling(2000, min_periods=200).std().replace(0,np.nan)
df['close_norm']=(df['close_log']-mu)/(sd+1e-8)
feat_cols=['ret1','ema10','ema50','ema200','rsi14','atr14','macd','macd_signal','macd_hist','bb_ma','bb_up','bb_lo','bb_width','tod_sin','tod_cos','dow_sin','dow_cos']
for col in feat_cols:
mu = df[col].rolling(2000, min_periods=200).mean(); sd = df[col].rolling(2000, min_periods=200).std().replace(0,np.nan)
df[col]=(df[col]-mu)/(sd+1e-8)
return df.dropna().reset_index(drop=True)
df = pd.read_csv(BARS, parse_dates=['time'])
df = add_features(df)
assert len(df) > int(cfg.get('window',128)) + 10, "Too few rows for features"
Path(FEAT).parent.mkdir(parents=True, exist_ok=True)
df.to_csv(FEAT, index=False)
base_cols = ['time','open','high','low','close','tick_volume','spread']
feature_columns = [c for c in df.columns if c not in base_cols]
spec = {"feature_columns": feature_columns, "price_column":"close_norm"}
Path('models').mkdir(parents=True, exist_ok=True)
Path('models/features_spec.json').write_text(json.dumps(spec, ensure_ascii=False, indent=2), encoding='utf-8')
print("Saved features and features_spec.json")
'@
# =========================================
# fetch__mt5_data.py (inkrementalny, 720 dni, atomowy zapis, weekend-safe)
# =========================================
Write-UTF8 "fetch__mt5_data.py" @'
-- coding: utf-8 --
"""
Fetch bars & 24h tick sample from MT5 (demo/real). Incremental merge to maintain 720d.
# Atomic CSV writes, Retries/backoff, Weekend-safe ticks (warn only)
"""
import MetaTrader5 as mt5
import pandas as pd
from datetime import datetime, timedelta, timezone
import yaml
from pathlib import Path
import numpy as np, time
from utils.atomic import atomic_write_csv
from utils.mt5_health import ensure_mt5_ready
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
SYMBOL = cfg['symbol']
BARS_CSV = cfg['files']['bars_csv']
TICKS_CSV = cfg['files']['ticks_csv']
HISTORY_DAYS = int(cfg.get('history_days', 720))
TF_MAP={'M1': mt5.TIMEFRAME_M1,'M5': mt5.TIMEFRAME_M5,'M15': mt5.TIMEFRAME_M15}
TF_NAME = cfg.get('timeframe','M5'); TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)
CHUNK = int(cfg.get('mt5',{}).get('bars_chunk', 20000))
UPDATE_DAYS = int(cfg.get('mt5',{}).get('update_days', 60))
def init_mt5():
if not ensure_mt5_ready(): raise RuntimeError("MT5 not ready (terminal/account). Please login in MT5 terminal.")
term = mt5.terminal_info(); acc = mt5.account_info()
parts=[]
if getattr(term,"company",None): parts.append(f"Company={term.company}")
if getattr(term,"name",None): parts.append(f"TerminalName={term.name}")
if getattr(acc,"login",0): parts.append(f"Login={acc.login}")
if getattr(acc,"server",None): parts.append(f"Server={acc.server}")
if getattr(acc,"name",None): parts.append(f"AccountName={acc.name}")
print("[MT5] " + " | ".join(parts) if parts else "[MT5] initialized")
def ensure_symbol(symbol: str)->bool:
info = mt5.symbol_info(symbol)
if info is None:
mt5.symbol_select(symbol, True)
info = mt5.symbol_info(symbol)
if info is None: return False
if not info.visible:
if not mt5.symbol_select(symbol, True): return False
return True
def suggest_similar(symbol: str, limit=10):
cands=[]
for s in mt5.symbols_get():
name=s.name.upper()
if ("GOLD" in name) or ("XAU" in name): cands.append(s.name)
if not cands:
cands = [s.name for s in mt5.symbols_get()[:limit]]
return sorted(set(cands))[:limit]
def with_retries(fn, attempts=3, sleep_s=2, *a, **kw):
last=None
for i in range(attempts):
try:
return fn(*a, *kw)
except Exception as e:
last = e; time.sleep(sleep_s(i+1))
if last: raise last
def _to_df(rates):
df = pd.DataFrame(rates)
if df.empty: return df
df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
if 'real_volume' in df.columns:
df.rename(columns={'real_volume':'tick_volume'}, inplace=True)
cols=['time','open','high','low','close','tick_volume','spread']
return df[cols].sort_values('time').drop_duplicates('time')
def fetch_range(symbol, timeframe, utc_from, utc_to):
rates = mt5.copy_rates_range(symbol, timeframe, utc_from, utc_to)
if rates is None or len(rates)==0:
last_err = mt5.last_error()
raise RuntimeError(f"copy_rates_range empty for '{symbol}'. last_error={last_err}")
return _to_df(rates)
def fetch_from_pos_tail(symbol, timeframe, count):
rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
if rates is None or len(rates)==0:
last_err = mt5.last_error()
raise RuntimeError(f"copy_rates_from_pos empty for '{symbol}'. last_error={last_err}")
return _to_df(rates)
def load_existing_bars(path: str):
p = Path(path)
if not p.exists(): return None
try:
df = pd.read_csv(p, parse_dates=['time'])
if df.empty: return None
return df.sort_values('time').drop_duplicates('time').reset_index(drop=True)
except Exception:
return None
def merge_clip(existing: pd.DataFrame | None, new_df: pd.DataFrame, keep_days:int)->pd.DataFrame:
if existing is None or existing.empty:
base = new_df.copy()
else:
base = (pd.concat([existing, new_df], ignore_index=True)
.drop_duplicates('time').sort_values('time'))
cutoff = datetime.now(timezone.utc) - timedelta(days=keep_days+1)
base = base[base['time'] >= pd.Timestamp(cutoff)]
return base.reset_index(drop=True)
def fetch_bars_incremental(symbol, timeframe, keep_days:int, update_days:int)->pd.DataFrame:
existing = load_existing_bars(BARS_CSV)
if existing is None:
to = datetime.now(timezone.utc); frm = to - timedelta(days=keep_days+2)
df = with_retries(lambda: fetch_range(symbol, timeframe, frm, to))
if len(df) < 1000:
tail = with_retries(lambda: fetch_from_pos_tail(symbol, timeframe, CHUNK))
df = merge_clip(df, tail, keep_days)
return df
to = datetime.now(timezone.utc); frm = to - timedelta(days=max(2, update_days))
fresh = with_retries(lambda: fetch_range(symbol, timeframe, frm, to))
merged = merge_clip(existing, fresh, keep_days)
return merged
def fetch_ticks(symbol: str, hours: int=24)->pd.DataFrame:
utc_to = datetime.now(timezone.utc); utc_from = utc_to - timedelta(hours=hours)
ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)
if ticks is None or len(ticks)==0:
last_err = mt5.last_error()
raise RuntimeError(f"No MT5 ticks for '{symbol}'. last_error={last_err}")
tdf = pd.DataFrame(ticks); tdf['time']=pd.to_datetime(tdf['time'], unit='s', utc=True)
return tdf[['time','bid','ask','last','volume']]
def suggest_costs(tdf: pd.DataFrame)->dict:
spr = (tdf['ask'] - tdf['bid']).astype(float).replace([np.inf,-np.inf], np.nan).dropna()
med_spread = float(np.median(spr)) if len(spr) else 0.0
p75_spread = float(np.percentile(spr, 75)) if len(spr) else 0.0
mid = (tdf['ask'] + tdf['bid'])/2.0
dm = (mid.diff().abs()).replace([np.inf,-np.inf], np.nan).dropna()
med_dm = float(np.median(dm)) if len(dm) else 0.0
med_price = float(np.nanmedian(mid)) if len(mid) else 1.0
slippage_k = float(min(max(med_dm / max(med_price, 1e-12), 0.0), 0.01))
return {'spread_abs_median': round(med_spread,5), 'spread_abs_p75': round(p75_spread,5), 'slippage_k_suggested': round(slippage_k,4)}
def main():
init_mt5()
try:
print(f"[CFG] symbol={SYMBOL} tf={TF_NAME} keep_days={HISTORY_DAYS} update_days={UPDATE_DAYS}")
if not ensure_symbol(SYMBOL):
similar = suggest_similar(SYMBOL)
raise SystemExit(
"Symbol not found or not visible in Market Watch: '{}'n"                 "Try one of these (server-specific): {}n"
"Also open Symbols window and SHOW the instrument."
.format(SYMBOL, ", ".join(similar))
)
bars = fetch_bars_incremental(SYMBOL, TF, keep_days=HISTORY_DAYS, update_days=UPDATE_DAYS)
assert {'time','open','high','low','close'}.issubset(bars.columns), "Bars frame malformed"
assert len(bars) > 100, "Too few bars"
atomic_write_csv(bars, BARS_CSV)
print(f"Saved bars: {BARS_CSV} ({len(bars)})")
# ticks są opcjonalne (weekend-safe)
try:
ticks = with_retries(lambda: fetch_ticks(SYMBOL, hours=24))
atomic_write_csv(ticks, TICKS_CSV)
print(f"Saved ticks: {TICKS_CSV} ({len(ticks)})")
sugg = suggest_costs(ticks)
Path('reports').mkdir(parents=True, exist_ok=True)
Path('reports/costs_suggestion.yaml').write_text(
yaml.safe_dump(sugg, allow_unicode=True, sort_keys=False), encoding='utf-8')
print("Cost suggestions -> reports/costs_suggestion.yaml")
except Exception as e:
print(f"[warn] ticks unavailable ({e}); keeping previous {TICKS_CSV}")
finally:
mt5.shutdown()
if name == 'main':
main()
'@
# =========================================
# utils/calibration.py
# =========================================
Write-UTF8 "utils/calibration.py" @'
-- coding: utf-8 --
"""Calibrate costs: spread (median/95p) and slippage_k ratio. Save suggestions; with --apply update config.yaml."""
import argparse, yaml, pandas as pd, numpy as np
from pathlib import Path
parser = argparse.ArgumentParser(); parser.add_argument('--apply', action='store_true'); args=parser.parse_args()
cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
TICKS = cfg['files']['ticks_csv']; BARS = cfg['files']['bars_csv']
ticks = pd.read_csv(TICKS, parse_dates=['time'])
ticks['spread']=ticks['ask']-ticks['bid']; mid=(ticks['ask']+ticks['bid'])/2.0; dmid=mid.diff().abs()
spread_median=float(ticks['spread'].median()); spread_p95=float(ticks['spread'].quantile(0.95)); dmid_median=float(dmid.median())
bars=pd.read_csv(BARS, parse_dates=['time']); bar_move_med=float(bars['close'].diff().abs().median())
slippage_k=float(np.clip(dmid_median/max(bar_move_med,1e-12), 0.0, 0.5))
Path('reports').mkdir(parents=True, exist_ok=True)
Path('reports/costs_suggestion.yaml').write_text(
yaml.safe_dump({'spread_abs_median':round(spread_median,5),'spread_abs_p95':round(spread_p95,5),'slippage_k_suggested':round(slippage_k,3)},
allow_unicode=True, sort_keys=False),
encoding='utf-8')
print('Suggestions -> reports/costs_suggestion.yaml')
if args.apply:
cfg['costs']['spread_abs']=float(round(spread_median,5)); cfg['costs']['slippage_k']=float(slippage_k)
Path('config.yaml').write_text(yaml.safe_dump(cfg, allow_unicode=True, sort_keys=False), encoding='utf-8')
print('Applied to config.yaml')
'@
# =========================================
# utils/make_report.py (POPRAWIONY)
# =========================================
Write-UTF8 "utils/make_report.py" @'
-- coding: utf-8 --
import argparse, base64
from pathlib import Path
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
parser=argparse.ArgumentParser()
parser.add_argument('--equity_csv', type=str, default='reports/eval_trace.csv')
parser.add_argument('--metrics', type=str, default='reports/val_metrics.txt')
parser.add_argument('--figure', type=str, default='reports/equity_val.png')
parser.add_argument('--out_html', type=str, default='reports/report.html')
parser.add_argument('--out_txt', type=str, default='reports/report.txt')
parser.add_argument('--bars_per_day', type=int, default=288)
parser.add_argument('--trading_days', type=int, default=252)
args=parser.parse_args()
out_html=Path(args.out_html); out_txt=Path(args.out_txt)
out_html.parent.mkdir(parents=True, exist_ok=True); out_txt.parent.mkdir(parents=True, exist_ok=True)
eq=None; p=Path(args.equity_csv)
if p.exists():
df=pd.read_csv(p)
if 'equity' in df.columns: eq=df['equity'].astype(float).to_numpy()
elif df.shape[1]==1: eq=df.iloc[:,0].astype(float).to_numpy()
metrics_text = Path(args.metrics).read_text(encoding='utf-8') if Path(args.metrics).exists() else ''
img_b64=''
if Path(args.figure).exists(): img_b64=base64.b64encode(Path(args.figure).read_bytes()).decode('ascii')
if not img_b64 and eq is not None and len(eq)>1:
fig,ax=plt.subplots(figsize=(9,4)); ax.plot(eq,color='#0057B8',lw=1.5)
ax.set_title('Equity (validation)'); ax.set_xlabel('Steps'); ax.set_ylabel('Equity')
fig.tight_layout(); Path(args.figure).parent.mkdir(parents=True, exist_ok=True); fig.savefig(args.figure,dpi=144); plt.close(fig)
img_b64=base64.b64encode(Path(args.figure).read_bytes()).decode('ascii')
extra={}
if eq is not None and len(eq)>2:
ret=np.diff(eq)/np.maximum(eq[:-1],1e-12); mu=float(np.nanmean(ret)); sigma=float(np.nanstd(ret)+1e-12)
downside=ret[ret<0]; ds=float(np.nanstd(downside)+1e-12)
daily_mu = mu * args.bars_per_day; daily_sigma = sigma * (args.bars_per_day0.5)
sharpe = daily_mu/(daily_sigma+1e-12); sortino = daily_mu/(((args.bars_per_day0.5)*ds)+1e-12)
n=len(eq); years=max(n/args.bars_per_day/args.trading_days,1e-6)
cagr=(eq[-1]/max(eq[0],1e-12))**(1/max(years,1e-6))-1 if years>0 else 0.0
peak=np.maximum.accumulate(eq); max_dd=float((eq/peak-1.0).min()); calmar=(cagr/abs(max_dd)) if max_dd<0 else float('inf')
extra=dict(n_steps=n, sharpe_daily=sharpe, sortino_daily=sortino, vol_daily=daily_sigma, cagr=cagr, max_dd=max_dd, calmar=calmar)
now=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
md=['# RL Validation Report (XAUUSD M5)', f'Date: {now}', '', '## Base metrics', '', metrics_text.strip() if metrics_text else '(no val_metrics.txt)', '', '']
if extra:
md += [
'## Extra metrics from equity',
f"- Steps: {extra['n_steps']}",
f"- Sharpe (daily): {extra['sharpe_daily']:.3f}",
f"- Sortino (daily): {extra['sortino_daily']:.3f}",
f"- Daily vol: {extra['vol_daily']:.4f}",
f"- CAGR (est.): {extra['cagr']:.3%}",
f"- MaxDD: {extra['max_dd']:.2%}",
f"- Calmar: {extra['calmar']:.3f}",
]
else:
md += ['No equity data.']
out_txt.write_text('\n'.join(md)+'\n', encoding='utf-8')
html=f"""RL Validation
RL Validation Report (XAUUSD M5)
Base metrics
{metrics_text if metrics_text else '(no val_metrics.txt)'}
Extra metrics
{('\n'.join(md)) if extra else 'No equity data.'}
Equity
{('data:image/png;base64,'+img_b64) if img_b64 else 'No plot.'}
Generated: {now}
"""
out_html.write_text(html, encoding='utf-8')
print('Saved report.')
'@
# =========================================
# utils/qc_bars.py & utils/time_utils.py
# =========================================
Write-UTF8 "utils/qc_bars.py" @'
#!/usr/bin/env python
import pandas as pd, argparse
parser=argparse.ArgumentParser(); parser.add_argument('--csv', required=True); parser.add_argument('--tf_min', type=int, default=5)
args=parser.parse_args()
df=pd.read_csv(args.csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
print(f"Rows: {len(df)}\nFrom: {df['time'].iloc[0]} To: {df['time'].iloc[-1]}")
dt=(df['time'].diff().dt.total_seconds()/60).fillna(args.tf_min)
gaps=df.loc[dt>args.tf_min+0.1,['time']].copy(); gaps['gap_min']=dt[dt>args.tf_min+0.1].values
print(f"Gaps > {args.tf_min} min: {len(gaps)}");
if len(gaps): print(gaps.head(10))
'@
Write-UTF8 "utils/time_utils.py" @'
from datetime import datetime, timezone
def now_utc(): return datetime.now(timezone.utc)
'@
# =========================================
# ops: env.ps1 & run scripts & promotion
# =========================================
Write-UTF8 "ops/env.ps1" @'
$ErrorActionPreference = "Stop"
$RepoRoot = Split-Path -Parent $MyInvocation.MyCommand.Path | Split-Path
$Py = Join-Path $RepoRoot ".venv\Scripts\python.exe"
$Cfg = Join-Path $RepoRoot "config.yaml"
$LogDir = Join-Path $RepoRoot "logs"
New-Item -ItemType Directory -Force -Path $LogDir | Out-Null
function RunPy([string]$script, [string]$args="") {
& $Py $script $args 2>&1 | Tee-Object -FilePath (Join-Path $LogDir ((Split-Path $script -Leaf) + ".log")) -Append
}
'@
Write-UTF8 "ops/run_collector.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) { try { RunPy "$RepoRoot\fetch__mt5_data.py" } catch { Write-Host "Collector error: $_" } Start-Sleep -Seconds 300 }
'@
Write-UTF8 "ops/run_features.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) { try { RunPy "$RepoRoot\features\build_features.py" } catch { Write-Host "Features error: $_" } Start-Sleep -Seconds 600 }
'@
Write-UTF8 "ops/run_paper.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) { try { RunPy "$RepoRoot\paper_demo\paper_loop_mt5_demo.py" } catch { Write-Host "Paper loop crash: $_"; Start-Sleep -Seconds 10 } Start-Sleep -Seconds 2 }
'@
Write-UTF8 "ops/run_train_short.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) {
try {
RunPy "$RepoRoot\rl\train_ppo.py" "--timesteps 500000 --seed 42 --eval_freq 100000"
RunPy "$RepoRoot\rl\evaluate.py" "--max_eval_steps 3000 --out_dir reports_live"
RunPy "$RepoRoot\ops\save_candidate.py"
& $Py "$RepoRoot\ops\promote_challenger.py"
} catch { Write-Host "Short training error: $_" }
Start-Sleep -Seconds 14400
}
'@
Write-UTF8 "ops/run_train_nightly.ps1" @'
. "$PSScriptRoot\env.ps1"
while ($true) {
try {
while ( (Get-Date).ToUniversalTime().Hour -ne 23 ) { Start-Sleep -Seconds 600 }
Start-Sleep -Seconds 300
RunPy "$RepoRoot\rl\train_ppo.py" "--timesteps 2000000 --seed 42 --eval_freq 200000"
RunPy "$RepoRoot\rl\walk_forward.py" "--segments 6 --train_days 120 --val_days 30 --timesteps 500000 --out_dir reports_wf"
RunPy "$RepoRoot\rl\evaluate.py" "--max_eval_steps 3000 --out_dir reports_nightly"
RunPy "$RepoRoot\ops\save_candidate.py"
& $Py "$RepoRoot\ops\promote_challenger.py"
} catch { Write-Host "Nightly training error: $_" }
Start-Sleep -Seconds 3600
}
'@
Write-UTF8 "ops/save_candidate.py" @'
import json, shutil
from pathlib import Path
ROOT = Path(file).resolve().parents[1]
REG = ROOT / 'models' / 'registry'; REG.mkdir(parents=True, exist_ok=True)
MODEL = ROOT / 'models' / 'ppo_xauusd_m5.zip'
VEC   = ROOT / 'models' / 'vecnorm_xauusd_m5.pkl'
METR  = ROOT / 'reports' / 'val_metrics.txt'
EQP   = ROOT / 'reports' / 'equity_val.png'
def parse_metrics_txt(p: Path):
d = {}
if p.exists():
txt = p.read_text(encoding='utf-8')
for line in txt.splitlines():
if ':' in line:
k, v = line.split(':', 1)
k = k.strip().lower().replace(' ', '')
try: d[k] = float(v.strip())
except: pass
return d
def main():
m = parse_metrics_txt(METR)
if not m:
print('[save_candidate] No metrics.'); return
tag = 'cand' + Path.cwd().name + '_'
n=0
while (REG / f"{tag}{n:02d}").exists(): n+=1
dst = REG / f"{tag}{n:02d}"
dst.mkdir(parents=True, exist_ok=True)
shutil.copy2(MODEL, dst/'model.zip')
if VEC.exists(): shutil.copy2(VEC, dst/'vecnorm.pkl')
if EQP.exists(): shutil.copy2(EQP, dst/'equity.png')
(dst/'metrics.json').write_text(json.dumps({
'final_equity': m.get('final_equity'),
'max_dd': m.get('max_drawdown'),
'sharpe': m.get('sharpe_daily'),
}, indent=2), encoding='utf-8')
print(f'[save_candidate] Saved candidate -> {dst}')
if name == 'main': main()
'@
Write-UTF8 "ops/promote_challenger.py" @'
import json, shutil
from pathlib import Path
ROOT = Path(file).resolve().parents[1]
REG = ROOT / 'models' / 'registry'; REG.mkdir(parents=True, exist_ok=True)
CUR_TXT = REG / 'current.txt'
MODEL_DST = ROOT / 'models' / 'ppo_xauusd_m5.zip'
VEC_DST   = ROOT / 'models' / 'vecnorm_xauusd_m5.pkl'
def load_metrics(d: Path):
if not d: return None
m = d / 'metrics.json'
return json.loads(m.read_text('utf-8')) if m.exists() else None
def pick_latest_dir():
dirs = [p for p in REG.iterdir() if p.is_dir()]
return sorted(dirs, key=lambda p: p.name)[-1] if dirs else None
def better(m_new, m_old):
if m_old is None: return True
s_new, s_old = m_new.get('sharpe', 0.0) or 0.0, m_old.get('sharpe', 0.0) or 0.0
dd_new, dd_old = m_new.get('max_dd', -1.0) or -1.0, m_old.get('max_dd', -1.0) or -1.0
dd_ok = (dd_new >= dd_old * 1.2)
s_ok  = (s_new >= s_old * 1.10) or (s_new >= 0.10 and s_new > s_old)
fe_new, fe_old = m_new.get('final_equity', 0.0) or 0.0, m_old.get('final_equity', 0.0) or 0.0
if (s_new == 0.0 and s_old == 0.0):
return (fe_new > fe_old) and dd_ok
return s_ok and dd_ok
def promote(new_dir: Path):
REG.mkdir(parents=True, exist_ok=True)
(REG/'current.txt').write_text(new_dir.name, encoding='utf-8')
shutil.copy2(new_dir/'model.zip', MODEL_DST)
if (new_dir/'vecnorm.pkl').exists(): shutil.copy2(new_dir/'vecnorm.pkl', VEC_DST)
print(f"[promote] Champion -> {new_dir.name}")
def main():
latest = pick_latest_dir()
if latest is None: print('[promote] No candidates.'); return
new_m = load_metrics(latest)
if new_m is None: print('[promote] Latest has no metrics.json'); return
cur_dir = REG / CUR_TXT.read_text('utf-8').strip() if CUR_TXT.exists() else None
old_m = load_metrics(cur_dir) if cur_dir and cur_dir.exists() else None
if better(new_m, old_m): promote(latest)
else: print('[promote] Challenger not better. No promotion.')
if name == 'main': main()
'@
Write-UTF8 "ops/install_services.ps1" @'
$nssm = "C:\nssm\nssm.exe"
$root = Split-Path -Parent $MyInvocation.MyCommand.Path
$ps = "powershell.exe"
& $nssm install XAU_Collector   $ps "-ExecutionPolicy Bypass -File "$root\run_collector.ps1""
& $nssm set    XAU_Collector   AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_Collector   Start SERVICE_AUTO_START
& $nssm install XAU_Features    $ps "-ExecutionPolicy Bypass -File "$root\run_features.ps1""
& $nssm set    XAU_Features    AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_Features    Start SERVICE_AUTO_START
& $nssm install XAU_Paper       $ps "-ExecutionPolicy Bypass -File "$root\run_paper.ps1""
& $nssm set    XAU_Paper       AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_Paper       Start SERVICE_AUTO_START
& $nssm install XAU_TrainShort  $ps "-ExecutionPolicy Bypass -File "$root\run_train_short.ps1""
& $nssm set    XAU_TrainShort  AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_TrainShort  Start SERVICE_AUTO_START
& $nssm install XAU_TrainNightly $ps "-ExecutionPolicy Bypass -File "$root\run_train_nightly.ps1""
& $nssm set    XAU_TrainNightly AppDirectory (Split-Path -Parent $root)
& $nssm set    XAU_TrainNightly Start SERVICE_AUTO_START
'@
# =========================================
# quick_env_test + features_spec placeholder
# =========================================
Write-UTF8 "quick_env_test.py" @'
import yaml, pandas as pd
from env_xau import XauTradingEnv
cfg=yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
df=pd.read_csv(cfg['files']['features_csv'], parse_dates=['time']).sort_values('time').reset_index(drop=True)
df=df.tail(max(int(cfg.get('window',128))+256,512))
env=XauTradingEnv(df, window=int(cfg.get('window',128)),
spread_abs=cfg['costs']['spread_abs'], commission_rate=cfg['costs']['commission_rate'], slippage_k=cfg['costs']['slippage_k'],
reward_mode=cfg.get('env',{}).get('reward_mode', cfg.get('reward_mode','pct')), use_close_norm=True,
min_equity=float(cfg.get('env',{}).get('min_equity',0.8)))
obs,info=env.reset(); print('reset OK', len(obs))
obs,r,term,trunc,info=env.step(1); print('step OK', r, term, trunc)
'@
Write-UTF8 "models/features_spec.json" '{"feature_columns": [], "price_column": "close_norm"}'
Write-Host "OK – repo files created in $Root"









=== FILE: check.py ===

import pandas as pd
df = pd.read_csv("data/XAUUSD_M5.csv", parse_dates=["time"])
print("bars rows:", len(df), "from:", df["time"].min(), "to:", df["time"].max())
print(df.tail(3))

=== FILE: config.yaml ===

symbol: GOLD.pro
timeframe: M5
history_days: 720
window: 128
mt5:
  bars_chunk: 100000
  update_days: 60
files:
  bars_csv: data/XAUUSD_M5.csv
  ticks_csv: data/XAUUSD_ticks_sample.csv
  features_csv: data/XAUUSD_M5_features.csv
  model_path: models/ppo_xauusd_m5.zip
  vecnorm_path: models/vecnorm_xauusd_m5.pkl
costs:
  spread_abs: 0.42
  commission_rate: 0.0001
  slippage_k: 0.0
env:
  reward_mode: pct
  flip_penalty: 0.002
  trade_hours_utc:
  - 06:00
  - '20:00'
  enforce_flat_outside_hours: true
  min_equity: 0.8
fundamentals:
  usd_series_id: DTWEXBGS
  gpr_csv: external/gpr/gpr.csv
  cot_csv: external/cot_gold.csv
calendar:
  calendar_csv: external/calendar_us.csv
  events_whitelist:
  - Non Farm Payrolls
  - Unemployment Rate
  - CPI
  - Core CPI
  - Core PCE
  - ISM Manufacturing PMI
  - ISM Services PMI
  - FOMC Interest Rate Decision
  - Fed Press Conference


=== FILE: env_xau.py ===

# -*- coding: utf-8 -*-
import numpy as np, pandas as pd
import gymnasium as gym
from gymnasium import spaces
from datetime import time as dtime

class XauTradingEnv(gym.Env):
    metadata = {"render_modes": []}

    def __init__(self, df: pd.DataFrame, window=128,
                 spread_abs=0.05, commission_rate=0.0001, slippage_k=0.10,
                 reward_mode: str = "pct", use_close_norm: bool = True,
                 flip_penalty: float = 0.0, trade_hours_utc=None,
                 enforce_flat_outside_hours: bool = True, features_spec: list | None = None,
                 min_equity: float = 0.8):
        super().__init__()
        self.df = df.sort_values('time').reset_index(drop=True)
        self.window = int(window)
        self.spread_abs = float(spread_abs)
        self.commission_rate = float(commission_rate)
        self.slippage_k = float(slippage_k)
        assert reward_mode in {"pct","points"}
        self.reward_mode = reward_mode
        self.use_close_norm = use_close_norm
        self.flip_penalty = float(flip_penalty)
        self.enforce_flat_outside = bool(enforce_flat_outside_hours)
        self.min_equity = float(min_equity)

        # Okno godzin handlu (UTC)
        self.trade_hours = None
        if trade_hours_utc and isinstance(trade_hours_utc, (list, tuple)) and len(trade_hours_utc) == 2:
            try:
                s = [int(x) for x in str(trade_hours_utc[0]).split(":")]
                e = [int(x) for x in str(trade_hours_utc[1]).split(":")]
                self.trade_hours = (dtime(s[0], s[1] if len(s) > 1 else 0),
                                    dtime(e[0], e[1] if len(e) > 1 else 0))
            except Exception:
                self.trade_hours = None

        # Kolumny cech
        base_cols = ['open', 'high', 'low', 'close', 'tick_volume', 'spread', 'time']
        if features_spec is not None:
            missing = [c for c in features_spec if c not in self.df.columns]
            if missing:
                raise ValueError(f"Missing feature columns: {missing}")
            self.feat_cols = list(features_spec)
        else:
            self.feat_cols = [c for c in self.df.columns if c not in base_cols]

        self.price_col = 'close_norm' if (self.use_close_norm and 'close_norm' in self.df.columns) else 'close'
        if len(self.df) <= self.window:
            raise ValueError(f"Not enough rows: {len(self.df)} <= window {self.window}")

        # Observation: [window x (1 + n_features)] + [pos, unrealized]
        obs_dim = self.window * (1 + len(self.feat_cols)) + 2
        self.action_space = spaces.Discrete(3)  # 0=SHORT, 1=FLAT, 2=LONG
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)

        # Stan początkowy
        self._start = self.window
        self._i = None
        self.pos = 0
        self.entry = None
        self.equity = 1.0
        self.prev_eq = 1.0

    def _in_trade_hours(self, ts) -> bool:
        if self.trade_hours is None:
            return True
        try:
            t = ts.to_pydatetime().time()
        except Exception:
            t = ts
        start, end = self.trade_hours
        if start <= end:
            return (t >= start) and (t <= end)
        # okno nocne np. 22:00–06:00
        return (t >= start) or (t <= end)

    def _obs(self):
        sl = slice(self._i - self.window, self._i)
        block_df = self.df.iloc[sl][[self.price_col] + self.feat_cols]
        block = block_df.to_numpy(dtype=np.float32)
        expected = 1 + len(self.feat_cols)
        if block.shape != (self.window, expected):
            raise RuntimeError(f"Bad window shape: {block.shape} vs {(self.window, expected)}")
        flat = block.flatten()

        price = float(self.df.iloc[self._i]['close'])
        unreal = 0.0
        if self.pos != 0 and self.entry is not None:
            dir_ = 1 if self.pos > 0 else -1
            unreal = dir_ * (price - self.entry)

        return np.concatenate([flat, np.array([self.pos, unreal], dtype=np.float32)])

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self._i = self._start
        self.pos = 0
        self.entry = None
        self.equity = 1.0
        self.prev_eq = 1.0
        return self._obs(), {}

    def step(self, action):
        if isinstance(action, (np.ndarray, list, tuple)):
            action = int(action[0])
        else:
            action = int(action)
        if not self.action_space.contains(action):
            raise ValueError("Invalid action")

        info = {}
        ts = self.df.iloc[self._i]['time']
        inside = self._in_trade_hours(ts)
        price = float(self.df.iloc[self._i]['close'])
        prev = float(self.df.iloc[self._i - 1]['close'])
        slip = self.slippage_k * abs(price - prev)

        desired = [-1, 0, 1][action]
        if not inside and self.enforce_flat_outside:
            desired = 0

        # Zmiana pozycji (koszty + ewentualny flip penalty)
        if desired != self.pos:
            # flip penalty gdy odwracasz kierunek bez przejścia na 0
            if self.pos != 0 and desired != 0 and np.sign(self.pos) != np.sign(desired) and self.flip_penalty > 0:
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - self.flip_penalty, 1e-6)
                else:
                    self.equity -= self.flip_penalty
                info['flip_penalty'] = float(self.flip_penalty)

            # koszt zamknięcia starej pozycji
            if self.pos != 0 and self.entry is not None:
                cost = (self.spread_abs + slip) / max(price, 1e-12) + self.commission_rate
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - cost, 1e-6)
                else:
                    self.equity -= cost

            # otwórz nową (lub wyzeruj)
            self.pos = desired
            if self.pos != 0:
                cost = (self.spread_abs + slip) / max(price, 1e-12) + self.commission_rate
                if self.reward_mode == 'pct':
                    self.equity *= max(1.0 - cost, 1e-6)
                else:
                    self.equity -= cost
                self.entry = price
            else:
                self.entry = None

        # Zmiana equity w kroku
        if self.reward_mode == 'pct':
            step_ret = 0.0
            if self.pos != 0:
                dir_ = 1 if self.pos > 0 else -1
                step_ret = dir_ * ((price / max(prev, 1e-12)) - 1.0)
            self.equity *= (1.0 + step_ret)
            reward = float(self.equity - self.prev_eq)
        else:
            reward = float(self.equity - self.prev_eq)

        self.prev_eq = self.equity
        self._i += 1

        terminated = bool(self._i >= len(self.df) - 1)
        truncated = False
        if self.equity <= self.min_equity:
            truncated = True
            info['early_stop'] = True

        info.update({
            'time': ts,
            'price': price,
            'pos': int(self.pos),
            'equity': float(self.equity),
            'inside_hours': bool(inside)
        })
        return self._obs(), reward, terminated, truncated, info

=== FILE: features_check.py ===

import pandas as pd

df = pd.read_csv("data/XAUUSD_M5_features.csv", parse_dates=["time"])
print("features rows:", len(df), "from:", df["time"].min(), "to:", df["time"].max())
print(df.tail(3))

=== FILE: fetch__mt5_data.py ===

# -*- coding: utf-8 -*-
"""
fetch__mt5_data.py
------------------
Pobiera świece M5 (inkrementalnie do 720 dni) oraz próbkę ticków z MT5.

• Inkrementalny merge do jednego CSV z 720 dni (przycinanie, deduplikacja po 'time').
• Obejścia brokerów: copy_rates_range (naive datetimes) z fallbackiem do copy_rates_from_pos (tail),
  retry/backoff, chunkowanie przy pierwszym pobraniu.
• Atomiczny zapis CSV.
• Ticki weekend-safe (ostrzeżenie, nie przerywa).
• Sugestie kosztów (median/p75 spread + heurystyka slippage_k).
• Czas wszędzie UTC-aware.

Użycie:
    python fetch__mt5_data.py
"""

from __future__ import annotations

import MetaTrader5 as mt5
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
from pathlib import Path
import time
import yaml

# nasze utilsy
from utils.atomic import atomic_write_csv
from utils.mt5_health import ensure_mt5_ready

# ============================================================
# Konfiguracja
# ============================================================
CFG_PATH = 'config.yaml'
cfg = yaml.safe_load(open(CFG_PATH, 'r', encoding='utf-8'))

if not isinstance(cfg, dict) or 'files' not in cfg:
    raise RuntimeError(f"{CFG_PATH} jest pusty lub ma złą strukturę (brak sekcji 'files').")

SYMBOL = cfg.get('symbol', 'XAUUSD')
BARS_CSV = cfg['files']['bars_csv']
TICKS_CSV = cfg['files']['ticks_csv']
HISTORY_DAYS = int(cfg.get('history_days', 720))

TF_MAP = {
    'M1':  mt5.TIMEFRAME_M1,
    'M5':  mt5.TIMEFRAME_M5,
    'M15': mt5.TIMEFRAME_M15,
    'H1':  mt5.TIMEFRAME_H1,
}
TF_NAME = str(cfg.get('timeframe', 'M5'))
TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)

MT5_CFG = cfg.get('mt5', {}) or {}
CHUNK = int(MT5_CFG.get('bars_chunk', 20000))   # 20k ~ 70–90 dni M5 (zależnie od brokera)
UPDATE_DAYS = int(MT5_CFG.get('update_days', 60))

# ============================================================
# MT5 init / symbol utils
# ============================================================
def init_mt5():
    """Initialize MT5 i wypisz krótkie info o terminalu/koncie."""
    if not ensure_mt5_ready():
        last_err = mt5.last_error()
        raise RuntimeError(f"MT5 not ready (terminal/account). Start MT5 and login. last_error={last_err}")
    term = mt5.terminal_info()
    acc = mt5.account_info()
    parts = []
    if getattr(term, "company", None): parts.append(f"Company={term.company}")
    if getattr(term, "name", None):    parts.append(f"TerminalName={term.name}")
    if getattr(acc, "login", 0):       parts.append(f"Login={acc.login}")
    if getattr(acc, "server", None):   parts.append(f"Server={acc.server}")
    if getattr(acc, "name", None):     parts.append(f"AccountName={acc.name}")
    print("[MT5] " + " | ".join(parts) if parts else "[MT5] initialized")

def ensure_symbol(symbol: str) -> bool:
    """Upewnij się, że symbol jest widoczny w Market Watch."""
    info = mt5.symbol_info(symbol)
    if info is None:
        mt5.symbol_select(symbol, True)
        info = mt5.symbol_info(symbol)
        if info is None:
            return False
    if not info.visible:
        if not mt5.symbol_select(symbol, True):
            return False
    return True

def suggest_similar(symbol: str, limit=12):
    """Podpowiedz nazwy symboli powiązanych (np. GOLD/XAU) – zależne od serwera."""
    out = []
    try:
        for s in mt5.symbols_get():
            nm = s.name.upper()
            if ("GOLD" in nm) or ("XAU" in nm):
                out.append(s.name)
        if not out:
            out = [s.name for s in mt5.symbols_get()][:limit]
        return sorted(set(out))[:limit]
    except Exception:
        return []

# ============================================================
# Helpers
# ============================================================
def with_retries(fn, attempts=3, sleep_s=2, *a, **kw):
    last = None
    for i in range(attempts):
        try:
            return fn(*a, **kw)
        except Exception as e:
            last = e
            time.sleep(sleep_s * (i + 1))
    if last:
        raise last

def _to_df(rates) -> pd.DataFrame:
    """Konwersja stawek MT5 -> kanoniczny DataFrame z sanity-checkiem kolumn i czasu."""
    df = pd.DataFrame(rates)
    if df.empty:
        return df
    df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
    if 'real_volume' in df.columns:
        df.rename(columns={'real_volume': 'tick_volume'}, inplace=True)
    cols = ['time', 'open', 'high', 'low', 'close', 'tick_volume', 'spread']
    # zachowaj tylko znane kolumny
    df = df[[c for c in cols if c in df.columns]].copy()
    # usuń zduplikowane nazwy i rekordy
    df = (df.loc[:, ~df.columns.duplicated()]
            .sort_values('time')
            .drop_duplicates('time')
            .reset_index(drop=True))
    return df

def _make_naive(dt: datetime) -> datetime:
    """Zwróć 'naive' datetime (bez tzinfo) – część brokerów tego wymaga dla copy_rates_range."""
    if dt.tzinfo is not None:
        return dt.replace(tzinfo=None)
    return dt

# ============================================================
# Pobieranie danych
# ============================================================
def fetch_range(symbol, timeframe, utc_from: datetime, utc_to: datetime) -> pd.DataFrame:
    """
    Pobierz zakres 'range' z obejściami:
    • daty 'naive' (bez tzinfo),
    • fallback: tail od końca (copy_rates_from_pos) gdy range zwraca pustkę/Invalid params.
    """
    frm_n = _make_naive(utc_from)
    to_n  = _make_naive(utc_to)
    rates = mt5.copy_rates_range(symbol, timeframe, frm_n, to_n)
    if rates is None or len(rates) == 0:
        # fallback: tail od końca
        tail = mt5.copy_rates_from_pos(symbol, timeframe, 0, CHUNK)
        if tail is None or len(tail) == 0:
            last_err = mt5.last_error()
            raise RuntimeError(f"copy_rates_range empty for '{symbol}'. last_error={last_err}")
        return _to_df(tail)
    return _to_df(rates)

def fetch_from_pos_tail(symbol, timeframe, count) -> pd.DataFrame:
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None or len(rates) == 0:
        last_err = mt5.last_error()
        raise RuntimeError(f"copy_rates_from_pos empty for '{symbol}'. last_error={last_err}")
    return _to_df(rates)

def load_existing_bars(path: str):
    p = Path(path)
    if not p.exists():
        return None
    try:
        df = pd.read_csv(p, parse_dates=['time'])
        if df.empty:
            return None
        # Usuń kolumny techniczne i duplikaty nazw
        bad = [c for c in df.columns if str(c).startswith('Unnamed')]
        if bad:
            df = df.drop(columns=bad)
        df = df.loc[:, ~df.columns.duplicated()]
        keep = ['time','open','high','low','close','tick_volume','spread']
        df = df[[c for c in keep if c in df.columns]].copy()
        # Upewnij się, że 'time' jest UTC-aware
        if pd.api.types.is_datetime64_any_dtype(df['time']):
            if df['time'].dt.tz is None:
                df['time'] = df['time'].dt.tz_localize('UTC')
            else:
                df['time'] = df['time'].dt.tz_convert('UTC')
        else:
            df['time'] = pd.to_datetime(df['time'], utc=True)
        df = (df.sort_values('time')
                .drop_duplicates('time')
                .reset_index(drop=True))
        return df
    except Exception:
        return None

def merge_clip(existing: pd.DataFrame | None, new_df: pd.DataFrame, keep_days: int) -> pd.DataFrame:
    """Scal istniejące i nowe świece, deduplikuj po 'time', przytnij do keep_days i oczyść nagłówki."""
    def _sanitize(d):
        if d is None or d.empty:
            return d
        d = d.loc[:, ~d.columns.duplicated()]
        bad = [c for c in d.columns if str(c).startswith('Unnamed')]
        if bad:
            d = d.drop(columns=bad)
        return d

    existing = _sanitize(existing)
    new_df   = _sanitize(new_df)

    if existing is None or existing.empty:
        base = new_df.copy()
    else:
        common = [c for c in existing.columns if c in new_df.columns]
        base = (pd.concat([existing[common], new_df[common]], axis=0, ignore_index=True, copy=False)
                  .drop_duplicates('time')
                  .sort_values('time'))

    # time -> UTC-aware
    if pd.api.types.is_datetime64_any_dtype(base['time']):
        if base['time'].dt.tz is None:
            base['time'] = base['time'].dt.tz_localize('UTC')
        else:
            base['time'] = base['time'].dt.tz_convert('UTC')
    else:
        base['time'] = pd.to_datetime(base['time'], utc=True)

    # clip do okna czasowego
    cutoff = datetime.now(timezone.utc) - timedelta(days=keep_days + 1)
    base = base[base['time'] >= pd.Timestamp(cutoff)].reset_index(drop=True)
    return base

def fetch_bars_incremental(symbol, timeframe, keep_days: int, update_days: int) -> pd.DataFrame:
    """
    Strategia:
    • Gdy brak pliku – zbierz historię 'od końca' (copy_rates_from_pos) w 1–4 chunkach,
      aż pokryjesz ~keep_days (z marginesem). Jeśli to się nie uda – spróbuj krótkiego range.
    • Gdy plik istnieje – dociągnij ostatnie update_days (mały range z fallbackiem).
    """
    existing = load_existing_bars(BARS_CSV)
    if existing is None:
        wanted_days = keep_days + 2
        parts_df = None
        # 1–4 próby z narastającą liczbą świec (od końca)
        for i in range(1, 5):
            count = CHUNK * i
            try:
                dft = with_retries(lambda: fetch_from_pos_tail(symbol, timeframe, count))
            except Exception:
                time.sleep(1.0)
                continue
            if dft is not None and len(dft) > 0:
                parts_df = dft
                span_days = (parts_df['time'].max() - parts_df['time'].min()).days
                if span_days >= wanted_days:
                    break
        if parts_df is None or parts_df.empty:
            # Ostatnia próba – krótki range (np. 60 dni)
            to  = datetime.now(timezone.utc)
            frm = to - timedelta(days=min(60, wanted_days))
            parts_df = fetch_range(symbol, timeframe, frm, to)
        return merge_clip(None, parts_df, keep_days)

    # Inkrementalnie: dociągnij update_days (mały range z fallbackiem)
    to  = datetime.now(timezone.utc)
    frm = to - timedelta(days=max(2, update_days))
    fresh = fetch_range(symbol, timeframe, frm, to)
    merged = merge_clip(existing, fresh, keep_days)
    return merged
def fetch_ticks(symbol: str, hours: int = 24) -> pd.DataFrame:
    utc_to   = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(hours=hours)
    ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)
    if ticks is None or len(ticks) == 0:
        last_err = mt5.last_error()
        raise RuntimeError(f"No MT5 ticks for '{symbol}'. last_error={last_err}")
    tdf = pd.DataFrame(ticks)
    tdf['time'] = pd.to_datetime(tdf['time'], unit='s', utc=True)
    return tdf[['time', 'bid', 'ask', 'last', 'volume']]

def suggest_costs(tdf: pd.DataFrame) -> dict:
    """Sugeruj koszty na bazie ticków: median/p75 spread, heurystyczny slippage_k."""
    spr = (tdf['ask'] - tdf['bid']).astype(float).replace([np.inf, -np.inf], np.nan).dropna()
    med_spread = float(np.median(spr)) if len(spr) else 0.0
    p75_spread = float(np.percentile(spr, 75)) if len(spr) else 0.0
    mid = (tdf['ask'] + tdf['bid']) / 2.0
    dm = (mid.diff().abs()).replace([np.inf, -np.inf], np.nan).dropna()
    med_dm = float(np.median(dm)) if len(dm) else 0.0
    med_price = float(np.nanmedian(mid)) if len(mid) else 1.0
    # heurystyka: jaką część typowego ruchu stanowi przeciętny skok mida
    slippage_k = float(min(max(med_dm / max(med_price, 1e-12), 0.0), 0.01))
    return {
        'spread_abs_median': round(med_spread, 5),
        'spread_abs_p75':    round(p75_spread, 5),
        'slippage_k_suggested': round(slippage_k, 4)
    }

# ============================================================
# Main
# ============================================================
def main():
    init_mt5()
    try:
        print(f"[CFG] symbol={SYMBOL} tf={TF_NAME} keep_days={HISTORY_DAYS} update_days={UPDATE_DAYS}")

        if not ensure_symbol(SYMBOL):
            similar = suggest_similar(SYMBOL)
            hint = ", ".join(similar) if similar else "(brak propozycji)"
            raise SystemExit(
                "Symbol not found or not visible in Market Watch: '{}'\n"
                "Try one of these (server-specific): {}\n"
                "Also open Symbols window and SHOW the instrument."
                .format(SYMBOL, hint)
            )

        # ŚWIECE
        bars = fetch_bars_incremental(SYMBOL, TF, keep_days=HISTORY_DAYS, update_days=UPDATE_DAYS)
        assert {'time', 'open', 'high', 'low', 'close'}.issubset(bars.columns), "Bars frame malformed"
        assert len(bars) > 100, "Too few bars"
        atomic_write_csv(bars, BARS_CSV)
        print(f"Saved bars: {BARS_CSV} ({len(bars)})")

        # TICKI (opcjonalnie; weekend-safe)
        try:
            ticks = with_retries(lambda: fetch_ticks(SYMBOL, hours=24))
            atomic_write_csv(ticks, TICKS_CSV)
            print(f"Saved ticks: {TICKS_CSV} ({len(ticks)})")

            sugg = suggest_costs(ticks)
            rep = Path('reports'); rep.mkdir(parents=True, exist_ok=True)
            Path('reports/costs_suggestion.yaml').write_text(
                yaml.safe_dump(sugg, allow_unicode=True, sort_keys=False),
                encoding='utf-8'
            )
            print("Cost suggestions -> reports/costs_suggestion.yaml")
        except Exception as e:
            # np. weekend: brak ticków (OK)
            print(f"[warn] ticks unavailable ({e}); keeping previous {TICKS_CSV}")

    finally:
        mt5.shutdown()

if __name__ == '__main__':
    main()

=== FILE: merge_repo.py ===

import os
import chardet

# === KONFIGURACJA ===
repo_path = r"C:\xau_rl"           # ≈öcie≈ºka do folderu repozytorium
output_file = "merged_repo.txt"    # Nazwa wynikowego pliku

# Rozszerzenia plik√≥w do do≈ÇƒÖczenia
included_extensions = (".py", ".ipynb", ".txt", ".md", ".yaml", ".yml", '.ps1')

# Foldery, kt√≥re majƒÖ byƒá pominiƒôte
excluded_dirs = {".venv", "__pycache__", ".git", ".idea", ".vscode", "build", "dist"}

# === FUNKCJA SCALANIA ===
with open(output_file, "w", encoding="utf-8") as outfile:
    for root, dirs, files in os.walk(repo_path):
        # Pomijaj foldery systemowe / techniczne
        dirs[:] = [d for d in dirs if d not in excluded_dirs]

        for filename in files:
            if filename.endswith(included_extensions):
                file_path = os.path.join(root, filename)
                try:
                    # Automatyczne wykrywanie kodowania
                    with open(file_path, "rb") as raw:
                        result = chardet.detect(raw.read(4096))
                        encoding = result["encoding"] or "utf-8"

                    with open(file_path, "r", encoding=encoding, errors="replace") as infile:
                        relative_path = os.path.relpath(file_path, repo_path)
                        outfile.write(f"\n\n=== FILE: {relative_path} ===\n\n")
                        outfile.write(infile.read())

                    print(f"‚úÖ Dodano: {relative_path}")

                except Exception as e:
                    print(f"‚ö†Ô∏è Nie uda≈Ço siƒô odczytaƒá pliku {filename}: {e}")

print(f"\nüéâ Gotowe! Wszystkie pliki scalono do: {output_file}")

=== FILE: quick_env_test.py ===

import yaml, pandas as pd
from env_xau import XauTradingEnv
cfg=yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
df=pd.read_csv(cfg['files']['features_csv'], parse_dates=['time']).sort_values('time').reset_index(drop=True)
df=df.tail(max(int(cfg.get('window',128))+256,512))
env=XauTradingEnv(df, window=int(cfg.get('window',128)),
    spread_abs=cfg['costs']['spread_abs'], commission_rate=cfg['costs']['commission_rate'], slippage_k=cfg['costs']['slippage_k'],
    reward_mode=cfg.get('env',{}).get('reward_mode', cfg.get('reward_mode','pct')), use_close_norm=True,
    min_equity=float(cfg.get('env',{}).get('min_equity',0.8)))
obs,info=env.reset(); print('reset OK', len(obs))
obs,r,term,trunc,info=env.step(1); print('step OK', r, term, trunc)


=== FILE: README.md ===

# XAUUSD RL PPO (M5) – Starter (Demo/Edu)
**Uwaga**: tylko do celów edukacyjnych, backtestów i *paper tradingu* (demo). Brak kodu wysyłającego realne zlecenia.

## Szybki start
```powershell
python -m venv .venv
# .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
# Na końcu zainstaluj TORCH odpowiedni dla CPU/GPU z pytorch.org (wheel).
python fetch__mt5_data.py
python utils\calibration.py
python utils\calibration.py --apply
python features\build_features.py
python rl\train_ppo.py --timesteps 1500000
python rl\evaluate.py --out_dir reports
python utils\make_report.py

24/7 na Windows
Użyj skryptów w ops\ oraz NSSM (patrz ops\install_services.ps1).
Dane i historia
Trzymamy 720 dni historii świec. Zbieracz działa inkrementalnie: dociąga ostatnie update_days (domyślnie 60), scala z istniejacym CSV i przycina do 720 dni. Zapisy CSV są atomowe.


=== FILE: requirements.txt ===

pandas>=2.0
numpy>=1.24
pytz
PyYAML
joblib
matplotlib
tensorboard
MetaTrader5
gymnasium>=0.29
stable-baselines3>=2.2.1
python-dotenv
fredapi
scikit-learn
requests
chardet

=== FILE: save_fred_cache.py ===

# save_gpr_cache.py
import pandas as pd
from pathlib import Path
import requests
import io

# Źródło CSV (benchmark GPR, miesięczny) – publiczny plik z witryny autorów:
URL = "https://www.matteoiacoviello.com/gpr_files/GPR.csv"

def main():
    r = requests.get(URL, timeout=30)
    r.raise_for_status()
    # Oryginalny CSV ma nagłówki 'date' i 'GPR' lub podobne – normalizujemy do (date,gpr)
    df = pd.read_csv(io.StringIO(r.text))
    # Ujednolicenie nazw kolumn:
    cols = {c.lower(): c for c in df.columns}
    # próbujemy znaleźć właściwe kolumny bez względu na wielkość liter
    date_col = [c for c in df.columns if c.lower() == 'date'][0]
    gpr_col  = [c for c in df.columns if c.lower() in ('gpr','gpr_index','value')][0]
    out = pd.DataFrame({
        'date': pd.to_datetime(df[date_col]).dt.tz_localize('UTC').dt.normalize(),
        'gpr': pd.to_numeric(df[gpr_col], errors='coerce')
    }).dropna().sort_values('date')
    out['date'] = out['date'].dt.strftime('%Y-%m-%d')

    path = Path("external/gpr"); path.mkdir(parents=True, exist_ok=True)
    out_path = path / "gpr.csv"
    out.to_csv(out_path, index=False)
    print(f"Saved GPR -> {out_path} ({len(out)} rows)")

if __name__ == "__main__":
    main()

=== FILE: save_te_calendar_cache.py ===

# -*- coding: utf-8 -*-
"""
save_te_calendar_cache.py
Pobiera kalendarz makro z Trading Economics i zapisuje cache do external/calendar_us.csv
Zmienna środowiskowa: TE_API_KEY=<Twój_klucz>
"""

from __future__ import annotations
import os, sys, csv
from pathlib import Path
from datetime import datetime, timedelta, timezone
import requests
import pandas as pd

# Domyślny zakres: od dziś-7 dni do dziś+21 dni (żeby objąć zdarzenia przed i po)
LOOKBACK_DAYS = int(os.getenv("TE_LOOKBACK_DAYS", "7"))
LOOKAHEAD_DAYS = int(os.getenv("TE_LOOKAHEAD_DAYS", "21"))
COUNTRY = os.getenv("TE_COUNTRY", "United States")

def iso_date(d: datetime) -> str:
    return d.strftime("%Y-%m-%d")

def fetch_te_calendar(api_key: str, country: str, start_date: str, end_date: str) -> pd.DataFrame:
    """
    API docs: https://docs.tradingeconomics.com/economic_calendar/
    Endpoint: GET /calendar/{start}/{end}?country=...&c=APIKEY
    """
    base = f"https://api.tradingeconomics.com/calendar/{start_date}/{end_date}"
    params = {"country": country, "c": api_key}
    r = requests.get(base, params=params, timeout=40)
    r.raise_for_status()
    data = r.json()

    rows = []
    for it in data:
        # Pola w TE bywają różne zależnie od endpointu — ujednolicamy
        ts = it.get("Date", it.get("DateUtc") or it.get("date"))
        if not ts:
            continue
        try:
            ts_utc = pd.to_datetime(ts, utc=True)
        except Exception:
            # czasami API zwraca bez Z — wymuś UTC
            ts_utc = pd.to_datetime(ts).tz_localize("UTC")
        rows.append({
            "time_utc" : ts_utc,
            "country"  : it.get("Country") or "",
            "event"    : it.get("Event") or it.get("Category") or "",
            "actual"   : it.get("Actual") if it.get("Actual") not in (None, "") else "",
            "previous" : it.get("Previous") if it.get("Previous") not in (None, "") else "",
            "forecast" : it.get("Forecast") if it.get("Forecast") not in (None, "") else "",
            # importance: TE zwykle zwraca 1..3 lub string; rzutujemy na int (brak -> 0)
            "importance": int(it.get("ImportanceValue", it.get("Importance", 0)) or 0),
        })

    df = pd.DataFrame(rows)
    if df.empty:
        return df
    # Normalizacja i sort
    df = df.sort_values("time_utc").reset_index(drop=True)
    return df

def main():
    api_key = os.getenv("TE_API_KEY")
    if not api_key:
        print("Brak TE_API_KEY. Ustaw zmienną środowiskową TE_API_KEY i uruchom ponownie.", file=sys.stderr)
        sys.exit(2)

    now = datetime.now(timezone.utc)
    start = now - timedelta(days=LOOKBACK_DAYS)
    end   = now + timedelta(days=LOOKAHEAD_DAYS)

    df = fetch_te_calendar(api_key, COUNTRY, iso_date(start), iso_date(end))
    out_dir = Path("external"); out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / "calendar_us.csv"

    if df.empty:
        # jeśli API nic nie zwróciło — nie nadpisujemy istniejącego cache
        if out_path.exists():
            print(f"[warn] Pusta odpowiedź z TE. Zachowuję istniejący cache: {out_path}")
            sys.exit(0)
        else:
            print("[warn] Pusta odpowiedź z TE i brak wcześniejszego cache. Nic nie zapisano.")
            sys.exit(0)

    # Zapisz CSV w formacie zgodnym z features/calendar_features.py
    df_out = df.copy()
    # Upewnijmy się, że kolumny są w oczekiwanym zestawie
    df_out["time_utc"] = df_out["time_utc"].dt.strftime("%Y-%m-%dT%H:%M:%SZ")
    cols = ["time_utc","country","event","actual","previous","forecast","importance"]
    df_out = df_out[cols]
    df_out.to_csv(out_path, index=False, quoting=csv.QUOTE_MINIMAL)
    print(f"Saved TE calendar -> {out_path} ({len(df_out)} rows)")

if __name__ == "__main__":
    main()

=== FILE: features\build_features.py ===

# -*- coding: utf-8 -*-
import pandas as pd, numpy as np, yaml, json, os
from pathlib import Path

from features.fundamentals import build_fundamentals_features
from features.calendar_features import make_event_features, load_calendar_csv

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
BARS = cfg['files']['bars_csv']; FEAT = cfg['files']['features_csv']

def macd(series, fast=12, slow=26, signal=9):
    ema_fast = series.ewm(span=fast, adjust=False).mean()
    ema_slow = series.ewm(span=slow, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False).mean()
    return macd_line, signal_line, macd_line - signal_line
def bbands(series, period=20, n_std=2.0):
    ma = series.rolling(period).mean()
    sd = series.rolling(period).std()
    upper = ma + n_std*sd; lower = ma - n_std*sd
    width = (upper - lower) / (ma.replace(0,np.nan).abs() + 1e-12)
    return ma, upper, lower, width

def add_tech_features(df):
    df = df.sort_values('time').reset_index(drop=True)
    c = df['close']
    df['ret1'] = np.log(c).diff()
    df['ema10'] = c.ewm(span=10).mean(); df['ema50']=c.ewm(span=50).mean(); df['ema200']=c.ewm(span=200).mean()
    d = c.diff(); up=d.clip(lower=0).ewm(alpha=1/14,adjust=False).mean(); down=(-d.clip(upper=0)).ewm(alpha=1/14,adjust=False).mean(); rs=up/(down+1e-12)
    df['rsi14'] = 100 - (100/(1+rs))
    h,l,cl = df['high'], df['low'], df['close']
    tr = np.maximum(h-l, np.maximum((h-cl.shift()).abs(), (l-cl.shift()).abs()))
    df['atr14'] = tr.ewm(alpha=1/14, adjust=False).mean()
    m,s,hst = macd(c); df['macd']=m; df['macd_signal']=s; df['macd_hist']=hst
    bb_ma, bb_up, bb_lo, bb_w = bbands(c); df['bb_ma']=bb_ma; df['bb_up']=bb_up; df['bb_lo']=bb_lo; df['bb_width']=bb_w
    df['minute']=df['time'].dt.hour*60+df['time'].dt.minute
    df['tod_sin']=np.sin(2*np.pi*df['minute']/1440); df['tod_cos']=np.cos(2*np.pi*df['minute']/1440); df.drop(columns=['minute'], inplace=True)
    df['dow']=df['time'].dt.dayofweek
    df['dow_sin']=np.sin(2*np.pi*df['dow']/7); df['dow_cos']=np.cos(2*np.pi*df['dow']/7); df.drop(columns=['dow'], inplace=True)
    df['close_log']=np.log(c.clip(lower=1e-12))
    mu = df['close_log'].rolling(2000, min_periods=200).mean()
    sd = df['close_log'].rolling(2000, min_periods=200).std().replace(0,np.nan)
    df['close_norm']=(df['close_log']-mu)/(sd+1e-8)
    # normalizacja rolling wybranych featurów
    feat_cols=['ret1','ema10','ema50','ema200','rsi14','atr14','macd','macd_signal','macd_hist','bb_ma','bb_up','bb_lo','bb_width','tod_sin','tod_cos','dow_sin','dow_cos']
    for col in feat_cols:
        mu = df[col].rolling(2000, min_periods=200).mean()
        sd = df[col].rolling(2000, min_periods=200).std().replace(0,np.nan)
        df[col]=(df[col]-mu)/(sd+1e-8)
    return df

def main():
    bars = pd.read_csv(BARS, parse_dates=['time']).sort_values('time').reset_index(drop=True)
    df = add_tech_features(bars)

    # --- FUNDAMENTALS ---
    fred_key = os.getenv("FRED_API_KEY", None)
    gpr_csv = cfg.get('fundamentals',{}).get('gpr_csv', None)
    cot_csv = cfg.get('fundamentals',{}).get('cot_csv', None)
    usd_series_id = cfg.get('fundamentals',{}).get('usd_series_id', "DTWEXBGS")
    fund = None
    try:
        fund = build_fundamentals_features(BARS, fred_key, gpr_csv=gpr_csv, cot_csv=cot_csv, usd_series_id=usd_series_id)
    except Exception as e:
        print(f"[warn] fundamentals skipped: {e}")

    # --- CALENDAR ---
    cal_csv = cfg.get('calendar',{}).get('calendar_csv', None)
    cal_events = cfg.get('calendar',{}).get('events_whitelist', None)
    cal_df = None
    cal_feat = None
    if cal_csv and Path(cal_csv).exists():
        cal_df = load_calendar_csv(cal_csv)
        cal_feat = make_event_features(BARS, cal_df, events_whitelist=cal_events, impact_threshold=2)

    # MERGE
    parts = [df]
    if fund is not None:
        parts.append(fund.drop(columns=['time']))
    if cal_feat is not None:
        parts.append(cal_feat.drop(columns=['time']))
    full = pd.concat(parts, axis=1)

    # dropna i zapis
    full = full.dropna().reset_index(drop=True)
    Path(FEAT).parent.mkdir(parents=True, exist_ok=True)
    full.to_csv(FEAT, index=False)

    # features_spec.json (jeśli istnieje – zachowaj; jeśli nie – wygeneruj z defaultu technicznego)
    base_cols = ['time','open','high','low','close','tick_volume','spread']
    feature_columns = [c for c in full.columns if c not in base_cols + ['close_log','close_norm']]
    spec = {"feature_columns": feature_columns, "price_column":"close_norm"}
    Path('models').mkdir(parents=True, exist_ok=True)
    Path('models/features_spec.json').write_text(json.dumps(spec, ensure_ascii=False, indent=2), encoding='utf-8')
    print("Saved features and features_spec.json (with fundamentals/calendar if available).")

if __name__ == "__main__":
    main()

=== FILE: features\calendar_features.py ===

# -*- coding: utf-8 -*-
"""
features/calendar_features.py
Kalendarz makro (Trading Economics) -> cechy 'time-to-event', 'post-event', dummies,
oraz 'surprise' (tylko w minucie publikacji). Bezpośredni request jest opcjonalny:
możesz też zasilać z lokalnego CSV (cache).

Dokumentacja TE Calendar API: https://docs.tradingeconomics.com/economic_calendar/  # cite: turn2search2
"""

from __future__ import annotations
import os, json
import pandas as pd, numpy as np
from pathlib import Path
from datetime import timezone

try:
    import requests  # type: ignore
except Exception:
    requests = None

DEFAULT_EVENTS = [
    "Non Farm Payrolls", "Unemployment Rate", "CPI", "Core CPI", "Core PCE",
    "ISM Manufacturing PMI", "ISM Services PMI", "FOMC Interest Rate Decision",
    "Fed Press Conference"
]

def fetch_te_calendar(api_key: str,
                      country: str = "United States",
                      start_iso: str = None,
                      end_iso: str = None) -> pd.DataFrame:
    """
    Pobiera kalendarz z Trading Economics. Jeśli requests brak -> podnieś błąd.
    Zwraca DataFrame z kolumnami: time_utc, country, event, actual, previous, forecast, importance
    """
    if requests is None:
        raise ImportError("Brak 'requests'. Użyj lokalnego CSV cache albo zainstaluj requests.")

    base = "https://api.tradingeconomics.com/calendar"
    params = {"country": country, "c": api_key}
    if start_iso and end_iso:
        base = f"{base}/{start_iso}/{end_iso}"

    r = requests.get(base, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    rows = []
    for it in data:
        try:
            ts = pd.to_datetime(it.get('Date', it.get('DateUtc') or it.get('date')), utc=True)
            rows.append({
                'time_utc': ts,
                'country': it.get('Country'),
                'event': it.get('Event') or it.get('Category'),
                'actual': it.get('Actual') if it.get('Actual') not in (None, '') else np.nan,
                'previous': it.get('Previous') if it.get('Previous') not in (None, '') else np.nan,
                'forecast': it.get('Forecast') if it.get('Forecast') not in (None, '') else np.nan,
                'importance': it.get('Importance') or it.get('ImportanceValue') or 0
            })
        except Exception:
            continue
    df = pd.DataFrame(rows).dropna(subset=['time_utc','event']).sort_values('time_utc')
    return df

def load_calendar_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df['time_utc'] = pd.to_datetime(df['time_utc'], utc=True)
    return df.sort_values('time_utc')

def make_event_features(bars_csv: str,
                        calendar_df: pd.DataFrame,
                        events_whitelist: list[str] | None = None,
                        impact_threshold: int = 2) -> pd.DataFrame:
    """
    Tworzy cechy eventowe na indeksie M5 z bars_csv:
    - tt_event_min: minuty do najbliższego eventu (clipped np. do +/- 720)
    - post_event_min: minuty od ostatniego eventu
    - is_event_hi: 1 w minucie publikacji eventu o wysokim impakcie
    - surprise_{short}: (actual - forecast) tylko w minucie publikacji
    - one-hot per event (bazując na events_whitelist)

    'impact_threshold' – filtruje tylko eventy o wysokim impakcie.
    """
    bars = pd.read_csv(bars_csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
    bars['time'] = bars['time'].dt.tz_localize("UTC") if bars['time'].dt.tz is None else bars['time'].dt.tz_convert("UTC")

    ev = calendar_df.copy()
    if events_whitelist:
        ev = ev[ev['event'].isin(events_whitelist)].copy()

    # Map importance do int (0..3)
    def _imp(v):
        if v in (None, np.nan, ''): return 0
        try:
            return int(v)
        except Exception:
            return 0
    ev['imp_i'] = ev['importance'].apply(_imp)
    ev = ev[ev['imp_i'] >= impact_threshold]

    idx = pd.DatetimeIndex(bars['time'])
    # Najbliższy event >= now
    next_ts = np.array([ev['time_utc'][ev['time_utc'] >= t].min() if (ev['time_utc'] >= t).any() else pd.NaT for t in idx])
    prev_ts = np.array([ev['time_utc'][ev['time_utc'] <= t].max() if (ev['time_utc'] <= t).any() else pd.NaT for t in idx])

    tt_event_min = pd.Series(((pd.to_datetime(next_ts) - idx).astype('timedelta64[m]')).astype('float'), index=bars.index)
    post_event_min = pd.Series(((idx - pd.to_datetime(prev_ts)).astype('timedelta64[m]')).astype('float'), index=bars.index)
    tt_event_min = tt_event_min.clip(-720, 720).fillna(720.0)
    post_event_min = post_event_min.clip(-720, 720).fillna(720.0)

    # Flaga minuty publikacji (dokładne dopasowanie do 5-min slotu)
    pub_map = {pd.Timestamp(t, tz="UTC"): i for i, t in enumerate(ev['time_utc'])}
    is_event_hi = bars['time'].map(lambda t: 1 if t.floor('5min') in pub_map else 0).astype(int)

    # Surprise: tylko gdy jest exact minuta publikacji
    ev = ev.set_index(ev['time_utc'].dt.floor('5min'))
    ev_small = ev[['event','actual','forecast']].copy()
    ev_small['surprise'] = pd.to_numeric(ev_small['actual'], errors='coerce') - pd.to_numeric(ev_small['forecast'], errors='coerce')
    surprise = pd.Series(0.0, index=bars.index)
    surp_idx = bars['time'].dt.floor('5min')
    surprise.loc[is_event_hi == 1] = ev_small.reindex(surp_idx[is_event_hi == 1])['surprise'].values

    # One-hot po nazwie eventu (whitelist)
    features = {
        'tt_event_min': tt_event_min.values,
        'post_event_min': post_event_min.values,
        'is_event_hi': is_event_hi.values,
        'ev_surprise': surprise.fillna(0.0).values,
    }
    if events_whitelist:
        for name in events_whitelist:
            k = f"ev_{name.lower().replace(' ', '_').replace('/', '_')}"
            mask = (surp_idx.isin(ev_small.index)) & (ev_small.reindex(surp_idx)['event'] == name)
            features[k] = mask.astype(int).values

    out = pd.DataFrame(features, index=bars.index)
    out.insert(0, 'time', bars['time'].values)
    return out


=== FILE: features\discover.py ===

# -*- coding: utf-8 -*-
"""
features/discover.py
Selektor cech:
- IC (Spearman) cecha vs ret_{+1} (M5) na rolling oknach -> mediana IC, stabilność.
- (Opcjonalnie) ważność z prostego modelu (Lasso / GBDT), jeśli scikit-learn dostępny.
- Ograniczenie zmian: max +/- 'change_frac' względem poprzedniego features_spec.json.

Wejście: pełny plik features_csv (po zbudowaniu), config window, K, change_frac.
Wyjście: models/features_spec.json (lista kolumn + column price 'close_norm').
"""

from __future__ import annotations
import json
from pathlib import Path
import pandas as pd, numpy as np
from scipy.stats import spearmanr

try:
    from sklearn.linear_model import LassoCV  # type: ignore
    HAVE_SKL = True
except Exception:
    HAVE_SKL = False

BASE_COLS = ['time','open','high','low','close','tick_volume','spread']

def compute_targets(df: pd.DataFrame) -> pd.Series:
    ret1 = np.log(df['close']).diff().shift(-1)  # target = kolejna świeca
    return ret1

def rolling_ic(df_feat: pd.DataFrame, target: pd.Series, win: int = 5000) -> pd.Series:
    scores = {}
    valid_cols = [c for c in df_feat.columns if c not in BASE_COLS + ['close_log','close_norm']]
    # prosty IC na całym okresie (dla szybkości); wariant rolling można dodać później
    for c in valid_cols:
        a = df_feat[c].values
        b = target.values
        ok = np.isfinite(a) & np.isfinite(b)
        if ok.sum() < 500:  # min próbek
            scores[c] = 0.0
            continue
        rho, _ = spearmanr(a[ok], b[ok])
        scores[c] = 0.0 if (rho is None or np.isnan(rho)) else float(rho)
    return pd.Series(scores).sort_values(ascending=False)

def model_importance(df_feat: pd.DataFrame, target: pd.Series, top_n: int = 128) -> pd.Series:
    if not HAVE_SKL:
        return pd.Series(dtype=float)
    cols = [c for c in df_feat.columns if c not in BASE_COLS + ['close_log','close_norm']]
    X = df_feat[cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).values
    y = target.replace([np.inf,-np.inf], np.nan).fillna(0.0).values
    if len(cols) == 0 or len(df_feat) < 1000:
        return pd.Series(dtype=float)
    model = LassoCV(cv=5, n_jobs=None, max_iter=10000).fit(X, y)
    imp = pd.Series(np.abs(model.coef_), index=cols)
    return imp.sort_values(ascending=False).head(top_n)

def select_topK(ic: pd.Series, imp: pd.Series | None, K: int = 96) -> list[str]:
    if imp is None or imp.empty:
        return list(ic.head(K).index)
    # fuzja rankingów (simple rank sum)
    ic_rank = ic.rank(ascending=False, method='dense')
    imp = imp.reindex(ic.index).fillna(0.0)
    imp_rank = imp.rank(ascending=False, method='dense')
    score = 1.0/ic_rank + 1.0/(imp_rank.replace(0, np.nan))
    return list(score.sort_values(ascending=False).head(K).index)

def apply_change_budget(prev_cols: list[str], new_cols: list[str], change_frac: float = 0.1) -> list[str]:
    if not prev_cols:
        return new_cols
    K = len(prev_cols)
    max_changes = max(1, int(round(K * change_frac)))
    keep = [c for c in new_cols if c in prev_cols]
    add  = [c for c in new_cols if c not in prev_cols][:max_changes]
    # Usuń najgorsze z poprzednich, żeby zwolnić miejsca
    drop_candidates = [c for c in prev_cols if c not in new_cols]
    drop = drop_candidates[:max_changes]
    # Final: (prev - drop) + add (z zachowaniem kolejności now_cols)
    result = [c for c in prev_cols if c not in drop]
    for c in new_cols:
        if c not in result and c in add:
            result.append(c)
    # Zabezpieczenie na dokładny rozmiar
    if len(result) > K:
        result = result[:K]
    elif len(result) < K:
        # dopełnij z 'keep' na wszelki wypadek
        for c in keep:
            if c not in result:
                result.append(c)
            if len(result) >= K:
                break
    return result

def write_features_spec(path_json: str, feat_cols: list[str], price_col: str = "close_norm") -> None:
    Path(path_json).parent.mkdir(parents=True, exist_ok=True)
    with open(path_json, "w", encoding="utf-8") as f:
        json.dump({"feature_columns": feat_cols, "price_column": price_col}, f, ensure_ascii=False, indent=2)

def main_discover(features_csv: str,
                  out_json: str = "models/features_spec.json",
                  topK: int = 96,
                  change_frac: float = 0.10):
    df = pd.read_csv(features_csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
    target = compute_targets(df)
    ic = rolling_ic(df, target)
    imp = model_importance(df, target, top_n=topK*2)  # opcjonalne
    proposal = select_topK(ic, imp, K=topK)

    prev = []
    p = Path(out_json)
    if p.exists():
        prev = json.loads(p.read_text(encoding='utf-8')).get("feature_columns", [])
        # nie bierzemy kolumn, których już fizycznie nie ma:
        prev = [c for c in prev if c in df.columns]

    final_cols = apply_change_budget(prev, proposal, change_frac=change_frac)
    write_features_spec(out_json, final_cols, "close_norm")
    print(f"[discover] Wrote {out_json} with {len(final_cols)} features.")

=== FILE: features\fundamentals.py ===

# -*- coding: utf-8 -*-
"""
features/fundamentals.py
Fundamenty dla XAUUSD: realne stopy (DGS10 - T10YIE), USD index (np. FRED),
GPR (Caldara & Iacoviello), COT Gold (CFTC). As-of alignment i FFill -> M5.

Wymaga: pandas, numpy, (opcjonalnie) fredapi, requests, python-dotenv.
Źródła:
- FRED: DGS10, T10YIE (real 10y = DGS10 - T10YIE)      [St. Louis Fed]  # cite: turn2search20
- CFTC COT (COMEX Gold) weekly                          # cite: turn2search9
- GPR (Caldara & Iacoviello) monthly                    # cite: turn2search28
"""

from __future__ import annotations
import os, json
import pandas as pd, numpy as np
from datetime import datetime, timezone, timedelta
from pathlib import Path

try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

# ---------- Utils ----------

def _zscore(s: pd.Series, win: int = 252) -> pd.Series:
    mu = s.rolling(win, min_periods=max(10, win//10)).mean()
    sd = s.rolling(win, min_periods=max(10, win//10)).std().replace(0, np.nan)
    return (s - mu) / (sd + 1e-12)

def _ensure_dt_utc(s: pd.Series) -> pd.Series:
    if not isinstance(s.index, pd.DatetimeIndex):
        raise ValueError("Index must be DatetimeIndex")
    if s.index.tz is None:
        s.index = s.index.tz_localize("UTC")
    else:
        s.index = s.index.tz_convert("UTC")
    return s

def _asof_ffill_daily_to_m5(daily: pd.Series,
                            bars_m5: pd.DataFrame,
                            available_hour_utc: int = 23,
                            available_minute_utc: int = 59) -> pd.Series:
    """
    Dla wartości dziennych zakładamy, że 'stają się znane' o available_* UTC.
    Każda wartość jest użyta od (data, available_time) do kolejnej publikacji.
    To zachowuje konserwatywność (eliminuje look-ahead).
    """
    s = daily.copy()
    s = _ensure_dt_utc(s)
    # Utnij do zakresu świec
    s = s[(s.index <= bars_m5['time'].max())]
    if s.empty:
        return pd.Series(index=bars_m5.index, dtype=float)

    # Tworzymy serię znaczników as-of
    idx_asof = []
    vals = []
    for ts, val in s.items():
        ts_asof = pd.Timestamp(ts.date(), tz=timezone.utc) + timedelta(
            hours=available_hour_utc, minutes=available_minute_utc)
        idx_asof.append(ts_asof)
        vals.append(val)
    asof_series = pd.Series(vals, index=pd.DatetimeIndex(idx_asof, tz="UTC")).sort_index()

    # Reindeks do M5 time i ffill
    m5_idx = pd.DatetimeIndex(bars_m5['time']).tz_convert("UTC")
    out = asof_series.reindex(m5_idx, method="ffill")
    out.name = daily.name
    return out

# ---------- FRED ----------

def load_fred_series(series_id: str, api_key: str | None) -> pd.Series:
    """
    Ładuje serię z FRED (jeśli brak fredapi/klucza -> spodziewa się CSV w external/fred/{series_id}.csv)
    CSV format: DATE,VALUE
    """
    try:
        from fredapi import Fred  # type: ignore
    except Exception:
        Fred = None

    if Fred is not None and api_key:
        fred = Fred(api_key=api_key)
        s = fred.get_series(series_id)
        s = s.dropna().astype(float)
        s.index = pd.to_datetime(s.index, utc=True)
        s.name = series_id
        return s

    # Fallback: lokalny CSV
    p = Path("external/fred") / f"{series_id}.csv"
    if not p.exists():
        raise FileNotFoundError(f"Brak {p}. Zainstaluj fredapi lub umieść CSV.")
    df = pd.read_csv(p)
    df['DATE'] = pd.to_datetime(df['DATE'], utc=True)
    s = pd.Series(df.iloc[:, 1].astype(float).values, index=df['DATE'], name=series_id)
    return s

def build_real10y_and_usd(bars_m5: pd.DataFrame,
                          fred_key: str | None,
                          usd_series_id: str = "DTWEXBGS",
                          real_asof_hh: int = 23,
                          real_asof_mm: int = 59) -> pd.DataFrame:
    """
    Zwraca DataFrame z kolumnami: real10y, real10y_z, d_real10y, usd, usd_z
    - real10y = DGS10 - T10YIE (oba dzienne)  # cite: turn2search20
    - usd = indeks USD (np. DTWEXBGS z FRED)  # cite: turn2search25
    """
    dgs10 = load_fred_series("DGS10", fred_key)
    t10yie = load_fred_series("T10YIE", fred_key)
    real10y_daily = (dgs10 - t10yie).dropna()
    usd_daily = load_fred_series(usd_series_id, fred_key).dropna()

    real10y_m5 = _asof_ffill_daily_to_m5(real10y_daily, bars_m5, real_asof_hh, real_asof_mm)
    usd_m5     = _asof_ffill_daily_to_m5(usd_daily,     bars_m5, 23, 59)

    # pochodne
    real10y_z = _zscore(real10y_m5, 252*24*12//5)  # ~252 dni w M5 ≈ 12*24*252/5
    d_real10y = real10y_m5.diff()

    usd_z = _zscore(usd_m5, 252*24*12//5)

    out = pd.DataFrame({
        'real10y': real10y_m5.values,
        'real10y_z': real10y_z.values,
        'd_real10y': d_real10y.values,
        'usd': usd_m5.values,
        'usd_z': usd_z.values
    }, index=bars_m5.index)
    return out

# ---------- GPR ----------

def load_gpr_local_csv(path: str = "external/gpr/gpr.csv") -> pd.Series:
    """
    Oczekuje miesięcznego GPR z kolumnami ['date','gpr'], gdzie date to YYYY-MM-01 UTC.
    Oryginał i metodologia: Caldara & Iacoviello (AER 2022)                      # cite: turn2search28
    Pobrane dane umieść offline – unikamy look-ahead przez FFill miesięczny.
    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Brak {p}. Pobierz GPR i zapisz jako CSV.")
    df = pd.read_csv(p)
    df['date'] = pd.to_datetime(df['date'], utc=True)
    s = pd.Series(df['gpr'].astype(float).values, index=df['date'], name='gpr')
    return s

def gpr_to_m5(bars_m5: pd.DataFrame, gpr_monthly: pd.Series) -> pd.Series:
    gpr_monthly = _ensure_dt_utc(gpr_monthly)
    # As-of: konserwatywnie początek kolejnego miesiąca 00:00 UTC
    idx_asof = [pd.Timestamp(ts.year + (1 if ts.month==12 else 0),
                             1 if ts.month==12 else ts.month+1, 1, tz="UTC") for ts in gpr_monthly.index]
    s_asof = pd.Series(gpr_monthly.values, index=pd.DatetimeIndex(idx_asof, tz="UTC")).sort_index()
    m5_idx = pd.DatetimeIndex(bars_m5['time']).tz_convert('UTC')
    gpr_m5 = s_asof.reindex(m5_idx, method='ffill')
    gpr_m5.name = 'gpr'
    return gpr_m5

# ---------- COT (Gold) ----------

def load_cot_gold_local_csv(path: str = "external/cot_gold.csv") -> pd.DataFrame:
    """
    Oczekuje tygodniowego COT dla Gold (COMEX): kolumny ['date','mm_long','mm_short'] dla Managed Money.
    Dane z CFTC (Disaggregated report).                               # cite: turn2search9
    """
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Brak {p}. Przygotuj eksport z CFTC i zapisz CSV.")
    df = pd.read_csv(p)
    df['date'] = pd.to_datetime(df['date'], utc=True)
    return df[['date','mm_long','mm_short']].sort_values('date')

def cot_to_m5(bars_m5: pd.DataFrame, cot_df: pd.DataFrame) -> pd.DataFrame:
    cot_df = cot_df.copy()
    cot_df['mm_net'] = cot_df['mm_long'].astype(float) - cot_df['mm_short'].astype(float)
    # Publikacja: piątek (as-of), dane na wtorek – używamy konserwatywnie od piątku 21:00 UTC
    asof_ts = cot_df['date'] + pd.Timedelta(days=3)  # wtorek -> piątek
    asof_ts = asof_ts.dt.tz_localize('UTC') + pd.Timedelta(hours=21)
    ser = pd.Series(cot_df['mm_net'].values, index=asof_ts, name='cot_mm_net').sort_index()

    m5_idx = pd.DatetimeIndex(bars_m5['time']).tz_convert("UTC")
    mm_net_m5 = ser.reindex(m5_idx, method='ffill')
    mm_net_z  = _zscore(mm_net_m5, 52)  # roczny tygodniowy zscore
    out = pd.DataFrame({'cot_mm_net': mm_net_m5.values, 'cot_mm_net_z': mm_net_z.values}, index=bars_m5.index)
    return out

# ---------- Orkiestracja ----------

def build_fundamentals_features(bars_csv: str,
                                fred_api_key: str | None,
                                gpr_csv: str | None = None,
                                cot_csv: str | None = None,
                                usd_series_id: str = "DTWEXBGS") -> pd.DataFrame:
    """
    Ładuje bars M5, dokleja fundamenty (real10y/usd/gpr/cot) jako cechy na indeksie świec M5.
    """
    bars = pd.read_csv(bars_csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
    core = build_real10y_and_usd(bars, fred_api_key, usd_series_id)

    cols = [core]
    if gpr_csv:
        gpr = load_gpr_local_csv(gpr_csv)
        gpr_m5 = gpr_to_m5(bars, gpr)
        cols.append(pd.DataFrame({'gpr': gpr_m5.values, 'gpr_z': _zscore(gpr_m5, 36).values}, index=core.index))
    if cot_csv:
        cot = load_cot_gold_local_csv(cot_csv)
        cols.append(cot_to_m5(bars, cot))

    out = pd.concat(cols, axis=1)
    out.insert(0, 'time', bars['time'].values)
    return out

=== FILE: features\__init__.py ===



=== FILE: ops\daemon_calibration_biweekly.ps1 ===

# Kalibracja kosztów co 3 dni 22:00
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\calibration_3d.log"
$LOCK="$REPO\logs\services\calibration_3d.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once { & $PY "utils\calibration.py" "--apply" *>> $LOG }
function Seconds-To-Next-22 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(22)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [calibration] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-22)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [calibration] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [calibration] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [calibration] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (3*24*3600 - 60)
}


=== FILE: ops\daemon_daily_report.ps1 ===

# Raport walidacyjny codziennie o 23:00 (czas lokalny)
$ErrorActionPreference = "Stop"
$REPO = "C:\xau_rl"
$PY = "C:\xau_rl\.venv\Scripts\python.exe"
$LOG = "C:\xau_rl\logs\services\daily_report.log"

New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO

function Run-Once {
  & $PY "rl\evaluate.py" "--out_dir" "reports" *>> $LOG
  & $PY "utils\make_report.py" *>> $LOG
}

function Seconds-To-Next-23 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(23)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}

# Poczekaj do 23:00, potem wykonuj raz dziennie
while ($true) {
  $sleep = Seconds-To-Next-23
  Start-Sleep -Seconds $sleep
  try {
    "`n[$(Get-Date -Format o)] [daily_report] start" | Out-File -FilePath $LOG -Append -Encoding utf8
    Run-Once
    "`n[$(Get-Date -Format o)] [daily_report] done" | Out-File -FilePath $LOG -Append -Encoding utf8
  } catch {
    "[ERROR] $($_ | Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
  }
  "[$(Get-Date -Format o)] [bootstrap_alive]" | Out-File -FilePath $LOG -Append -Encoding utf8
  Start-Sleep -Seconds (24*3600 - 60)  # buffer
}

=== FILE: ops\daemon_feature_discovery.ps1 ===

# Selekcja cech co 3 dni 21:00
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\feature_discovery.log"
$LOCK="$REPO\logs\services\feature_discovery.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "ops\feature_discovery_weekly.py" "--topK" "96" "--change_frac" "0.10" *>> $LOG
}
function Seconds-To-Next-21 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(21)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [feature_discovery] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-21)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [feature_discovery] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [feature_discovery] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [feature_discovery] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (3*24*3600 - 60)
}


=== FILE: ops\daemon_fetch_features.ps1 ===

# Fetch bars/ticks + build features co 10 minut (lock-file)
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\fetch_features.log"
$LOCK="$REPO\logs\services\fetch_features.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "fetch__mt5_data.py"         *>> $LOG
  & $PY "features\build_features.py" *>> $LOG
}
"[$(Get-Date -Format o)] [fetch_features] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [fetch_features] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [fetch_features] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [fetch_features] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds 600
}


=== FILE: ops\daemon_mini_update.ps1 ===

# Mini-update PPO co 6 godzin (lock-file). Po treningu -> eval, report, save/promote
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\mini_update.log"
$LOCK="$REPO\logs\services\mini_update.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "ops\mini_update_ppo.py" "--timesteps" "300000" *>> $LOG
}
"[$(Get-Date -Format o)] [mini_update] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [mini_update] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [mini_update] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [mini_update] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (6*3600)
}


=== FILE: ops\daemon_nightly_report.ps1 ===

# Codzienny raport walidacyjny o 23:00 (eval + report)
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\nightly_report.log"
$LOCK="$REPO\logs\services\nightly_report.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "rl\evaluate.py" "--out_dir" "reports" *>> $LOG
  & $PY "utils\make_report.py"                 *>> $LOG
}
function Seconds-To-Next-2300 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(23)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [nightly_report] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-2300)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [nightly_report] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [nightly_report] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [nightly_report] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (24*3600 - 60)
}


=== FILE: ops\daemon_nightly_train.ps1 ===

# Pełny nightly train (1.5M ts) + eval + report + save/promote o 00:30
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\nightly_train.log"
$LOCK="$REPO\logs\services\nightly_train.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "rl\train_ppo.py" "--timesteps" "1500000" *>> $LOG
  & $PY "rl\evaluate.py" "--out_dir" "reports"    *>> $LOG
  & $PY "utils\make_report.py"                    *>> $LOG
  & $PY "ops\save_candidate.py"                   *>> $LOG
  & $PY "ops\promote_challenger.py"               *>> $LOG
}
function Seconds-To-Next-0030 {
  $now = Get-Date
  $target = [DateTime]::Today.AddDays(1).AddHours(0).AddMinutes(30)
  if ($now.Hour -lt 0 -or ($now.Hour -eq 0 -and $now.Minute -lt 30)) { $target = [DateTime]::Today.AddHours(0).AddMinutes(30) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [nightly_train] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-0030)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [nightly_train] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [nightly_train] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [nightly_train] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (24*3600 - 60)
}


=== FILE: ops\daemon_te_calendar.ps1 ===

# TE calendar codziennie 05:00 + rebuild features (wymaga TE_API_KEY w env usługi)
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\te_calendar.log"
$LOCK="$REPO\logs\services\te_calendar.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "save_te_calendar_cache.py" *>> $LOG
  & $PY "features\build_features.py" *>> $LOG
}
function Seconds-To-Next-0500 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(5)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [te_calendar] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-0500)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [te_calendar] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [te_calendar] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [te_calendar] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (24*3600 - 60)
}


=== FILE: ops\env.ps1 ===

$ErrorActionPreference = "Stop"
$RepoRoot = Split-Path -Parent $MyInvocation.MyCommand.Path | Split-Path
$Py = Join-Path $RepoRoot ".venv\Scripts\python.exe"
$Cfg = Join-Path $RepoRoot "config.yaml"
$LogDir = Join-Path $RepoRoot "logs"
New-Item -ItemType Directory -Force -Path $LogDir | Out-Null
function RunPy([string]$script, [string]$args="") {
& $Py $script $args 2>&1 | Tee-Object -FilePath (Join-Path $LogDir ((Split-Path $script -Leaf) + ".log")) -Append
}


=== FILE: ops\feature_discovery_weekly.py ===

# -*- coding: utf-8 -*-
"""
ops/feature_discovery_weekly.py
Tygodniowy job: na podstawie aktualnego FEAT wybiera top-K cech (IC + opcjonalnie model),
respektuje budżet zmian (+/- change_frac), aktualizuje models/features_spec.json.

Uruchamiaj np. w niedzielę 21:00 UTC. Po sukcesie -> pełny nightly train (bo zmienia się obs_dim).
"""
import argparse, yaml, json
from pathlib import Path
import pandas as pd
from features.discover import main_discover

parser = argparse.ArgumentParser()
parser.add_argument('--features_csv', default=None, help='Ścieżka do CSV z cechami; domyślnie z config.yaml')
parser.add_argument('--topK', type=int, default=96)
parser.add_argument('--change_frac', type=float, default=0.10)
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = args.features_csv or cfg['files']['features_csv']
OUT_JSON = 'models/features_spec.json'

def main():
    if not Path(FEAT).exists():
        raise SystemExit(f"[feature_discovery] Brak {FEAT}. Najpierw uruchom features/build_features.py")
    main_discover(FEAT, OUT_JSON, topK=args.topK, change_frac=args.change_frac)

if __name__ == '__main__':
    main()
``

=== FILE: ops\install_all_services.ps1 ===

<#
install_all_services.ps1
Instaluje/aktualizuje wszystkie usługi NSSM dla pipeline M5:
- xau_paper_loop (python -m paper_demo.paper_loop_mt5_demo)
- xau_fetch_features (co 10 min)
- xau_mini_update (co 6h)
- xau_nightly_report (codziennie 23:00)
- xau_nightly_train (codziennie 00:30)
- xau_feature_discovery (co 3 dni 21:00)
- xau_calibration (co 3 dni 22:00)
- xau_te_calendar (codziennie 05:00)

Autor: (twój) 
#>

[CmdletBinding(SupportsShouldProcess=$true)]
param(
  [string]$RepoRoot = "C:\xau_rl",
  [string]$NssmPath = "C:\nssm\nssm.exe",
  [string]$PythonPath = "$RepoRoot\.venv\Scripts\python.exe",
  [string]$UserAccount = "$env:COMPUTERNAME\$env:USERNAME",
  [securestring]$SecurePassword,
  [string]$Password,
  [switch]$UsePwsh7,                      # użyj pwsh.exe z PS7 zamiast klasycznego powershell.exe
  [string]$TEApiKey = ""                  # jeśli podasz, zostanie ustawiony w xau_te_calendar
)

#region helpers
function Write-Info($msg){ Write-Host "[INFO] $msg" -ForegroundColor Cyan }
function Write-Warn($msg){ Write-Host "[WARN] $msg" -ForegroundColor Yellow }
function Write-Err ($msg){ Write-Host "[ERR ] $msg" -ForegroundColor Red }

function Ensure-Dir($p){ if(-not(Test-Path $p)){ New-Item -ItemType Directory -Force -Path $p | Out-Null } }

function Assert-File($p){
  if(-not(Test-Path $p)){ throw "Brak pliku: $p" }
}

function Get-PlainPassword([securestring]$sec, [string]$plain){
  if ($sec){
    $BSTR = [Runtime.InteropServices.Marshal]::SecureStringToBSTR($sec)
    try { return [Runtime.InteropServices.Marshal]::PtrToStringBSTR($BSTR) }
    finally { if($BSTR -ne [IntPtr]::Zero){ [Runtime.InteropServices.Marshal]::ZeroFreeBSTR($BSTR) } }
  }
  return $plain
}

function PS-Exe(){
  if($UsePwsh7){
    $p = "C:\Program Files\PowerShell\7\pwsh.exe"
    if(Test-Path $p){ return $p }
    Write-Warn "PS7 nie znaleziony w `$p. Używam klasycznego PowerShell."
  }
  return "C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe"
}

function NSSM-Check(){
  if(-not (Test-Path $NssmPath)){ throw "Nie znaleziono NSSM: $NssmPath" }
}

function NSSM-Install($name, $application, $parameters, $appDir, $envExtra, $stdout, $stderr, [switch]$DelayedStart, [switch]$NoConsole){
  & $NssmPath stop   $name 2>$null | Out-Null
  & $NssmPath remove $name confirm | Out-Null
  & $NssmPath install $name $application | Out-Null

  & $NssmPath set $name AppParameters  $parameters | Out-Null
  & $NssmPath set $name AppDirectory   $appDir     | Out-Null
  if($envExtra){ & $NssmPath set $name AppEnvironmentExtra $envExtra | Out-Null }

  if($stdout){ & $NssmPath set $name AppStdout $stdout | Out-Null }
  if($stderr){ & $NssmPath set $name AppStderr $stderr | Out-Null }

  & $NssmPath set $name AppRotateFiles   1       | Out-Null
  & $NssmPath set $name AppRotateOnline  1       | Out-Null
  & $NssmPath set $name AppRotateBytes   10485760| Out-Null
  & $NssmPath set $name AppRotateSeconds 86400   | Out-Null
  if($NoConsole){ & $NssmPath set $name AppNoConsole 1 | Out-Null }
  if($DelayedStart){ & $NssmPath set $name Start SERVICE_DELAYED_AUTO_START | Out-Null }
}

function NSSM-SetLogon($name, $user, $plainPassword){
  if([string]::IsNullOrWhiteSpace($user)){ return }
  if([string]::IsNullOrWhiteSpace($plainPassword)){
    Write-Warn "Nie ustawiono hasła dla usługi $name (ObjectName=$user). Zrób to ręcznie (nssm edit) lub podaj -SecurePassword."
    return
  }
  & $NssmPath set $name ObjectName $user $plainPassword | Out-Null
}

function Write-File($path, [string]$content){
  $dir = Split-Path $path
  Ensure-Dir $dir
  Set-Content -LiteralPath $path -Value $content -Encoding UTF8
  Unblock-File -LiteralPath $path -ErrorAction SilentlyContinue
  Write-Info "Zapisano: $path"
}
#endregion

try{
  NSSM-Check
  Ensure-Dir $RepoRoot
  $logs = Join-Path $RepoRoot "logs\services"
  Ensure-Dir $logs
  Ensure-Dir (Join-Path $RepoRoot "paper_demo")

  if(-not(Test-Path $PythonPath)){ Write-Warn "Brak Pythona venv: $PythonPath" }

  #region generate daemon scripts (harmonogram zaktualizowany pod M5)
  $psFetch = @'
# Fetch bars/ticks + build features co 10 minut (lock-file)
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\fetch_features.log"
$LOCK="$REPO\logs\services\fetch_features.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "fetch__mt5_data.py"         *>> $LOG
  & $PY "features\build_features.py" *>> $LOG
}
"[$(Get-Date -Format o)] [fetch_features] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [fetch_features] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [fetch_features] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [fetch_features] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds 600
}
'@

  $psMiniUpdate = @'
# Mini-update PPO co 6 godzin (lock-file). Po treningu -> eval, report, save/promote
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\mini_update.log"
$LOCK="$REPO\logs\services\mini_update.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "ops\mini_update_ppo.py" "--timesteps" "300000" *>> $LOG
}
"[$(Get-Date -Format o)] [mini_update] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [mini_update] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [mini_update] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [mini_update] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (6*3600)
}
'@

  $psNightlyReport = @'
# Codzienny raport walidacyjny o 23:00 (eval + report)
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\nightly_report.log"
$LOCK="$REPO\logs\services\nightly_report.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "rl\evaluate.py" "--out_dir" "reports" *>> $LOG
  & $PY "utils\make_report.py"                 *>> $LOG
}
function Seconds-To-Next-2300 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(23)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [nightly_report] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-2300)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [nightly_report] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [nightly_report] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [nightly_report] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (24*3600 - 60)
}
'@

  $psNightlyTrain = @'
# Pełny nightly train (1.5M ts) + eval + report + save/promote o 00:30
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\nightly_train.log"
$LOCK="$REPO\logs\services\nightly_train.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "rl\train_ppo.py" "--timesteps" "1500000" *>> $LOG
  & $PY "rl\evaluate.py" "--out_dir" "reports"    *>> $LOG
  & $PY "utils\make_report.py"                    *>> $LOG
  & $PY "ops\save_candidate.py"                   *>> $LOG
  & $PY "ops\promote_challenger.py"               *>> $LOG
}
function Seconds-To-Next-0030 {
  $now = Get-Date
  $target = [DateTime]::Today.AddDays(1).AddHours(0).AddMinutes(30)
  if ($now.Hour -lt 0 -or ($now.Hour -eq 0 -and $now.Minute -lt 30)) { $target = [DateTime]::Today.AddHours(0).AddMinutes(30) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [nightly_train] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-0030)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [nightly_train] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [nightly_train] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [nightly_train] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (24*3600 - 60)
}
'@

  $psFeatDisc = @'
# Selekcja cech co 3 dni 21:00
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\feature_discovery.log"
$LOCK="$REPO\logs\services\feature_discovery.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "ops\feature_discovery_weekly.py" "--topK" "96" "--change_frac" "0.10" *>> $LOG
}
function Seconds-To-Next-21 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(21)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [feature_discovery] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-21)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [feature_discovery] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [feature_discovery] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [feature_discovery] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (3*24*3600 - 60)
}
'@

  $psCalibration = @'
# Kalibracja kosztów co 3 dni 22:00
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\calibration_3d.log"
$LOCK="$REPO\logs\services\calibration_3d.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once { & $PY "utils\calibration.py" "--apply" *>> $LOG }
function Seconds-To-Next-22 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(22)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [calibration] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-22)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [calibration] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [calibration] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [calibration] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (3*24*3600 - 60)
}
'@

  $psTECalendar = @'
# TE calendar codziennie 05:00 + rebuild features (wymaga TE_API_KEY w env usługi)
$ErrorActionPreference="Stop"
$REPO="C:\xau_rl"
$PY="$REPO\.venv\Scripts\python.exe"
$LOG="$REPO\logs\services\te_calendar.log"
$LOCK="$REPO\logs\services\te_calendar.lock"
New-Item -ItemType Directory -Force -Path (Split-Path $LOG) | Out-Null
Set-Location $REPO
function Acquire-Lock { if(Test-Path $LOCK){$false}else{New-Item -ItemType File -Path $LOCK -Force|Out-Null;$true}}
function Release-Lock { if(Test-Path $LOCK){Remove-Item $LOCK -Force} }
function Run-Once {
  & $PY "save_te_calendar_cache.py" *>> $LOG
  & $PY "features\build_features.py" *>> $LOG
}
function Seconds-To-Next-0500 {
  $now = Get-Date
  $target = [DateTime]::Today.AddHours(5)
  if ($now -ge $target) { $target = $target.AddDays(1) }
  return int.TotalSeconds
}
"[$(Get-Date -Format o)] [te_calendar] bootstrap_alive" | Out-File -FilePath $LOG -Append -Encoding utf8
while($true){
  Start-Sleep -Seconds (Seconds-To-Next-0500)
  $ts=Get-Date -Format o
  try{
    if(Acquire-Lock){
      "`n[$ts] [te_calendar] start" | Out-File -FilePath $LOG -Append -Encoding utf8
      Run-Once
      "`n[$(Get-Date -Format o)] [te_calendar] done" | Out-File -FilePath $LOG -Append -Encoding utf8
      Release-Lock
    }else{
      "`n[$ts] [te_calendar] skipped (locked)" | Out-File -FilePath $LOG -Append -Encoding utf8
    }
  }catch{
    "[ERROR] $($_|Out-String)" | Out-File -FilePath $LOG -Append -Encoding utf8
    Release-Lock
  }
  Start-Sleep -Seconds (24*3600 - 60)
}
'@

  Write-File (Join-Path $RepoRoot "ops\daemon_fetch_features.ps1")        $psFetch
  Write-File (Join-Path $RepoRoot "ops\daemon_mini_update.ps1")           $psMiniUpdate
  Write-File (Join-Path $RepoRoot "ops\daemon_nightly_report.ps1")        $psNightlyReport
  Write-File (Join-Path $RepoRoot "ops\daemon_nightly_train.ps1")         $psNightlyTrain
  Write-File (Join-Path $RepoRoot "ops\daemon_feature_discovery.ps1")     $psFeatDisc
  Write-File (Join-Path $RepoRoot "ops\daemon_calibration_biweekly.ps1")  $psCalibration
  Write-File (Join-Path $RepoRoot "ops\daemon_te_calendar.ps1")           $psTECalendar
  #endregion

  #region install services
  $psExe = PS-Exe
  $envBase = "PYTHONPATH=$RepoRoot"
  if($TEApiKey){ $envTE = "$envBase;TE_API_KEY=$TEApiKey" } else { $envTE = $envBase }

  $pw = Get-PlainPassword -sec $SecurePassword -plain $Password

  # 1) paper loop (python module)
  NSSM-Install -name "xau_paper_loop" `
    -application $PythonPath `
    -parameters "-m paper_demo.paper_loop_mt5_demo" `
    -appDir $RepoRoot `
    -envExtra $envBase `
    -stdout (Join-Path $logs "paper_loop.log") `
    -stderr (Join-Path $logs "paper_loop.err.log") `
    -DelayedStart:$true
  NSSM-SetLogon "xau_paper_loop" $UserAccount $pw

  # 2) fetch_features (daemon ps1)
  NSSM-Install -name "xau_fetch_features" `
    -application $psExe `
    -parameters "-NoProfile -ExecutionPolicy Bypass -WindowStyle Hidden -File $RepoRoot\ops\daemon_fetch_features.ps1" `
    -appDir $RepoRoot `
    -envExtra $envBase `
    -stdout (Join-Path $logs "xau_fetch_features.out.log") `
    -stderr (Join-Path $logs "xau_fetch_features.err.log") `
    -DelayedStart:$true -NoConsole
  NSSM-SetLogon "xau_fetch_features" $UserAccount $pw

  # 3) mini_update
  NSSM-Install -name "xau_mini_update" `
    -application $psExe `
    -parameters "-NoProfile -ExecutionPolicy Bypass -WindowStyle Hidden -File $RepoRoot\ops\daemon_mini_update.ps1" `
    -appDir $RepoRoot `
    -envExtra $envBase `
    -stdout (Join-Path $logs "xau_mini_update.out.log") `
    -stderr (Join-Path $logs "xau_mini_update.err.log") `
    -DelayedStart:$true -NoConsole
  NSSM-SetLogon "xau_mini_update" $UserAccount $pw

  # 4) nightly_report
  NSSM-Install -name "xau_nightly_report" `
    -application $psExe `
    -parameters "-NoProfile -ExecutionPolicy Bypass -WindowStyle Hidden -File $RepoRoot\ops\daemon_nightly_report.ps1" `
    -appDir $RepoRoot `
    -envExtra $envBase `
    -stdout (Join-Path $logs "xau_nightly_report.out.log") `
    -stderr (Join-Path $logs "xau_nightly_report.err.log") `
    -DelayedStart:$true -NoConsole
  NSSM-SetLogon "xau_nightly_report" $UserAccount $pw

  # 5) nightly_train
  NSSM-Install -name "xau_nightly_train" `
    -application $psExe `
    -parameters "-NoProfile -ExecutionPolicy Bypass -WindowStyle Hidden -File $RepoRoot\ops\daemon_nightly_train.ps1" `
    -appDir $RepoRoot `
    -envExtra $envBase `
    -stdout (Join-Path $logs "xau_nightly_train.out.log") `
    -stderr (Join-Path $logs "xau_nightly_train.err.log") `
    -DelayedStart:$true -NoConsole
  NSSM-SetLogon "xau_nightly_train" $UserAccount $pw

  # 6) feature_discovery (co 3 dni)
  NSSM-Install -name "xau_feature_discovery" `
    -application $psExe `
    -parameters "-NoProfile -ExecutionPolicy Bypass -WindowStyle Hidden -File $RepoRoot\ops\daemon_feature_discovery.ps1" `
    -appDir $RepoRoot `
    -envExtra $envBase `
    -stdout (Join-Path $logs "xau_feature_discovery.out.log") `
    -stderr (Join-Path $logs "xau_feature_discovery.err.log") `
    -DelayedStart:$true -NoConsole
  NSSM-SetLogon "xau_feature_discovery" $UserAccount $pw

  # 7) calibration (co 3 dni)
  NSSM-Install -name "xau_calibration" `
    -application $psExe `
    -parameters "-NoProfile -ExecutionPolicy Bypass -WindowStyle Hidden -File $RepoRoot\ops\daemon_calibration_biweekly.ps1" `
    -appDir $RepoRoot `
    -envExtra $envBase `
    -stdout (Join-Path $logs "xau_calibration.out.log") `
    -stderr (Join-Path $logs "xau_calibration.err.log") `
    -DelayedStart:$true -NoConsole
  NSSM-SetLogon "xau_calibration" $UserAccount $pw

  # 8) te_calendar (codziennie 05:00)
  NSSM-Install -name "xau_te_calendar" `
    -application $psExe `
    -parameters "-NoProfile -ExecutionPolicy Bypass -WindowStyle Hidden -File $RepoRoot\ops\daemon_te_calendar.ps1" `
    -appDir $RepoRoot `
    -envExtra $envTE `
    -stdout (Join-Path $logs "xau_te_calendar.out.log") `
    -stderr (Join-Path $logs "xau_te_calendar.err.log") `
    -DelayedStart:$true -NoConsole
  NSSM-SetLogon "xau_te_calendar" $UserAccount $pw
  #endregion

  # start services
  $svcs = @(
    "xau_paper_loop","xau_fetch_features","xau_mini_update",
    "xau_nightly_report","xau_nightly_train",
    "xau_feature_discovery","xau_calibration","xau_te_calendar"
  )
  foreach($s in $svcs){
    & $NssmPath start $s | Out-Null
  }

  Start-Sleep -Seconds 2
  Write-Host "`n=== STATUS USŁUG ==="
  foreach($s in $svcs){
    sc.exe query $s
  }

  Write-Host "`n=== TAIL LOGÓW (jeśli istnieją) ==="
  Get-ChildItem "$logs\*.log" | Sort-Object LastWriteTime | %{
    Write-Host "`n--- $($_.Name) ---"
    try { Get-Content $_.FullName -Tail 20 } catch {}
  }

  Write-Info "Gotowe. Jeśli któryś serwis ma STATUS=PAUSED, upewnij się, że Application wskazuje na 64-bit $psExe i plik .ps1 istnieje."
  if(-not $SecurePassword -and -not $Password){
    Write-Warn "Usługi uruchomione z ObjectName=$UserAccount bez ustawionego hasła. Jeśli wymagane, ustaw je: nssm edit <service>."
  }
}
catch{
  Write-Err $_
  exit 1
}

=== FILE: ops\install_scheduled_tasks.ps1 ===

# ops\install_scheduled_tasks.ps1
param(
  [string]$RepoRoot = "C:\xau_rl",
  [string]$PyExe    = "C:\xau_rl\.venv\Scripts\python.exe",
  [string]$UserName = "$env:USERNAME"
)

$ErrorActionPreference = "Stop"

# --- Katalog logów zadań ---
$TasksLogDir = Join-Path $RepoRoot "logs\tasks"
New-Item -ItemType Directory -Force -Path $TasksLogDir | Out-Null

# --- Funkcja: generowanie wrappera z logowaniem ---
function New-LogWrapper {
  param(
    [Parameter(Mandatory=$true)][string]$WrapperPath,
    [Parameter(Mandatory=$true)][string]$CommandLine,
    [Parameter(Mandatory=$true)][string]$LogPath
  )
  $content = @"
`$ErrorActionPreference = 'Continue'
`$ts = (Get-Date).ToString('yyyy-MM-dd HH:mm:ss')
"[$ts] START $CommandLine" | Out-File -FilePath "$LogPath" -Append -Encoding utf8

try {
  & $CommandLine 1>> "$LogPath" 2>> "$LogPath"
} catch {
  "`$($_.Exception.Message)" | Out-File -FilePath "$LogPath" -Append -Encoding utf8
}

`$ts2 = (Get-Date).ToString('yyyy-MM-dd HH:mm:ss')
"[$ts2] END $CommandLine" | Out-File -FilePath "$LogPath" -Append -Encoding utf8
"@
  $content | Set-Content -Path $WrapperPath -Encoding utf8 -Force
}

# --- 1) fetch + features co 10 min ---
$wrap1 = Join-Path $RepoRoot "ops\wr_fetch_features.ps1"
$log1  = Join-Path $TasksLogDir "fetch_features.log"
$cmd1  = "`"$PyExe`" `"$RepoRoot\fetch__mt5_data.py`" && `"$PyExe`" -m features.build_features"
New-LogWrapper -WrapperPath $wrap1 -CommandLine $cmd1 -LogPath $log1

schtasks /Create /TN "xau_fetch_features" `
  /TR "powershell -ExecutionPolicy Bypass -File `"$wrap1`"" `
  /SC MINUTE /MO 10 /RU $UserName /RL HIGHEST /F

# --- 2) Trading Economics cache raz dziennie 06:05 ---
$wrap2 = Join-Path $RepoRoot "ops\wr_te_calendar.ps1"
$log2  = Join-Path $TasksLogDir "te_calendar.log"
$cmd2  = "`"$PyExe`" `"$RepoRoot\save_te_calendar_cache.py`""
New-LogWrapper -WrapperPath $wrap2 -CommandLine $cmd2 -LogPath $log2

schtasks /Create /TN "xau_te_calendar" `
  /TR "powershell -ExecutionPolicy Bypass -File `"$wrap2`"" `
  /SC DAILY /ST 06:05 /RU $UserName /RL HIGHEST /F

# --- 3) mini-update co 6 godzin ---
$wrap3 = Join-Path $RepoRoot "ops\wr_mini_update.ps1"
$log3  = Join-Path $TasksLogDir "mini_update_6h.log"
$cmd3  = "`"$PyExe`" `"$RepoRoot\ops\mini_update_ppo.py`" --timesteps 300000 --train_days 120 --val_days 30"
New-LogWrapper -WrapperPath $wrap3 -CommandLine $cmd3 -LogPath $log3

schtasks /Create /TN "xau_mini_update" `
  /TR "powershell -ExecutionPolicy Bypass -File `"$wrap3`"" `
  /SC HOURLY /MO 6 /RU $UserName /RL HIGHEST /F

# --- 4) nightly train 23:10 ---
$wrap4 = Join-Path $RepoRoot "ops\wr_nightly_train.ps1"
$log4  = Join-Path $TasksLogDir "nightly_train.log"
$cmd4  = "`"$PyExe`" `"$RepoRoot\rl\train_ppo.py`" --timesteps 1500000"
New-LogWrapper -WrapperPath $wrap4 -CommandLine $cmd4 -LogPath $log4

schtasks /Create /TN "xau_nightly_train" `
  /TR "powershell -ExecutionPolicy Bypass -File `"$wrap4`"" `
  /SC DAILY /ST 23:10 /RU $UserName /RL HIGHEST /F

# --- 5) nightly raport/promocja 23:40 ---
$wrap5 = Join-Path $RepoRoot "ops\wr_nightly_report.ps1"
$log5  = Join-Path $TasksLogDir "nightly_report.log"
$cmd5  = "`"$PyExe`" `"$RepoRoot\rl\evaluate.py`" --out_dir reports && `"$PyExe`" `"$RepoRoot\utils\make_report.py`" && `"$PyExe`" `"$RepoRoot\ops\save_candidate.py`" && `"$PyExe`" `"$RepoRoot\ops\promote_challenger.py`""
New-LogWrapper -WrapperPath $wrap5 -CommandLine $cmd5 -LogPath $log5

schtasks /Create /TN "xau_nightly_report" `
  /TR "powershell -ExecutionPolicy Bypass -File `"$wrap5`"" `
  /SC DAILY /ST 23:40 /RU $UserName /RL HIGHEST /F

# --- 6) cleanup logów 03:15 ---
$wrap6 = Join-Path $RepoRoot "ops\wr_cleanup.ps1"
$log6  = Join-Path $TasksLogDir "cleanup.log"
$cmd6  = "powershell -ExecutionPolicy Bypass -File `"$RepoRoot\ops\cleanup_logs.ps1`" -RepoRoot `"$RepoRoot`" -CompressOldReports"

# wrapper dla cleanupu
$cleanupWrapper = @"
`$ts = (Get-Date).ToString('yyyy-MM-dd HH:mm:ss')
"[$ts] START cleanup" | Out-File -FilePath "$log6" -Append -Encoding utf8
try { & $cmd6 1>> "$log6" 2>> "$log6" } catch { "`$($_.Exception.Message)" | Out-File -FilePath "$log6" -Append -Encoding utf8 }
`$ts2 = (Get-Date).ToString('yyyy-MM-dd HH:mm:ss')
"[$ts2] END cleanup" | Out-File -FilePath "$log6" -Append -Encoding utf8
"@
$cleanupWrapper | Set-Content -Path $wrap6 -Encoding utf8 -Force

schtasks /Create /TN "xau_cleanup_logs" `
  /TR "powershell -ExecutionPolicy Bypass -File `"$wrap6`"" `
  /SC DAILY /ST 03:15 /RU $UserName /RL HIGHEST /F

Write-Host "Zadania zainstalowane. Logi zadan: $TasksLogDir"

=== FILE: ops\install_services.ps1 ===

# ops\install_services.ps1
param(
  [string]$RepoRoot = "C:\xau_rl",
  [string]$ServiceName = "xau_paper_loop",
  [string]$NssmExe = "C:\nssm\nssm.exe"  # jeśli nie w PATH, podaj pełną ścieżkę, np. "C:\tools\nssm\nssm.exe"
)

$ErrorActionPreference = "Stop"

# Ścieżki
$PyExe       = Join-Path $RepoRoot ".venv\Scripts\python.exe"
$ScriptPath  = Join-Path $RepoRoot "paper_demo\paper_loop_mt5_demo.py"
$LogsDir     = Join-Path $RepoRoot "logs\services"
$StdOutLog   = Join-Path $LogsDir  "paper_loop.log"
$StdErrLog   = Join-Path $LogsDir  "paper_loop.err.log"

# Katalogi
New-Item -ItemType Directory -Force -Path $LogsDir | Out-Null
New-Item -ItemType Directory -Force -Path (Join-Path $RepoRoot "paper_demo") | Out-Null

# Instalacja usługi NSSM
& $NssmExe install $ServiceName $PyExe $ScriptPath

# Katalog roboczy usługi
& $NssmExe set $ServiceName AppDirectory $RepoRoot

# Rotacja logów NSSM
& $NssmExe set $ServiceName AppStdout $StdOutLog
& $NssmExe set $ServiceName AppStderr $StdErrLog
& $NssmExe set $ServiceName AppRotateFiles 1
& $NssmExe set $ServiceName AppRotateOnline 1
& $NssmExe set $ServiceName AppRotateBytes 10485760   # 10 MB
& $NssmExe set $ServiceName AppRotateDelay 86400      # 24h

# Automatyczny restart
& $NssmExe set $ServiceName AppRestartDelay 5000
& $NssmExe set $ServiceName AppThrottle 1500

# Start typu Automat
& $NssmExe set $ServiceName Start SERVICE_AUTO_START

# Uruchom usługę
& $NssmExe start $ServiceName

Write-Host "Usługa $ServiceName zainstalowana i uruchomiona."
Write-Host "Logi: $StdOutLog (stdout), $StdErrLog (stderr)"

=== FILE: ops\mini_update_ppo.py ===

# -*- coding: utf-8 -*-
"""
ops/mini_update_ppo.py
Mini-update PPO co 6h: warm-start z championa, rolling window danych (ostatnie N dni),
krÃ³tszy trening, mniejszy LR/clip_range. Po treningu -> zapis modelu i vecnorm -> walidacja -> save_candidate/promote.
"""

import argparse, yaml, json
import pandas as pd
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor, sync_envs_normalization
from gymnasium.wrappers import TimeLimit
from env_xau import XauTradingEnv

parser = argparse.ArgumentParser()
parser.add_argument('--timesteps', type=int, default=300000)
parser.add_argument('--train_days', type=int, default=120)
parser.add_argument('--val_days', type=int, default=30)
parser.add_argument('--seed', type=int, default=42)
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = Path(cfg['files']['model_path'])
VECNORM_PATH = Path(cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl'))
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))

df = pd.read_csv(FEAT, parse_dates=['time']).sort_values('time').reset_index(drop=True)
end_time = df['time'].max()
train_start = end_time - pd.Timedelta(days=args.train_days + args.val_days)
train_end = end_time - pd.Timedelta(days=args.val_days)
val_start = train_end
train_df = df[(df['time']>=train_start)&(df['time']<train_end)].copy()
val_df = df[(df['time']>=val_start)&(df['time']<=end_time)].copy()

features_spec=None
fspec = Path('models/features_spec.json')
if fspec.exists():
    features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)

def make_env(sub_df):
    return TimeLimit(XauTradingEnv(sub_df, window=WINDOW,
                                   spread_abs=COSTS['spread_abs'],
                                   commission_rate=COSTS['commission_rate'],
                                   slippage_k=COSTS['slippage_k'],
                                   reward_mode=REWARD_MODE,
                                   use_close_norm=True,
                                   flip_penalty=FLIP_PENALTY,
                                   trade_hours_utc=TRADE_HOURS,
                                   enforce_flat_outside_hours=True,
                                   features_spec=features_spec,
                                   min_equity=MIN_EQ),
                     max_episode_steps=6000)

venv_train = DummyVecEnv([lambda: make_env(train_df)])
venv_train = VecMonitor(venv_train)
venv_train = VecNormalize(venv_train, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)

venv_eval = DummyVecEnv([lambda: make_env(val_df)])
venv_eval = VecMonitor(venv_eval)
venv_eval = VecNormalize(venv_eval, training=False, norm_obs=True, norm_reward=False)

# Warm-start z championa
model = PPO.load(str(MODEL_PATH), env=venv_train)
model.learning_rate = 3e-5
model.clip_range = 0.1

sync_envs_normalization(venv_eval, venv_train)

print(f"[mini-update] Training on {len(train_df)} rows (~{args.train_days}d), timesteps={args.timesteps}")
model.learn(total_timesteps=args.timesteps)
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
model.save(str(MODEL_PATH))
venv_train.save(str(VECNORM_PATH))
print(f"[mini-update] Saved model and VecNormalize.")

# Walidacja + raport
import subprocess
subprocess.run(["python","rl/evaluate.py","--out_dir","reports"])
subprocess.run(["python","utils/make_report.py"])
subprocess.run(["python","ops/save_candidate.py"])
subprocess.run(["python","ops/promote_challenger.py"])

=== FILE: ops\promote_challenger.py ===

# ops/promote_challenger.py
import json, shutil
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
REG  = ROOT / 'models' / 'registry'
REG.mkdir(parents=True, exist_ok=True)

CUR_TXT  = REG / 'current.txt'
MODEL_DST = ROOT / 'models' / 'ppo_xauusd_m5.zip'
VEC_DST   = ROOT / 'models' / 'vecnorm_xauusd_m5.pkl'

def load_metrics(d: Path):
    if not d: return None
    m = d / 'metrics.json'
    return json.loads(m.read_text('utf-8')) if m.exists() else None

def pick_latest_dir():
    dirs = [p for p in REG.iterdir() if p.is_dir()]
    return sorted(dirs, key=lambda p: p.name)[-1] if dirs else None

def better(m_new, m_old):
    if m_old is None: return True
    s_new, s_old = (m_new.get('sharpe') or 0.0), (m_old.get('sharpe') or 0.0)
    dd_new, dd_old = (m_new.get('max_dd') or -1.0), (m_old.get('max_dd') or -1.0)
    fe_new, fe_old = (m_new.get('final_equity') or 0.0), (m_old.get('final_equity') or 0.0)
    dd_ok = (dd_new >= dd_old * 1.2)
    s_ok  = (s_new >= s_old * 1.10) or (s_new >= 0.10 and s_new > s_old)
    if s_new == 0.0 and s_old == 0.0:
        return (fe_new > fe_old) and dd_ok
    return s_ok and dd_ok

def promote(new_dir: Path):
    REG.mkdir(parents=True, exist_ok=True)
    CUR_TXT.write_text(new_dir.name, encoding='utf-8')
    shutil.copy2(new_dir/'model.zip', MODEL_DST)
    if (new_dir/'vecnorm.pkl').exists():
        shutil.copy2(new_dir/'vecnorm.pkl', VEC_DST)
    print(f"[promote] Champion -> {new_dir.name}")

def main():
    latest = pick_latest_dir()
    if latest is None:
        print('[promote] No candidates.'); return
    m_new = load_metrics(latest)
    if m_new is None:
        print('[promote] Latest has no metrics.json'); return
    cur_dir = (REG / CUR_TXT.read_text('utf-8').strip()) if CUR_TXT.exists() else None
    m_old = load_metrics(cur_dir) if cur_dir and cur_dir.exists() else None
    if better(m_new, m_old):
        promote(latest)
    else:
        print('[promote] Challenger not better. No promotion.')

if __name__ == '__main__':
    main()

=== FILE: ops\save_candidate.py ===

# ops/save_candidate.py
import json, shutil
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
REG  = ROOT / 'models' / 'registry'
REG.mkdir(parents=True, exist_ok=True)

MODEL = ROOT / 'models' / 'ppo_xauusd_m5.zip'
VEC   = ROOT / 'models' / 'vecnorm_xauusd_m5.pkl'
METR  = ROOT / 'reports' / 'val_metrics.txt'
EQP   = ROOT / 'reports' / 'equity_val.png'

def parse_metrics_txt(p: Path):
    d = {}
    if p.exists():
        txt = p.read_text(encoding='utf-8')
        for line in txt.splitlines():
            if ':' in line:
                k, v = line.split(':', 1)
                k = k.strip().lower().replace(' ', '')
                try:
                    d[k] = float(v.strip())
                except:
                    pass
    return d

def main():
    m = parse_metrics_txt(METR)
    if not m:
        print('[save_candidate] No metrics.')
        return
    tag = 'cand_'
    n = 0
    while (REG / f"{tag}{n:02d}").exists():
        n += 1
    dst = REG / f"{tag}{n:02d}"
    dst.mkdir(parents=True, exist_ok=True)
    shutil.copy2(MODEL, dst/'model.zip')
    if VEC.exists():
        shutil.copy2(VEC, dst/'vecnorm.pkl')
    if EQP.exists():
        shutil.copy2(EQP, dst/'equity.png')
    (dst/'metrics.json').write_text(json.dumps({
        'final_equity': m.get('final_equity'),
        'max_dd': m.get('maxdrawdown'),
        'sharpe': m.get('sharpe_daily'),
    }, indent=2), encoding='utf-8')
    print(f'[save_candidate] Saved candidate -> {dst}')

if __name__ == '__main__':
    main()


=== FILE: ops\wr_cleanup.ps1 ===

$ts = (Get-Date).ToString('yyyy-MM-dd HH:mm:ss')
"[] START cleanup" | Out-File -FilePath "C:\xau_rl\logs\tasks\cleanup.log" -Append -Encoding utf8
try { & powershell -ExecutionPolicy Bypass -File "C:\xau_rl\ops\cleanup_logs.ps1" -RepoRoot "C:\xau_rl" -CompressOldReports 1>> "C:\xau_rl\logs\tasks\cleanup.log" 2>> "C:\xau_rl\logs\tasks\cleanup.log" } catch { "$(.Exception.Message)" | Out-File -FilePath "C:\xau_rl\logs\tasks\cleanup.log" -Append -Encoding utf8 }
$ts2 = (Get-Date).ToString('yyyy-MM-dd HH:mm:ss')
"[] END cleanup" | Out-File -FilePath "C:\xau_rl\logs\tasks\cleanup.log" -Append -Encoding utf8


=== FILE: paper_demo\paper_loop_mt5_demo.py ===

# -*- coding: utf-8 -*-
import MetaTrader5 as mt5
import pandas as pd, numpy as np, yaml, time, logging, json
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from utils.mt5_health import ensure_mt5_ready


# Upewnij się, że folder na logi istnieje zanim skonfigurujemy logging
Path('paper_demo').mkdir(parents=True, exist_ok=True)
logging.basicConfig(
    filename='paper_demo/paper_trading.log',
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)

# --- Konfiguracja z config.yaml ---
cfg = yaml.safe_load(open('config.yaml', 'r', encoding='utf-8'))

SYMBOL       = cfg['symbol']
WINDOW       = int(cfg.get('window', 128))
MODEL_PATH   = cfg['files']['model_path']
VECNORM_PATH = cfg.get('files', {}).get('vecnorm_path', 'models/vecnorm_xauusd_m5.pkl')

TF_MAP  = {'M1': mt5.TIMEFRAME_M1, 'M5': mt5.TIMEFRAME_M5, 'M15': mt5.TIMEFRAME_M15}
TF_NAME = cfg.get('timeframe', 'M5')
TF      = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)

COSTS       = cfg['costs']
SPREAD_ABS  = float(COSTS['spread_abs'])
COMM        = float(COSTS['commission_rate'])
SLIP_K      = float(COSTS['slippage_k'])

ENV_CFG     = cfg.get('env', {})
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', None)
ENF_FLAT    = bool(ENV_CFG.get('enforce_flat_outside_hours', True))


def in_trade_hours(ts_utc) -> bool:
    """Czy aktualna świeca mieści się w dozwolonych godzinach handlu (UTC)."""
    if not TRADE_HOURS or len(TRADE_HOURS) != 2:
        return True
    s = [int(x) for x in str(TRADE_HOURS[0]).split(':')]
    e = [int(x) for x in str(TRADE_HOURS[1]).split(':')]
    from datetime import time as dtime
    start = dtime(s[0], s[1] if len(s) > 1 else 0)
    end   = dtime(e[0], e[1] if len(e) > 1 else 0)
    t = ts_utc.to_pydatetime().time()
    if start <= end:
        return (t >= start) and (t <= end)
    # okno przez północ, np. 22:00–06:00
    return (t >= start) or (t <= end)


# Opcjonalna specyfikacja listy cech (jeśli istnieje po build_features/discover)
features_spec = None
fs = Path('models/features_spec.json')
if fs.exists():
    features_spec = json.loads(fs.read_text(encoding='utf-8')).get('feature_columns', None)


def add_features_incremental(df: pd.DataFrame) -> pd.DataFrame:
    """Lekka wersja wyliczania cech (jak w features/build_features.py, ale bez fundamentów/kalendarza)."""
    c = df['close']
    df['ret1'] = np.log(c).diff()
    df['ema10'] = c.ewm(span=10).mean()
    df['ema50'] = c.ewm(span=50).mean()
    df['ema200'] = c.ewm(span=200).mean()

    d = c.diff()
    up   = d.clip(lower=0).ewm(alpha=1/14, adjust=False).mean()
    down = (-d.clip(upper=0)).ewm(alpha=1/14, adjust=False).mean()
    rs = up / (down + 1e-12)
    df['rsi14'] = 100 - (100 / (1 + rs))

    h, l, cl = df['high'], df['low'], df['close']
    tr = np.maximum(h - l, np.maximum(abs(h - cl.shift()), abs(l - cl.shift())))
    df['atr14'] = tr.ewm(alpha=1/14, adjust=False).mean()

    ema_fast  = c.ewm(span=12, adjust=False).mean()
    ema_slow  = c.ewm(span=26, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_ln = macd_line.ewm(span=9, adjust=False).mean()
    df['macd']        = macd_line
    df['macd_signal'] = signal_ln
    df['macd_hist']   = macd_line - signal_ln

    ma = c.rolling(20).mean()
    sd = c.rolling(20).std()
    df['bb_ma']    = ma
    df['bb_up']    = ma + 2*sd
    df['bb_lo']    = ma - 2*sd
    df['bb_width'] = (df['bb_up'] - df['bb_lo']) / (ma.replace(0, np.nan).abs() + 1e-12)

    df['minute']  = df['time'].dt.hour*60 + df['time'].dt.minute
    df['tod_sin'] = np.sin(2*np.pi*df['minute']/1440)
    df['tod_cos'] = np.cos(2*np.pi*df['minute']/1440)
    df.drop(columns=['minute'], inplace=True)

    df['dow']     = df['time'].dt.dayofweek
    df['dow_sin'] = np.sin(2*np.pi*df['dow']/7)
    df['dow_cos'] = np.cos(2*np.pi*df['dow']/7)
    df.drop(columns=['dow'], inplace=True)

    df['close_log'] = np.log(c.clip(lower=1e-12))
    mu = df['close_log'].rolling(2000, min_periods=200).mean()
    s  = df['close_log'].rolling(2000, min_periods=200).std().replace(0, np.nan)
    df['close_norm'] = (df['close_log'] - mu) / (s + 1e-8)

    norm_cols = [
        'ret1','ema10','ema50','ema200','rsi14','atr14','macd','macd_signal',
        'macd_hist','bb_ma','bb_up','bb_lo','bb_width','tod_sin','tod_cos','dow_sin','dow_cos'
    ]
    for c_ in norm_cols:
        mu = df[c_].rolling(2000, min_periods=200).mean()
        s  = df[c_].rolling(2000, min_periods=200).std().replace(0, np.nan)
        df[c_] = (df[c_] - mu) / (s + 1e-8)

    return df.dropna().reset_index(drop=True)


def get_last_bars(symbol, timeframe, n: int):
    """Pobierz ostatnie n świec z MT5 (copy_rates_from_pos)."""
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, n)
    if rates is None or len(rates) < n:
        return None
    df = pd.DataFrame(rates)
    df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
    df.rename(columns={'real_volume': 'tick_volume'}, inplace=True)
    return df[['time', 'open', 'high', 'low', 'close', 'tick_volume', 'spread']]


def build_obs(df_feat: pd.DataFrame, model_obs_dim: int, pos: int, entry: float) -> np.ndarray:
    """Zbuduj wektor obserwacji zgodnie z kształtem MLP (WINDOW * (1 + n_feat) + 2)."""
    per_step = (model_obs_dim - 2) // WINDOW
    price_col = 'close_norm' if 'close_norm' in df_feat.columns else 'close'

    if features_spec is None:
        base = ['open', 'high', 'low', 'close', 'tick_volume', 'spread', 'time']
        feat_cols = [c for c in df_feat.columns if c not in base]
    else:
        feat_cols = [c for c in features_spec if c in df_feat.columns]

    tail  = df_feat.tail(WINDOW)
    block = tail[[price_col] + feat_cols].to_numpy(dtype=np.float32)
    if block.shape != (WINDOW, per_step):
        raise RuntimeError(f"Bad block {block.shape} vs {(WINDOW, per_step)}")

    flat  = block.flatten()
    price = float(df_feat['close'].iloc[-1])
    unreal = 0.0
    if pos != 0 and not np.isnan(entry):
        dir_ = 1 if pos > 0 else -1
        unreal = dir_ * (price - entry)
    return np.concatenate([flat, np.array([pos, unreal], dtype=np.float32)])


def main():
    # MT5: terminal musi być uruchomiony i zalogowany (demo OK), symbol w Market Watch
    if not ensure_mt5_ready():
        raise RuntimeError("MT5 not ready (terminal/account). Start MT5 and login to a demo account.")

    try:
        # Załaduj model i sprawdź wymiar obserwacji
        model = PPO.load(MODEL_PATH)
        expected_dim = int(model.observation_space.shape[0])
        assert (expected_dim - 2) % WINDOW == 0, "Observation dim not divisible by WINDOW."

        # VecNormalize (opcjonalnie)
        vecnorm = None
        if Path(VECNORM_PATH).exists():
            import numpy as np
            import gymnasium as gym
            from gymnasium import spaces

            class _ObsOnlyEnv(gym.Env):
                """Minimalne środowisko tylko do załadowania VecNormalize (nośnik kształtu obserwacji)."""
                metadata = {"render_modes": []}

                def __init__(self, obs_dim: int):
                    self.observation_space = spaces.Box(
                        low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32
                    )
                    self.action_space = spaces.Discrete(3)

                def reset(self, *, seed=None, options=None):
                    super().reset(seed=seed)
                    obs = np.zeros(self.observation_space.shape, dtype=np.float32)
                    return obs, {}

                def step(self, action):
                    obs = np.zeros(self.observation_space.shape, dtype=np.float32)
                    reward = 0.0
                    terminated = True   # pojedynczy krok wystarczy – to tylko kontener do VecNormalize
                    truncated  = False
                    info = {}
                    return obs, reward, terminated, truncated, info

            dummy = DummyVecEnv([lambda: _ObsOnlyEnv(expected_dim)])
            vecnorm = VecNormalize.load(VECNORM_PATH, dummy)
            vecnorm.training = False
            vecnorm.norm_reward = False

        # CSV z decyzjami
        csv = Path('paper_demo/decisions.csv')
        if not csv.exists():
            csv.write_text('time_utc,price,action_id,action_label,pos,equity\n', encoding='utf-8')

        last_ts = None
        hb_t = time.time()

        # Stan pozycji (jak w env)
        pos = 0
        entry = np.nan
        equity = 1.0

        while True:
            bars = get_last_bars(SYMBOL, TF, n=WINDOW + 800)
            if bars is None or len(bars) < WINDOW + 200:
                time.sleep(5)
                continue

            feat = add_features_incremental(bars)
            if len(feat) < WINDOW:
                time.sleep(5)
                continue

            cur_ts = feat['time'].iloc[-1]
            if last_ts is not None and cur_ts == last_ts:
                time.sleep(2)
                continue

            # Godziny handlu / enforce flat
            inside = in_trade_hours(cur_ts)

            # Obserwacja (z aktualnym pos/unreal)
            obs = build_obs(feat, expected_dim, pos=pos, entry=entry)
            if vecnorm is not None:
                obs = vecnorm.normalize_obs(obs.reshape(1, -1)).reshape(-1)

            action, _ = model.predict(obs, deterministic=True)
            desired = {0: -1, 1: 0, 2: 1}[int(action)]
            if not inside and ENF_FLAT:
                desired = 0

            price = float(feat['close'].iloc[-1])
            prev  = float(feat['close'].iloc[-2])
            slip  = SLIP_K * abs(price - prev)

            # Zmiana pozycji i koszty (jak w env)
            if desired != pos:
                # koszt zamknięcia starej
                if pos != 0 and not np.isnan(entry):
                    cost = (SPREAD_ABS + slip) / max(price, 1e-12) + COMM
                    equity = max(equity * (1.0 - cost), 1e-6)

                pos = desired
                if pos != 0:
                    cost = (SPREAD_ABS + slip) / max(price, 1e-12) + COMM
                    equity = max(equity * (1.0 - cost), 1e-6)
                    entry = price
                else:
                    entry = np.nan

            # Zmiana equity w kroku (reward_mode='pct' jak w env)
            if pos != 0:
                dir_ = 1 if pos > 0 else -1
                step_ret = dir_ * ((price / max(prev, 1e-12)) - 1.0)
                equity = equity * (1.0 + step_ret)

            decision = {-1: 'SHORT', 0: 'FLAT', 1: 'LONG'}[int(pos)]
            msg = f"DECISION {decision} @ {price:.2f} (ts={cur_ts}) equity={equity:.6f}"
            print(msg)
            logging.info(msg)

            with open(csv, 'a', encoding='utf-8') as f:
                f.write(f"{cur_ts},{price:.5f},{int(action)},{decision},{pos},{equity:.6f}\n")

            last_ts = cur_ts

            # Heartbeat co 10 minut
            if time.time() - hb_t > 600:
                logging.info(f"[HB] {SYMBOL} {TF_NAME} last_ts={cur_ts} price={price:.2f} pos={pos} equity={equity:.6f}")
                hb_t = time.time()

            time.sleep(30)

    finally:
        mt5.shutdown()


if __name__ == '__main__':
    main()

=== FILE: paper_demo\simulate_execution.py ===

# -*- coding: utf-8 -*-
import yaml, pandas as pd
from env_xau import XauTradingEnv
from stable_baselines3 import PPO
from pathlib import Path

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = cfg['files']['model_path']
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']

df = pd.read_csv(FEAT, parse_dates=['time'])
cut = df['time'].max() - pd.Timedelta(days=120)
sim = df[df['time']>=cut].copy()

env = XauTradingEnv(sim, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'])
obs, info = env.reset(); model = PPO.load(MODEL_PATH)
rows=[]; done=False
while not done:
    action,_=model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    done = bool(terminated) or bool(truncated)
    rows.append({'time': info.get('time',None), 'price': info.get('price',None), 'action': int(action), 'reward': float(reward), 'equity': float(env.equity)})
Path('reports').mkdir(parents=True, exist_ok=True)
pd.DataFrame(rows).to_csv('reports/trace_simulated.csv', index=False)
pd.Series([r['equity'] for r in rows]).to_csv('reports/equity_simulated.csv', index=False)
print('Saved simulated trace/equity.')


=== FILE: paper_demo\__init__.py ===



=== FILE: reports\costs_suggestion.yaml ===

spread_abs_median: 0.41
spread_abs_p75: 0.46
slippage_k_suggested: 0.0


=== FILE: reports\report.txt ===

# RL Validation Report (XAUUSD M5)
Date: 2025-11-04 21:19:36

## Base metrics

Final equity: 1.000000
Min equity: 0.993288
Max drawdown: -0.067083


## Extra metrics from equity
- Steps: 3000
- Sharpe (daily): 0.010
- Sortino (daily): 0.007
- Daily vol: 0.0210
- CAGR (est.): -0.269%
- MaxDD: -6.71%
- Calmar: -0.040


=== FILE: reports\val_metrics.txt ===

Final equity: 1.000000
Min equity: 0.993288
Max drawdown: -0.067083


=== FILE: rl\evaluate.py ===

# -*- coding: utf-8 -*-
import argparse, yaml, json
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from env_xau import XauTradingEnv

parser = argparse.ArgumentParser()
parser.add_argument('--max_eval_steps', type=int, default=3000)
parser.add_argument('--out_dir', type=str, default='reports')
args = parser.parse_args()

OUT = Path(args.out_dir); OUT.mkdir(parents=True, exist_ok=True)

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = cfg['files']['model_path']
VECNORM_PATH = cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl')
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))

df = pd.read_csv(FEAT, parse_dates=['time'])
cut_val = df['time'].max() - pd.Timedelta(days=120)
val = df[df['time']>=cut_val].copy()

features_spec = None
fspec = Path('models/features_spec.json')
if fspec.exists():
    features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)

def make_eval():
    env = XauTradingEnv(val, window=WINDOW,
        spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
        reward_mode=REWARD_MODE, use_close_norm=True, features_spec=features_spec, min_equity=MIN_EQ)
    return TimeLimit(env, max_episode_steps=args.max_eval_steps)

venv = DummyVecEnv([make_eval]); venv = VecMonitor(venv)
if Path(VECNORM_PATH).exists():
    venv = VecNormalize.load(VECNORM_PATH, venv); venv.training=False; venv.norm_reward=False
model = PPO.load(MODEL_PATH, env=venv)

obs = venv.reset(); eq=[]; rows=[]; done=False
while not done:
    action,_ = model.predict(obs, deterministic=True)
    obs, rewards, dones, infos = venv.step(action)
    equity = venv.get_attr('equity', indices=0)[0]
    eq.append(float(equity))
    info0 = infos[0] if isinstance(infos,(list,tuple)) else infos
    rows.append({'time': str(info0.get('time','')), 'price': info0.get('price',np.nan), 'pos': info0.get('pos',np.nan),
                 'equity': equity, 'action': int(action[0]) if hasattr(action,'__len__') else int(action)})
    done = bool(dones[0])

s = pd.Series(eq)
final_eq = float(s.iloc[-1]); min_eq=float(s.min()); peak=s.cummax(); max_dd=float((s/peak-1.0).min())
plt.figure(figsize=(10,4)); plt.plot(s.values); plt.title('Equity (validation)'); plt.tight_layout()
plt.savefig((OUT/'equity_val.png').as_posix(), dpi=150)
(OUT/'val_metrics.txt').write_text(
    f"Final equity: {final_eq:.6f}\nMin equity: {min_eq:.6f}\nMax drawdown: {max_dd:.6f}\n", encoding='utf-8')
pd.DataFrame(rows).to_csv(OUT/'eval_trace.csv', index=False)
print("Saved validation results.")


=== FILE: rl\train_ppo.py ===

# -*- coding: utf-8 -*-
import argparse, yaml, json
import pandas as pd
from pathlib import Path
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor, sync_envs_normalization
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.utils import set_random_seed
from gymnasium.wrappers import TimeLimit
from env_xau import XauTradingEnv

parser = argparse.ArgumentParser()
parser.add_argument('--timesteps', type=int, default=1500000)
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--eval_freq', type=int, default=100000)
parser.add_argument('--n_eval_episodes', type=int, default=1)
parser.add_argument('--max_train_steps', type=int, default=6000)
parser.add_argument('--max_eval_steps', type=int, default=3000)
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
MODEL_PATH = Path(cfg['files']['model_path'])
VECNORM_PATH = Path(cfg.get('files',{}).get('vecnorm_path','models/vecnorm_xauusd_m5.pkl'))
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))

df = pd.read_csv(FEAT, parse_dates=['time'])
cut_val = df['time'].max() - pd.Timedelta(days=30)
train_df = df[df['time'] < cut_val].copy()
val_df   = df[df['time'] >= cut_val].copy()

features_spec = None
fspec = Path('models/features_spec.json')
if fspec.exists():
    features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)

set_random_seed(args.seed)

def make_train():
    env = XauTradingEnv(train_df, window=WINDOW,
        spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
        reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
        enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
    return TimeLimit(env, max_episode_steps=args.max_train_steps)

def make_eval():
    env = XauTradingEnv(val_df, window=WINDOW,
        spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
        reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
        enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
    return TimeLimit(env, max_episode_steps=args.max_eval_steps)

venv_train = DummyVecEnv([make_train]); venv_train = VecMonitor(venv_train); venv_train = VecNormalize(venv_train, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)
venv_eval  = DummyVecEnv([make_eval]);  venv_eval  = VecMonitor(venv_eval);  venv_eval  = VecNormalize(venv_eval, training=False, norm_obs=True, norm_reward=False)

policy_kwargs = dict(net_arch=dict(pi=[128,128], vf=[128,128]))
try:
    import tensorboard as _tb
    tb_log_dir = 'logs/ppo_gold_m5'
except Exception:
    tb_log_dir = None

model = PPO('MlpPolicy', venv_train, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02,
            policy_kwargs=policy_kwargs, seed=args.seed, verbose=1, tensorboard_log=tb_log_dir)

sync_envs_normalization(venv_eval, venv_train)
eval_cb = EvalCallback(venv_eval, best_model_save_path=str(MODEL_PATH.parent), log_path='logs/eval',
                       eval_freq=max(args.eval_freq,1), n_eval_episodes=max(args.n_eval_episodes,1),
                       deterministic=True, render=False)

model.learn(total_timesteps=args.timesteps, callback=eval_cb)
MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
model.save(str(MODEL_PATH))
venv_train.save(str(VECNORM_PATH))
print(f"Saved model: {MODEL_PATH}")
print(f"Saved VecNormalize: {VECNORM_PATH}")


=== FILE: rl\walk_forward.py ===

# -*- coding: utf-8 -*-
import argparse, yaml, json
import pandas as pd, numpy as np
from pathlib import Path
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from env_xau import XauTradingEnv

parser = argparse.ArgumentParser()
parser.add_argument('--segments', type=int, default=6)
parser.add_argument('--train_days', type=int, default=120)
parser.add_argument('--val_days', type=int, default=30)
parser.add_argument('--timesteps', type=int, default=500000)
parser.add_argument('--seed', type=int, default=42)
parser.add_argument('--out_dir', type=str, default='reports_wf')
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
FEAT = cfg['files']['features_csv']
WINDOW = int(cfg.get('window',128))
COSTS = cfg['costs']
ENV_CFG = cfg.get('env',{})
REWARD_MODE = ENV_CFG.get('reward_mode', cfg.get('reward_mode','pct'))
FLIP_PENALTY = float(ENV_CFG.get('flip_penalty', cfg.get('flip_penalty',0.0)))
TRADE_HOURS = ENV_CFG.get('trade_hours_utc', cfg.get('trade_hours_utc', None))
MIN_EQ = float(ENV_CFG.get('min_equity', 0.8))

out = Path(args.out_dir); out.mkdir(parents=True, exist_ok=True)
df = pd.read_csv(FEAT, parse_dates=['time']).sort_values('time').reset_index(drop=True)
end_time = df['time'].max()

features_spec=None
fspec = Path('models/features_spec.json')
if fspec.exists(): features_spec = json.loads(fspec.read_text(encoding='utf-8')).get('feature_columns', None)

rows=[]
for k in range(args.segments):
    val_end = end_time - pd.Timedelta(days=k*args.val_days)
    val_start = val_end - pd.Timedelta(days=args.val_days)
    train_end = val_start
    train_start = train_end - pd.Timedelta(days=args.train_days)
    tr = df[(df['time']>=train_start)&(df['time']<train_end)].copy()
    va = df[(df['time']>=val_start)&(df['time']<val_end)].copy()
    if len(tr)<=WINDOW or len(va)<=WINDOW: continue

    def make_train():
        env = XauTradingEnv(tr, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
            reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
            enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
        return TimeLimit(env, max_episode_steps=6000)

    def make_eval():
        env = XauTradingEnv(va, window=WINDOW, spread_abs=COSTS['spread_abs'], commission_rate=COSTS['commission_rate'], slippage_k=COSTS['slippage_k'],
            reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
            enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ)
        return TimeLimit(env, max_episode_steps=3000)

    vt = DummyVecEnv([make_train]); vt = VecMonitor(vt); vt = VecNormalize(vt, norm_obs=True, norm_reward=True)
    ve = DummyVecEnv([make_eval]);  ve = VecMonitor(ve)

    model = PPO('MlpPolicy', vt, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02, seed=args.seed, verbose=0)
    model.learn(total_timesteps=args.timesteps)

    tmp = out/f'vecnorm_{k}.pkl'; vt.save(str(tmp))
    ve = VecNormalize.load(str(tmp), ve); ve.training=False; ve.norm_reward=False

    obs = ve.reset(); eq=[]; trace=[]; done=False
    while not done:
        action,_=model.predict(obs, deterministic=True)
        obs, rewards, dones, infos = ve.step(action)
        equity = ve.get_attr('equity', indices=0)[0]
        eq.append(float(equity))
        info0=infos[0] if isinstance(infos,(list,tuple)) else infos
        trace.append({'k':k,'time':str(info0.get('time','')),'price':info0.get('price',np.nan),'pos':info0.get('pos',np.nan),'equity':equity,
                      'action':int(action[0]) if hasattr(action,'__len__') else int(action)})
        done = bool(dones[0])

    s = pd.Series(eq); final_eq=float(s.iloc[-1]); min_eq=float(s.min()); peak=s.cummax(); max_dd=float((s/peak-1.0).min())
    rows.append({'k':k,'train_start':train_start,'train_end':train_end,'val_start':val_start,'val_end':val_end,'final_eq':final_eq,'min_eq':min_eq,'max_dd':max_dd,'n_steps':int(len(s))})
    pd.DataFrame(trace).to_csv(out/f'fold_{k}_trace.csv', index=False)

res = pd.DataFrame(rows).sort_values('k'); res.to_csv(out/'wf_results.csv', index=False)
if res.empty:
    txt=['# Walk-Forward Report','No results (windows too short?).']
else:
    agg={'folds':len(res),'final_eq_med':float(res['final_eq'].median()),'max_dd_med':float(res['max_dd'].median()),'n_steps_sum':int(res['n_steps'].sum())}
    txt=['# Walk-Forward Report',f"Folds: {agg['folds']}",f"Median Final Equity: {agg['final_eq_med']:.4f}",f"Median MaxDD: {agg['max_dd_med']:.2%}",f"Total steps: {agg['n_steps_sum']}"]
(out/'wf_report.txt').write_text('\n'.join(txt), encoding='utf-8')
print("Saved walk-forward results.")


=== FILE: rl\__init__.py ===



=== FILE: utils\atomic.py ===

from pathlib import Path

def atomic_write_csv(df, path: str):
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    tmp = p.with_suffix(p.suffix + ".tmp")
    df.to_csv(tmp, index=False)
    tmp.replace(p)


=== FILE: utils\calibration.py ===

# utils/calibration.py
# -*- coding: utf-8 -*-
import argparse, yaml, pandas as pd, numpy as np
from pathlib import Path

parser = argparse.ArgumentParser()
parser.add_argument('--apply', action='store_true', help='Nadpisz config.yaml sugerowanymi kosztami')
parser.add_argument('--ticks_csv', type=str, default=None, help='Ścieżka do CSV z tickami (domyślnie z config.yaml)')
parser.add_argument('--bars_csv', type=str, default=None, help='Ścieżka do CSV ze świecami (domyślnie z config.yaml)')
args = parser.parse_args()

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
TICKS = args.ticks_csv or cfg['files']['ticks_csv']
BARS  = args.bars_csv  or cfg['files']['bars_csv']

ticks = pd.read_csv(TICKS, parse_dates=['time'])
ticks['spread'] = ticks['ask'] - ticks['bid']
mid = (ticks['ask'] + ticks['bid']) / 2.0
dmid = mid.diff().abs()

spread_median = float(ticks['spread'].median())
spread_p95    = float(ticks['spread'].quantile(0.95))
bars = pd.read_csv(BARS, parse_dates=['time'])
bar_move_med = float(bars['close'].diff().abs().median())

# Slippage ~ median|Δmid| jako proporcja do mediany ruchu barowego (ucięta do sensownych widełek)
slippage_k = float(np.clip(dmid.median() / max(bar_move_med, 1e-12), 0.0, 0.5))

Path('reports').mkdir(parents=True, exist_ok=True)
Path('reports/costs_suggestion.yaml').write_text(
    yaml.safe_dump(
        {
            'spread_abs_median': round(spread_median, 5),
            'spread_abs_p95': round(spread_p95, 5),
            'slippage_k_suggested': round(slippage_k, 3),
        },
        allow_unicode=True, sort_keys=False
    ),
    encoding='utf-8'
)
print("Saved: reports/costs_suggestion.yaml")

if args.apply:
    cfg.setdefault('costs', {})
    cfg['costs']['spread_abs']   = round(spread_median, 5)
    cfg['costs']['slippage_k']   = round(slippage_k, 3)
    Path('config.yaml').write_text(yaml.safe_dump(cfg, allow_unicode=True, sort_keys=False), encoding='utf-8')
    print("Applied suggestions to config.yaml")

=== FILE: utils\make_report.py ===

# -*- coding: utf-8 -*-
import argparse, base64
from pathlib import Path
import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

parser = argparse.ArgumentParser()
parser.add_argument('--equity_csv', type=str, default='reports/eval_trace.csv')
parser.add_argument('--metrics', type=str, default='reports/val_metrics.txt')
parser.add_argument('--figure', type=str, default='reports/equity_val.png')
parser.add_argument('--out_html', type=str, default='reports/report.html')
parser.add_argument('--out_txt', type=str, default='reports/report.txt')
parser.add_argument('--bars_per_day', type=int, default=288)
parser.add_argument('--trading_days', type=int, default=252)
args = parser.parse_args()

out_html = Path(args.out_html)
out_txt  = Path(args.out_txt)
out_html.parent.mkdir(parents=True, exist_ok=True)
out_txt.parent.mkdir(parents=True, exist_ok=True)

eq = None
p = Path(args.equity_csv)
if p.exists():
    df = pd.read_csv(p)
    if 'equity' in df.columns:
        eq = df['equity'].astype(float).to_numpy()
    elif df.shape[1] == 1:
        eq = df.iloc[:, 0].astype(float).to_numpy()

metrics_text = Path(args.metrics).read_text(encoding='utf-8') if Path(args.metrics).exists() else ''

img_b64 = ''
if Path(args.figure).exists():
    img_b64 = base64.b64encode(Path(args.figure).read_bytes()).decode('ascii')

# Jeśli nie ma obrazka, wygeneruj go z danych equity
if not img_b64 and eq is not None and len(eq) > 1:
    fig, ax = plt.subplots(figsize=(9, 4))
    ax.plot(eq, color='#0057B8', lw=1.5)
    ax.set_title('Equity (validation)')
    ax.set_xlabel('Steps')
    ax.set_ylabel('Equity')
    fig.tight_layout()
    Path(args.figure).parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(args.figure, dpi=144)
    plt.close(fig)
    img_b64 = base64.b64encode(Path(args.figure).read_bytes()).decode('ascii')

# Dodatkowe metryki
extra = {}
if eq is not None and len(eq) > 2:
    ret = np.diff(eq) / np.maximum(eq[:-1], 1e-12)
    mu = float(np.nanmean(ret))
    sigma = float(np.nanstd(ret) + 1e-12)
    downside = ret[ret < 0]
    ds = float(np.nanstd(downside) + 1e-12)

    daily_mu = mu * args.bars_per_day
    daily_sigma = sigma * (args.bars_per_day ** 0.5)
    sharpe = daily_mu / (daily_sigma + 1e-12)
    sortino = daily_mu / (((args.bars_per_day ** 0.5) * ds) + 1e-12)

    n = len(eq)
    years = max(n / args.bars_per_day / args.trading_days, 1e-6)
    cagr = (eq[-1] / max(eq[0], 1e-12)) ** (1 / years) - 1 if years > 0 else 0.0
    peak = np.maximum.accumulate(eq)
    max_dd = float((eq / peak - 1.0).min())
    calmar = (cagr / abs(max_dd)) if max_dd < 0 else float('inf')

    extra = dict(
        n_steps=n,
        sharpe_daily=sharpe,
        sortino_daily=sortino,
        vol_daily=daily_sigma,
        cagr=cagr,
        max_dd=max_dd,
        calmar=calmar
    )

now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

md = [
    '# RL Validation Report (XAUUSD M5)',
    f'Date: {now}',
    '',
    '## Base metrics',
    '',
    metrics_text.strip() if metrics_text else '(no val_metrics.txt)',
    '',
    ''
]

if extra:
    md += [
        '## Extra metrics from equity',
        f"- Steps: {extra['n_steps']}",
        f"- Sharpe (daily): {extra['sharpe_daily']:.3f}",
        f"- Sortino (daily): {extra['sortino_daily']:.3f}",
        f"- Daily vol: {extra['vol_daily']:.4f}",
        f"- CAGR (est.): {extra['cagr']:.3%}",
        f"- MaxDD: {extra['max_dd']:.2%}",
        f"- Calmar: {extra['calmar']:.3f}",
    ]
else:
    md += ['No equity data.']

out_txt.write_text('\n'.join(md) + '\n', encoding='utf-8')

html = f"""RL Validation
RL Validation Report (XAUUSD M5)
Base metrics
{metrics_text if metrics_text else '(no val_metrics.txt)'}
Extra metrics
{('\n'.join(md)) if extra else 'No equity data.'}
Equity
{('data:image/png;base64,' + img_b64) if img_b64 else 'No plot.'}
Generated: {now}
"""
out_html.write_text(html, encoding='utf-8')
print('Saved report.')

=== FILE: utils\mt5_health.py ===

import MetaTrader5 as mt5
import time

def ensure_mt5_ready(retries=3, sleep_s=2):
    """Ponawia initialize i weryfikuje, że terminal + konto są gotowe."""
    for i in range(retries):
        if mt5.initialize():
            ti, ai = mt5.terminal_info(), mt5.account_info()
            if ti and ai and getattr(ai, "login", 0):
                return True
            mt5.shutdown()
        time.sleep(sleep_s * (i + 1))
    return False


=== FILE: utils\qc_bars.py ===

#!/usr/bin/env python
import pandas as pd, argparse
parser=argparse.ArgumentParser(); parser.add_argument('--csv', required=True); parser.add_argument('--tf_min', type=int, default=5)
args=parser.parse_args()
df=pd.read_csv(args.csv, parse_dates=['time']).sort_values('time').reset_index(drop=True)
print(f"Rows: {len(df)}\nFrom: {df['time'].iloc[0]} To: {df['time'].iloc[-1]}")
dt=(df['time'].diff().dt.total_seconds()/60).fillna(args.tf_min)
gaps=df.loc[dt>args.tf_min+0.1,['time']].copy(); gaps['gap_min']=dt[dt>args.tf_min+0.1].values
print(f"Gaps > {args.tf_min} min: {len(gaps)}");
if len(gaps): print(gaps.head(10))


=== FILE: utils\time_utils.py ===

from datetime import datetime, timezone
def now_utc(): return datetime.now(timezone.utc)


=== FILE: utils\__init__.py ===



=== FILE: _backup_py\env_xau.py ===

-- coding: utf-8 --
import numpy as np, pandas as pd
import gymnasium as gym
from gymnasium import spaces
from datetime import time as dtime
class XauTradingEnv(gym.Env):
metadata = {"render_modes": []}
def init(self, df: pd.DataFrame, window=128,
spread_abs=0.05, commission_rate=0.0001, slippage_k=0.10,
reward_mode: str = "pct", use_close_norm: bool = True,
flip_penalty: float = 0.0, trade_hours_utc=None,
enforce_flat_outside_hours: bool = True, features_spec: list | None = None,
min_equity: float = 0.8):
super().init()
self.df = df.reset_index(drop=True)
self.window = int(window)
self.spread_abs = float(spread_abs)
self.commission_rate = float(commission_rate)
self.slippage_k = float(slippage_k)
assert reward_mode in {"pct","points"}
self.reward_mode = reward_mode
self.use_close_norm = use_close_norm
self.flip_penalty = float(flip_penalty)
self.enforce_flat_outside = bool(enforce_flat_outside_hours)
self.min_equity = float(min_equity)
self.trade_hours = None
if trade_hours_utc and isinstance(trade_hours_utc,(list,tuple)) and len(trade_hours_utc)==2:
try:
s = [int(x) for x in str(trade_hours_utc[0]).split(":")]
e = [int(x) for x in str(trade_hours_utc[1]).split(":")]
self.trade_hours = (dtime(s[0], s[1] if len(s)>1 else 0), dtime(e[0], e[1] if len(e)>1 else 0))
except Exception:
self.trade_hours = None
base_cols = ['open','high','low','close','tick_volume','spread','time']
if features_spec is not None:
missing = [c for c in features_spec if c not in self.df.columns]
if missing: raise ValueError(f"Missing feature columns: {missing}")
self.feat_cols = list(features_spec)
else:
self.feat_cols = [c for c in self.df.columns if c not in base_cols]
self.price_col = 'close_norm' if (self.use_close_norm and 'close_norm' in self.df.columns) else 'close'
if len(self.df) <= self.window:
raise ValueError(f"Not enough rows: {len(self.df)} <= window {self.window}")
obs_dim = self.window * (1 + len(self.feat_cols)) + 2
self.action_space = spaces.Discrete(3)
self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
self._start = self.window
self._i = None
self.pos = 0
self.entry = None
self.equity = 1.0
self.prev_eq = 1.0
def _in_trade_hours(self, ts)->bool:
if self.trade_hours is None: return True
try: t = ts.to_pydatetime().time()
except Exception: t = ts
start, end = self.trade_hours
if start <= end: return (t>=start) and (t<=end)
return (t>=start) or (t<=end)
def _obs(self):
sl = slice(self._i - self.window, self.i)
block_df = self.df.iloc[sl][[self.price_col] + self.feat_cols]
block = block_df.to_numpy(dtype=np.float32)
expected = 1 + len(self.feat_cols)
if block.shape != (self.window, expected):
raise RuntimeError(f"Bad window shape: {block.shape} vs {(self.window, expected)}")
flat = block.flatten()
price = float(self.df.iloc[self.i]['close'])
unreal = 0.0
if self.pos != 0 and self.entry is not None:
dir = 1 if self.pos>0 else -1
unreal = dir * (price - self.entry)
import numpy as np
return np.concatenate([flat, np.array([self.pos, unreal], dtype=np.float32)])
def reset(self, seed=None, options=None):
super().reset(seed=seed)
self._i = self._start
self.pos = 0; self.entry = None
self.equity = 1.0; self.prev_eq = 1.0
return self._obs(), {}
def step(self, action):
import numpy as np
if isinstance(action,(np.ndarray,list,tuple)): action = int(action[0])
else: action = int(action)
if not self.action_space.contains(action): raise ValueError("Invalid action")
info = {}
ts = self.df.iloc[self._i]['time']
inside = self._in_trade_hours(ts)
price = float(self.df.iloc[self.i]['close'])
prev = float(self.df.iloc[self.i-1]['close'])
slip = self.slippage_k * abs(price - prev)
desired = [-1,0,1][action]
if not inside and self.enforce_flat_outside: desired = 0
if desired != self.pos:
if self.pos != 0 and desired != 0 and np.sign(self.pos) != np.sign(desired) and self.flip_penalty>0:
if self.reward_mode == 'pct': self.equity *= max(1.0 - self.flip_penalty, 1e-6)
else: self.equity -= self.flip_penalty
info['flip_penalty'] = float(self.flip_penalty)
if self.pos != 0 and self.entry is not None:
cost = (self.spread_abs + slip)/max(price,1e-12) + self.commission_rate
if self.reward_mode == 'pct': self.equity *= max(1.0 - cost, 1e-6)
else: self.equity -= cost
self.pos = desired
if self.pos != 0:
cost = (self.spread_abs + slip)/max(price,1e-12) + self.commission_rate
if self.reward_mode == 'pct': self.equity *= max(1.0 - cost, 1e-6)
else: self.equity -= cost
self.entry = price
else:
self.entry = None
if self.reward_mode == 'pct':
step_ret = 0.0
if self.pos != 0:
dir = 1 if self.pos>0 else -1
step_ret = dir * ((price/max(prev,1e-12)) - 1.0)
self.equity *= (1.0 + step_ret)
reward = float(self.equity - self.prev_eq)
else:
reward = float(self.equity - self.prev_eq)
self.prev_eq = self.equity
self._i += 1
terminated = bool(self._i >= len(self.df) - 1)
truncated = False
if self.equity <= self.min_equity:
truncated = True
info['early_stop'] = True
info.update({'time': ts, 'price': price, 'pos': int(self.pos), 'equity': float(self.equity), 'inside_hours': bool(inside)})
return self._obs(), reward, terminated, truncated, info


=== FILE: _backup_py\fetch__mt5_data.py ===

# -*- coding: utf-8 -*-
"""
Fetch bars & 24h tick sample from MT5 (demo/real). Incremental merge to maintain 720d.
Atomic CSV writes, Retries/backoff, Weekend-safe ticks (warn only)
"""
import MetaTrader5 as mt5
import pandas as pd
from datetime import datetime, timedelta, timezone
import yaml
from pathlib import Path
import numpy as np, time
from utils.atomic import atomic_write_csv
from utils.mt5_health import ensure_mt5_ready

cfg = yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
SYMBOL = cfg['symbol']
BARS_CSV = cfg['files']['bars_csv']
TICKS_CSV = cfg['files']['ticks_csv']
HISTORY_DAYS = int(cfg.get('history_days', 720))
TF_MAP={'M1': mt5.TIMEFRAME_M1,'M5': mt5.TIMEFRAME_M5,'M15': mt5.TIMEFRAME_M15}
TF_NAME = cfg.get('timeframe','M5')
TF = TF_MAP.get(TF_NAME, mt5.TIMEFRAME_M5)
CHUNK = int(cfg.get('mt5',{}).get('bars_chunk', 20000))
UPDATE_DAYS = int(cfg.get('mt5',{}).get('update_days', 60))

def init_mt5():
    # inicjalizacja + czytelny komunikat, gdy terminal/konto nie są gotowe
    if not ensure_mt5_ready():
        last_err = mt5.last_error()
        raise RuntimeError(f"MT5 not ready (terminal/account). Start MT5 and login to a demo/real account. last_error={last_err}")
    term = mt5.terminal_info(); acc = mt5.account_info()
    parts=[]
    if getattr(term,"company",None): parts.append(f"Company={term.company}")
    if getattr(term,"name",None): parts.append(f"TerminalName={term.name}")
    if getattr(acc,"login",0): parts.append(f"Login={acc.login}")
    if getattr(acc,"server",None): parts.append(f"Server={acc.server}")
    if getattr(acc,"name",None): parts.append(f"AccountName={acc.name}")
    print("[MT5] " + " | ".join(parts) if parts else "[MT5] initialized")

def ensure_symbol(symbol: str)->bool:
    info = mt5.symbol_info(symbol)
    if info is None:
        mt5.symbol_select(symbol, True)
        info = mt5.symbol_info(symbol)
    if info is None: return False
    if not info.visible:
        if not mt5.symbol_select(symbol, True): return False
    return True

def with_retries(fn, attempts=3, sleep_s=2, *a, **kw):
    last=None
    for i in range(attempts):
        try:
            return fn(*a, **kw)
        except Exception as e:
            last = e
            time.sleep(sleep_s*(i+1))
    if last: raise last

def _to_df(rates):
    df = pd.DataFrame(rates)
    if df.empty: return df
    df['time'] = pd.to_datetime(df['time'], unit='s', utc=True)
    if 'real_volume' in df.columns:
        df.rename(columns={'real_volume':'tick_volume'}, inplace=True)
    cols=['time','open','high','low','close','tick_volume','spread']
    return df[cols].sort_values('time').drop_duplicates('time')

def fetch_range(symbol, timeframe, utc_from, utc_to):
    rates = mt5.copy_rates_range(symbol, timeframe, utc_from, utc_to)
    if rates is None or len(rates)==0:
        last_err = mt5.last_error()
        raise RuntimeError(f"copy_rates_range empty for '{symbol}'. last_error={last_err}")
    return _to_df(rates)

def fetch_from_pos_tail(symbol, timeframe, count):
    rates = mt5.copy_rates_from_pos(symbol, timeframe, 0, count)
    if rates is None or len(rates)==0:
        last_err = mt5.last_error()
        raise RuntimeError(f"copy_rates_from_pos empty for '{symbol}'. last_error={last_err}")
    return _to_df(rates)

def load_existing_bars(path: str):
    p = Path(path)
    if not p.exists(): return None
    try:
        df = pd.read_csv(p, parse_dates=['time'])
        if df.empty: return None
        return df.sort_values('time').drop_duplicates('time').reset_index(drop=True)
    except Exception:
        return None

def merge_clip(existing: pd.DataFrame | None, new_df: pd.DataFrame, keep_days:int)->pd.DataFrame:
    if existing is None or existing.empty:
        base = new_df.copy()
    else:
        base = (pd.concat([existing, new_df], ignore_index=True)
                .drop_duplicates('time').sort_values('time'))
    cutoff = datetime.now(timezone.utc) - timedelta(days=keep_days+1)
    base = base[base['time'] >= pd.Timestamp(cutoff)]
    return base.reset_index(drop=True)

def fetch_bars_incremental(symbol, timeframe, keep_days:int, update_days:int)->pd.DataFrame:
    existing = load_existing_bars(BARS_CSV)
    if existing is None:
        to = datetime.now(timezone.utc); frm = to - timedelta(days=keep_days+2)
        df = with_retries(lambda: fetch_range(symbol, timeframe, frm, to))
        if len(df) < 1000:
            tail = with_retries(lambda: fetch_from_pos_tail(symbol, timeframe, CHUNK))
            df = merge_clip(df, tail, keep_days)
        return df
    to = datetime.now(timezone.utc); frm = to - timedelta(days=max(2, update_days))
    fresh = with_retries(lambda: fetch_range(symbol, timeframe, frm, to))
    merged = merge_clip(existing, fresh, keep_days)
    return merged

def fetch_ticks(symbol: str, hours: int=24)->pd.DataFrame:
    utc_to = datetime.now(timezone.utc)
    utc_from = utc_to - timedelta(hours=hours)
    ticks = mt5.copy_ticks_range(symbol, utc_from, utc_to, mt5.COPY_TICKS_ALL)
    if ticks is None or len(ticks)==0:
        last_err = mt5.last_error()
        raise RuntimeError(f"No MT5 ticks for '{symbol}'. last_error={last_err}")
    tdf = pd.DataFrame(ticks)
    tdf['time'] = pd.to_datetime(tdf['time'], unit='s', utc=True)
    return tdf[['time','bid','ask','last','volume']]

def suggest_costs(tdf: pd.DataFrame)->dict:
    spr = (tdf['ask'] - tdf['bid']).astype(float).replace([np.inf,-np.inf], np.nan).dropna()
    med_spread = float(np.median(spr)) if len(spr) else 0.0
    p75_spread = float(np.percentile(spr, 75)) if len(spr) else 0.0
    mid = (tdf['ask'] + tdf['bid'])/2.0
    dm = (mid.diff().abs()).replace([np.inf,-np.inf], np.nan).dropna()
    med_dm = float(np.median(dm)) if len(dm) else 0.0
    med_price = float(np.nanmedian(mid)) if len(mid) else 1.0
    slippage_k = float(min(max(med_dm / max(med_price, 1e-12), 0.0), 0.01))
    return {'spread_abs_median': round(med_spread,5), 'spread_abs_p75': round(p75_spread,5), 'slippage_k_suggested': round(slippage_k,4)}

def main():
    init_mt5()
    try:
        print(f"[CFG] symbol={SYMBOL} tf={TF_NAME} keep_days={HISTORY_DAYS} update_days={UPDATE_DAYS}")
        if not ensure_symbol(SYMBOL):
            raise SystemExit(
                "Symbol not found or not visible in Market Watch. "
                "Open Symbols window in MT5 and SHOW the instrument."
            )
        bars = fetch_bars_incremental(SYMBOL, TF, keep_days=HISTORY_DAYS, update_days=UPDATE_DAYS)
        assert {'time','open','high','low','close'}.issubset(bars.columns), "Bars frame malformed"
        assert len(bars) > 100, "Too few bars"
        atomic_write_csv(bars, BARS_CSV)
        print(f"Saved bars: {BARS_CSV} ({len(bars)})")
        # ticks są opcjonalne (weekend-safe)
        try:
            ticks = with_retries(lambda: fetch_ticks(SYMBOL, hours=24))
            atomic_write_csv(ticks, TICKS_CSV)
            print(f"Saved ticks: {TICKS_CSV} ({len(ticks)})")
            sugg = suggest_costs(ticks)
            Path('reports').mkdir(parents=True, exist_ok=True)
            Path('reports/costs_suggestion.yaml').write_text(
                yaml.safe_dump(sugg, allow_unicode=True, sort_keys=False), encoding='utf-8')
            print("Cost suggestions -> reports/costs_suggestion.yaml")
        except Exception as e:
            print(f"[warn] ticks unavailable ({e}); keeping previous {TICKS_CSV}")
    finally:
        mt5.shutdown()

if __name__ == '__main__':
    main()

=== FILE: _backup_py\quick_env_test.py ===

import yaml, pandas as pd
from env_xau import XauTradingEnv
cfg=yaml.safe_load(open('config.yaml','r',encoding='utf-8'))
df=pd.read_csv(cfg['files']['features_csv'], parse_dates=['time']).sort_values('time').reset_index(drop=True)
df=df.tail(max(int(cfg.get('window',128))+256,512))
env=XauTradingEnv(df, window=int(cfg.get('window',128)),
spread_abs=cfg['costs']['spread_abs'], commission_rate=cfg['costs']['commission_rate'], slippage_k=cfg['costs']['slippage_k'],
reward_mode=cfg.get('env',{}).get('reward_mode', cfg.get('reward_mode','pct')), use_close_norm=True,
min_equity=float(cfg.get('env',{}).get('min_equity',0.8)))
obs,info=env.reset(); print('reset OK', len(obs))
obs,r,term,trunc,info=env.step(1); print('step OK', r, term, trunc)
