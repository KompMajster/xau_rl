# -*- coding: utf-8 -*-
"""
ops/mini_update_ppo.py – bezpieczny mini-update PPO na świeżych danych (z rozszerzonym logowaniem).
Użycie:
  python ops/mini_update_ppo.py --timesteps 300000 --train_days 120 --val_days 30 --seed 42
"""
import argparse, json, sys, traceback
from pathlib import Path
import pandas as pd, yaml
from gymnasium.wrappers import TimeLimit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor, sync_envs_normalization
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.utils import set_random_seed

ap = argparse.ArgumentParser()
ap.add_argument("--timesteps", type=int, default=300000)
ap.add_argument("--train_days", type=int, default=120)
ap.add_argument("--val_days", type=int, default=30)
ap.add_argument("--seed", type=int, default=42)
args = ap.parse_args()

def _to_naive_utc(s: pd.Series) -> pd.Series:
    # Ujednolić czas: jeżeli tz-aware → na UTC i zdejmij tzinfo (naive),
    # jeżeli naive → tylko upewnij się, że to datetime64[ns]
    if pd.api.types.is_datetime64_any_dtype(s):
        try:
            if s.dt.tz is not None:
                return s.dt.tz_convert("UTC").dt.tz_localize(None)
            return s
        except Exception:
            # np. miks typów – parsuj jeszcze raz
            return pd.to_datetime(s, utc=True).dt.tz_localize(None)
    return pd.to_datetime(s, utc=True).dt.tz_localize(None)

def main():
    cfg = yaml.safe_load(open("config.yaml","r",encoding="utf-8"))
    FEAT = cfg["files"]["features_csv"]
    MODEL_PATH = Path(cfg["files"]["model_path"])
    VECNORM_PATH = Path(cfg.get("files",{}).get("vecnorm_path","models/vecnorm_xauusd_m5.pkl"))
    WINDOW = int(cfg.get("window",128))
    COSTS = cfg["costs"]
    ENV_CFG = cfg.get("env",{}) or {}
    REWARD_MODE = ENV_CFG.get("reward_mode", cfg.get("reward_mode","pct"))
    FLIP_PENALTY = float(ENV_CFG.get("flip_penalty", cfg.get("flip_penalty",0.0)))
    TRADE_HOURS = ENV_CFG.get("trade_hours_utc", cfg.get("trade_hours_utc", None))
    MIN_EQ = float(ENV_CFG.get("min_equity", 0.8))

    # ====== Dane + sanity =====================================================
    df = pd.read_csv(FEAT, parse_dates=["time"])
    # porządek i unikalny 'time'
    df = (df.sort_values("time")
            .drop_duplicates("time")
            .reset_index(drop=True))
    # standaryzuj 'time'
    df["time"] = _to_naive_utc(df["time"])

    total_rows = len(df)
    tmin, tmax = df["time"].min(), df["time"].max()
    print(f"[mini_update] features_csv={FEAT}")
    print(f"[mini_update] df rows={total_rows}  from={tmin}  to={tmax}", flush=True)

    # okna czasowe
    val_end   = tmax
    val_start = val_end - pd.Timedelta(days=args.val_days)
    train_start = val_start - pd.Timedelta(days=args.train_days)

    train_df = df[(df["time"] >= train_start) & (df["time"] <  val_start)].copy()
    val_df   = df[(df["time"] >= val_start)  & (df["time"] <= val_end)].copy()

    print(f"[mini_update] ranges: train_start={train_start}  val_start={val_start}  val_end={val_end}")
    print(f"[mini_update] counts: train={len(train_df)}  val={len(val_df)}  (WINDOW={WINDOW})", flush=True)

    # Zapis próbki do debug
    Path("logs").mkdir(parents=True, exist_ok=True)
    try:
        sample = pd.DataFrame({
            "tmin":[tmin], "tmax":[tmax],
            "train_start":[train_start], "val_start":[val_start], "val_end":[val_end],
            "train_rows":[len(train_df)], "val_rows":[len(val_df)]
        })
        sample.to_csv("logs/debug_mini_update.csv", index=False)
    except Exception:
        pass

    # ====== Fallback, jeśli okna puste / za krótkie ==========================
    if len(train_df) <= WINDOW or len(val_df) <= WINDOW:
        # spróbuj „ostatni ogon” – tyle żeby było > WINDOW
        need = max(WINDOW*3, 3000)  # minimalna bufora
        if total_rows > need:
            tail = df.iloc[-(need+WINDOW):, :].copy()
            # 2/3 train, 1/3 val na ogonie
            split = int(len(tail)*2/3)
            train_df = tail.iloc[:split, :].copy()
            val_df   = tail.iloc[split:, :].copy()
            print(f"[mini_update][fallback] tail-split used: train={len(train_df)}, val={len(val_df)}", flush=True)

    print(f"[mini_update] Training on {len(train_df)} rows (~{args.train_days}d), timesteps={args.timesteps}", flush=True)
    if len(train_df) <= WINDOW or len(val_df) <= WINDOW:
        raise RuntimeError(f"Too few rows for window={WINDOW}: train={len(train_df)}, val={len(val_df)}")

    # ====== features_spec =====================================================
    features_spec = None
    fspec = Path("models/features_spec.json")
    if fspec.exists():
        features_spec = json.loads(fspec.read_text(encoding="utf-8")).get("feature_columns", None)

    # ====== Środowiska ========================================================
    from env_xau import XauTradingEnv
    def make_train():
        env = XauTradingEnv(
            train_df, window=WINDOW,
            spread_abs=COSTS["spread_abs"], commission_rate=COSTS["commission_rate"], slippage_k=COSTS["slippage_k"],
            reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=FLIP_PENALTY, trade_hours_utc=TRADE_HOURS,
            enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ
        )
        return TimeLimit(env, max_episode_steps=6000)

    def make_eval():
        env = XauTradingEnv(
            val_df, window=WINDOW,
            spread_abs=COSTS["spread_abs"], commission_rate=COSTS["commission_rate"], slippage_k=COSTS["slippage_k"],
            reward_mode=REWARD_MODE, use_close_norm=True, flip_penalty=0.0, trade_hours_utc=TRADE_HOURS,
            enforce_flat_outside_hours=True, features_spec=features_spec, min_equity=MIN_EQ
        )
        return TimeLimit(env, max_episode_steps=3000)

    set_random_seed(args.seed)
    vtrain = DummyVecEnv([make_train]); vtrain = VecMonitor(vtrain); vtrain = VecNormalize(vtrain, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)
    veval  = DummyVecEnv([make_eval]);  veval  = VecMonitor(veval);  veval  = VecNormalize(veval, training=False, norm_obs=True, norm_reward=False)
    sync_envs_normalization(veval, vtrain)

    policy_kwargs = dict(net_arch=dict(pi=[128,128], vf=[128,128]))
    tb_log_dir = "logs/ppo_gold_m5"
    model = PPO("MlpPolicy", vtrain, n_steps=4096, batch_size=256, learning_rate=3e-4, ent_coef=0.02,
                policy_kwargs=policy_kwargs, seed=args.seed, verbose=1, tensorboard_log=tb_log_dir)

    eval_cb = EvalCallback(veval, best_model_save_path=str(MODEL_PATH.parent), log_path="logs/eval",
                           eval_freq=max(args.timesteps//4, 10000), n_eval_episodes=1,
                           deterministic=True, render=False)
    model.learn(total_timesteps=args.timesteps, callback=eval_cb)

    MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)
    model.save(str(MODEL_PATH))
    vtrain.save(str(VECNORM_PATH))
    print(f"[mini_update] Saved model -> {MODEL_PATH}")
    print(f"[mini_update] Saved VecNormalize -> {VECNORM_PATH}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("\n[mini_update][FATAL] Uncaught exception:\n", file=sys.stderr, flush=True)
        traceback.print_exc()
        sys.exit(1)
